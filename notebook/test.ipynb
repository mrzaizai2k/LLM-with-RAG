{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/code_Bao/LLM-with-RAG/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(file_path)\n",
    "except:\n",
    "    file_path = os.path.abspath('')\n",
    "    os.chdir(os.path.dirname(file_path))\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    RecursiveUrlLoader, \n",
    "    DirectoryLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    BSHTMLLoader,\n",
    "    UnstructuredImageLoader,\n",
    "    Docx2txtLoader,\n",
    "    TextLoader,\n",
    "    SeleniumURLLoader,\n",
    "    YoutubeLoader,\n",
    "    UnstructuredURLLoader,\n",
    "    NewsURLLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    ")\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "import torch\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = config_parser(data_config_path = 'config/data_config.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_url_list = data.get('single_url_list')\n",
    "loader = NewsURLLoader(urls=single_url_list, \n",
    "                    post_processors=[clean_extra_whitespace],)\n",
    "text = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recursive_url_list = data.get('recursive_url_list')\n",
    "\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=recursive_url_list[0], max_depth=2,\n",
    "      extractor=lambda x: Soup(x, \"html.parser\").text,\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = Docx2txtLoader(\"web_data/sample4.docx\")\n",
    "# data = loader.load()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [\n",
    "# \"https://e.vnexpress.net/news/travel-guide/binh-phuoc-boasts-jungle-treks-charming-feasts-4673392.html\",\n",
    "# 'https://e.vnexpress.net/news/culture/vietnamese-comic-artist-wins-silver-at-int-l-manga-awards-4695364.html',\n",
    "# ]\n",
    "# loader = UnstructuredURLLoader(urls=urls, \n",
    "#                     post_processors=[clean_extra_whitespace],)\n",
    "# data = loader.load()\n",
    "# print(data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='So now I get to tell you about the very\\ncool randomized contraction algorithm for computing the minimum cut of a graph.\\nLet\\'s just recall what the minimum cut problem is. We\\'re given as input an\\nundirected graph. And the parallel edges are allowed. In fact, they will arise naturally\\nthroughout the course of the algorithm. That is, we\\'re given pair of vertices, which\\nhave multiple edges which have that pair as endpoints. Now, I do sort of assume\\nyou\\'ve watched the other video on how graphs are actually represented, although\\nthat\\'s not going to play a major role in the description of this particular\\nalgorithm. And, again, the goal is to compute the cut. So, a cut is a partition\\nof the graph vertices into two groups, A and B. The number of edges crossing the\\ncut is simply those that have one endpoint on each side. And amongst all the\\nexponentially possible cuts, we want to identify one that has The fewest number of\\ncrossing edges, or a \"min cut\". >>So, here\\'s the random contraction algorithm. So, this\\nalgorithm was devised by David Karger back when he was an early Ph.D student here at\\nStanford, and this was in the early 90s. So like I said, quote unquote only about\\ntwenty years ago. And the basic idea is to use random sampling. Now, we\\'d known\\nforever, right, ever since QuickSort, that random sampling could be a good idea in\\ncertain context, in particular when you\\'re sorting and searching. Now one of the\\nthings that was such a breakthrough about Karger\\'s contraction algorithm is, it\\nshowed that random sampling can be extremely effective for fundamental graph\\nproblems. >>So here\\'s how it works. We\\'re just gonna have one main loop. Each\\niteration of this while-Loop is going to decrease the number of vertices in the\\ngraph by 1, and we\\'re gonna terminate when we get down to just two vertices\\nremaining. Now, in a given iteration, here\\'s the random sampling: amongst all of\\nthe edges that remain in the graph to this point, we\\'re going to choose one of those\\nedges uniformly at random. Each edge is equally likely. Once you\\'ve chosen an\\nedge, that\\'s when we do the contraction. So we take the two endpoints of the edge,\\ncall them the vertex u and the vertex v, and we fuse them into a single vertex that\\nrepresents both of them. This may become more clear when I go through a couple\\nexamples on the next couple of slides. This merging may create parallel edges,\\neven if you didn\\'t have them before. That\\'s okay. We\\'re gonna leave the\\nparallel edges. And it may create a self-loop edge pointer that both of the endpoints\\nis the same. And self-loops are stupid, so we\\'re just gonna delete as they\\narise. Each generation decreases the number of vertices that remain. We start\\nwith N vertices. We end up with 2. So after N-2 generations, that\\'s when\\nwe stop and at that point we return the cuts represented by those two final\\nvertices. You might well be wondering what I mean by the cut represented by the final\\ntwo vertices. But I think that will become clear in the examples, which I\\'ll proceed\\nto now. >>So suppose the input graph is the following four node, four edge\\ngraph. There\\'s a square plus one diagonal. So, how would the contraction algorithm\\nwork on this graph? Well, of course, it\\'s a randomized algorithm so it could work in\\ndifferent ways. And so, we\\'re gonna look at two different trajectories. In the\\nfirst iteration each of these five edges is equally likely. Each is chosen for\\ncontraction with twenty percent probability. For concreteness, let\\'s say\\nthat the algorithm happens to choose this edge to contract, to fuse the two\\nendpoints. After the fusion these two vertices on the left have become one,\\nwhereas the two vertices on the right are still hanging around like they always\\nwere. So, the edge between the two original vertices is unchanged. The\\ncontracted edge between the two vertices on the left has gotten sucked up, so that\\'s gone. And\\nso what remains are these two edges here. The edge on top, and the diagonal. And\\nthose are now parallel edges, between the fused node and the upper right node. And\\nthen I also shouldn\\'t forget the bottom edge, which is edge from the lower right\\nnode to the super node. So that\\'s what we mean by taking a pair of the vertices and\\ncontracting them. The edge that was previously connected with them vanishes,\\nand then all the other edges just get pulled into the fusion. >>So that\\'s the\\nfirst iteration of Karger\\'s algorithm of one possible execution. So now we proceed\\nto the second iteration of the contraction algorithm, and the same thing happens all\\nover again. We pick an edge, uniformly at random. Now there\\'s only four edges that\\nremain, each of which is equally likely to be chosen, so the 25% probability.\\nFor concreteness, let\\'s say that in the second iteration, we wind up choosing one\\nof the two parallel edges, say this one here. So what happens? Well, now, instead\\nof three vertices, we go down to 2. We have the original bottom right vertex that\\nhasn\\'t participated in any contractions at all, so that\\'s as it was. And then we have\\nthe second vertex, which actually represents diffusion of all of the other\\nthree vertices. So two of them were fused, the leftmost vertices were fused in\\niteration 1. And now the upper right vertex got fused into with them to create\\nthis super node representing three original vertices. So, what happens to the\\nfour edges? Well, the contracted one disappears. That just gets sucked into the\\nsuper node, and we never see it again. Again, and then the other three go, and\\nwhere there\\'s, go where they\\'re supposed to go. So there\\'s the edge that used to be\\nthe right most edge. That has no hash mark. There\\'s the edge with two hash\\nmarks. That goes between the, the same two nodes that it did before. Just the super\\nnode is now an even bigger node representing three nodes. And then the\\nedge which was parallel to the one that we contracted, the other one with a hash mark\\nbecomes a self-loop. And remember what the, what the algorithm does is, whenever\\nself loops like this appear, they get deleted automatically. And now that we\\'ve\\ndone our N-2 iterations, we\\'re down to just two nodes. We return the\\ncorresponding cut. By corresponding cut, what I mean is, one group of the cut is\\nthe vertices that got fused into each other, and wound up corresponding to the\\nsuper node. In this case, everything but the bottom right node, And then the other\\ngroup is the original nodes corresponding to the other super node of the contracted\\ngraphs, which, in this case, in just the bottom right node by itself. So this Set A\\nis going to be these three nodes here, which all got fused into each other,\\ncontracted into each other. And B is going to be this node over here which never\\nparticipated in any contractions at all. And what\\'s cool is, you\\'ll notice, this\\ndoes, in fact, define a min cut. There are two edges crossing this cut. This one, the\\nrightmost one and the bottommost one. And I\\'ll leave it for you to check that there\\nis no cut in this graph with fewer than two crossing edges, so this is in fact a\\nmin cut. >>Of course, this is a randomized algorithm, and randomized algorithms can\\nbehave differently on different executions. So let\\'s look at a second\\npossible execution of the contraction algorithm on this exact same input. Let\\'s\\neven suppose the first iteration goes about in exactly the same way. So, in\\nparticular, this leftmost edge is gonna get chosen in the first iteration. Then\\ninstead of choosing one of the two parallel edges, which suppose that we\\nchoose the rightmost edge to contract in the second iteration. Totally possible, 25%\\nchance that it\\'s gonna happen. Now what happens after the contraction? Well,\\nagain, we\\'re gonna be left with two nodes, no surprise there. The contracted node\\ngets sucked into oblivion and vanishes. But the other three edges, the ones with\\nthe hash marks, all stick around, and become parallel edges between these two\\nfinal nodes. This, again, corresponds to a cut (A, B), where A is the left two\\nvertices, and B is the right two vertices. Now, this cut you\\'ll notice has three\\ncrossing edges, and we\\'ve already seen that there is a cut with two crossing edges.\\nTherefore, this is not a min cut. >>So what have we learned? We\\'ve learned that,\\nthe contractual algorithm sometimes identifies the min cut, and sometimes it\\ndoes not. It depends on the random choices that it makes. It depends on which edges\\nit chooses to randomly contract. So the obvious question is, you know, is this a\\nuseful algorithm. So in particular, what is the probability that it gets the right\\nanswer? We know it\\'s bigger than 0, and we know it\\'s less than 1. Is it close to\\n1, or is it close to 0? So we find ourselves in a familiar position. We have\\nwhat seems like a quite sweet algorithm, this random contraction\\nalgorithm. And we don\\'t really know if it\\'s good or not. We don\\'t really know how\\noften it works, and we\\'re going to need to do a little bit of math to answer that\\nquestion. So in particular, we\\'ll need some conditional probability. So for those\\nof you, who need a refresher, go to your favorite source, or you can watch the\\nProbability Review Part II, to get a refresher on conditional probability and\\nindependence. Once you have that in your mathematical toolbox, we\\'ll be able to\\ntotally nail this question. Get a very precise answer to exactly how frequently\\nthe contraction algorithm successfully computes the minimum cut.', metadata={'source': 'c75gg0wicus'})]\n"
     ]
    }
   ],
   "source": [
    "youtube_url_list = data.get('youtube_url_list')\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    youtube_url_list[0], add_video_info=False\n",
    ")\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/code_Bao/LLM-with-RAG/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "class NewsSummarizer:\n",
    "    def __init__(self, summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\"),\n",
    "                 max_length:int=230, \n",
    "                 min_length:int=30,\n",
    "                 ):\n",
    "        self.summarizer = summarizer\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        \n",
    "    def summary_text(self,text:str)->str:\n",
    "        '''Summary short text'''\n",
    "        text = self.summarizer(text, max_length=self.max_length, min_length=self.min_length, do_sample=False)[0]['summary_text']\n",
    "        return text\n",
    "    \n",
    "    def summary_news(self, news:str)->str:\n",
    "        '''\n",
    "        Summary news. Because the news is too long (Loss of Information) so I will split news into 2 parts\n",
    "        '''\n",
    "        sample = news.split('. ')\n",
    "        art1 = '. '.join(sample[:int(len(sample)/2)])\n",
    "        art2 = '. '.join(sample[int(len(sample)/2):])\n",
    "        sum_art1 = self.summary_text(art1)\n",
    "        sum_art2 = self.summary_text(art2)\n",
    "        summary_text = f'   {sum_art1}\\n    {sum_art2}'\n",
    "        return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is this that is not prime numbers I know 49 is not a prime number I know that 9 is not a prime number yet llama thinks that this function actually finds prime numbers accuracy aside you might want to run llama on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can actually take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million unless you're Apple or Google okay how do we run llama locally on an Apple silicon machine that's what I'm going to show you today if you don't have an apple silicon machine running llamas easy you go to the GitHub URL then I'm gonna leave all the links down below including the instructions and the steps you're gonna go to this one Facebook research slash Lama on GitHub you're going to clone this repository follow the instructions and boom you're done that's if you're running on Linux or Windows but this is not going to work on Apple silicon there is a C plus plus project that's also open source and that one will now you're gonna need a few things for this and again I'm gonna put all the steps down below you will need to clone this repository this Facebook research llama Repository I've created a folder called llama2 and I'm going to git clone this repository I'm going to do this along with you so you can do it along with me there we go clone that's this one right here llama now we're going to go to this other repository and this is the C plus Port this port is done by the same person that created whisper and I've done videos on whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker it supports all these models llama alpaca and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha with this thing is that you need to convert your model from the original to a different model structure and I'm going to show you how to do that don't worry so let's clone this one too git clone and boom done that's two out of the three tasks already done holy cow what's the third task well if you go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you're gonna need to click on this link that says request and new download link and I will link to this link in the description also when you go here you're going to need to fill in a little bit of information for MATA to keep track of you accept the terms and click on this accept and continue at which point they will approve you and a couple hours later at least it was a couple hours for me they sent me a link and the link looks like this this is the email I got from them this link is not going to work anymore because it's only good for 24 hours you just basically use this very special and very short-lived link and that'll allow you to download all these models how do you download the models well this repository right here the original one will tell you how to do it first of all there's this download.sh file here the email actually tells you the instructions you visit the repository you download it and you run this download.sh script when you run that script it's going to ask you for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a checklist.chk file and then a bunch of these pth files these are the actual models but still all the other files are important as well as the params.json and when you do the download you're also going to get the tokenizer checklist check file and the tokenizer model file make sure you copy those somewhere safe as well so I put them here in my models folder now once these download and it took me a long time to download these and I have fast internet so maybe you want to start out with a 7 billion parameter model and just get your feet wet and then download the bigger ones later which is what I'm going to show you right now I'm going to use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here we're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it and it's going to build a bunch of binaries but the two most important ones the ones that we're gonna use Let's do an LS here it's going to be quantize and Main I'll show you how to use that momentarily you're still dependent on python because you're going to need to use a script in here called convert this script right here is a python script and this python script converts from the original models from The Source models into this new format that this program runs on so to run this python file we're going to need to install some python dependencies now you might or you might not already have python installed I do I have 3.9 this could work I could just install the requirements by by the way that's this file right here requirements.txt that contains all the requirements for the python environment to have before you can run convert all these steps I know I know I actually will add a little bit more steps for my environment but you don't have to if you already have python a decent new version of python installed but what I like to do is I have a conda environment in Anaconda environment set up and I'll create a brand new environment just for this task and if you don't know what I'm talking about I made a video on how to create and manage conda environments these are python environments these keep your python projects separate so they don't interfere with each other with different version numbers and all that I created that video and I'll link to that video down below if you don't know how to do that so I'm gonna do that real quick right now conda create and I'm going to give this environment a name llama2 I'm going to activate the environment using this command con to activate llama2 and now I'm going to install the dependencies into this environment well first of all let's check the version of python here I have 2.7 that's not gonna work we need a newer version of python so I'm going to say conda install python equals 3.11 and now inside this llama2 environment I'm gonna have a version a new version of python python 311 just for this environment if I check my python version 3.11 so now I can use this new version of python to install the dependencies which is this command python pip install Dash R requirements Boom the requirements installed good to go now that our python environment is set up and our python requirements are installed we can run this convert dot Pi script so I'm going to paste it in here I'm still in my content environment notice in the parentheses it says llama2 that's the name of my environment python convert and then I'm going to give it the output file which is in the models folder 7B and then the name of the model I'll type F16 and then where can I find the original model let's go through that real quick shall we I'm going to open up my finder here and we can take a look at where everything is first of all I'm inside my llama.cpp folder all right which is right here everything's inside my llama2 folder including the llama.cpp I'm running this convert script and I'm telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion parameter models and the 70 billion parameter models we're just gonna do this with one of the seven billion one so that's gonna be the output file it's going to go into models 7B and then it's going to be called ggml Dash model Dash F16 dot bin and where are we getting the original files from the original models well that's coming from llama 2 meta models llama 2 7B chat so this folder right here remember these are the ones that I downloaded from meta it's going to look into this folder and basically pick out the one it needs and process it so let's run that there it is it's running it okay so it's doing the conversion right now this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin it's just a different format but now this file is huge it's 13.48 gigabytes just for that model and if you get to the 13 billion parameter model is 26 and the 70 billion parameter model is even bigger it's what eight files that are 17.25 gigabytes each that's huge so there's another step and that's to quantize these models into smaller file size and we're going to use the quantize command remember when we did make we built Main and we built quantize now we're going to use quantize to convert this model that we've converted to quantize this model that we've converted sorry I didn't come up with this stuff okay I'm just a messenger don't kill me I'm gonna paste this command in here again commands are down there quantize models 7B this is the one right here the one we just converted we're pointing to that and we're gonna put the new one in the same folder and this is going to be the quantized version so let's run that and there it is the Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do our uh inference finally it's time for the inference to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I think it's the number of tokens correct me if I'm wrong I think it's the number of tokens that we can respond with I'm gonna set that to I don't know let's do 10 24. and doesn't have to be you can pick whatever number you want let's try this I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is but it's doing it there it is that was generated by the 7 billion parameter model and I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all and you also have this little prompts folder here well this has some examples we can send one of these files to the model as an example or as a starting prompt for example this chat with bob.txt there's a little bit of a primer and a little bit of a start of the dialogue uh part of the prompt and the understanding is that the model will continue or prompt the user to continue let's try this out I'm gonna paste a new command here and some of this will be familiar main the model and then there is something called repeat penalty 1.0 I have not looked into that I don't know what that is but it's in one of the examples in the repository if you want to read through the repository which you should probably do that it might explain it then Color dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here so I'm gonna go ahead and run that now this right now is running on the CPU how do I know that if I pop open activity monitor you'll see that our CPU is being used here I believe it's waiting for me to type something let's see hello Bob says how may I assist you today hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript function that'll find prime numbers this is a little bit different than the one it gave me earlier but what the heck let's try this out also notice that while that thing is generating there was the main process name and it was using a bunch of CPUs you can actually specify you can give the main program a flag I believe it's dash dash t for the number of threads to use and you can use more threads and it'll output faster that way let's go to our JavaScript project here it's a little node project I created to try this out and looks like we pasted right in there without any modifications and I'm going to run it oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well that did not work so as far as reliability of the code that it generates not exactly usable code it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does um it's printing out the function is prime generate primes it defined is prime let's copy this code paste it in here save it and run it and find prime numbers is not defined well it's because it's a different function name so we need to change that that's my fault and there it is are these all Primes it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code if you want to use llama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parameter model and it's not consistent and sometimes it gives simple answers that work and sometimes it gives simple programs that don't work use it for Creative tasks use it for writing tasks because for completion of sentences spreading poetry I don't think it's ready for code yet so hopefully this was helpful to you if you want to try this out on your own make sure you check the steps and the links and the commands that I pasted Down Below in the description thanks for watching and I'll see you next time\n"
     ]
    }
   ],
   "source": [
    "text = data[0].page_content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is this that is not prime numbers I know 49 is not a prime number I know that 9 is not a prime number yet llama thinks that this function actually finds prime numbers accuracy aside you might want to run llama on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can actually take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million unless you're Apple or Google okay how do we run llama locally on an Apple silicon machine that's what I'm going to show you today if you don't have an apple silicon machine running llamas easy you go to the GitHub URL then I'm gonna leave all the links down below including the instructions and the steps you're gonna go to this one Facebook research slash Lama on GitHub you're going to clone this repository follow the instructions and boom you're done that's if you're running on Linux or Windows but this is not going to work on Apple silicon there is a C plus plus project that's also open source and\n"
     ]
    }
   ],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=230, chunk_overlap=10)\n",
    "texts = text_splitter.split_text(text)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"what is this that is not prime numbers I know 49 is not a prime number I know that 9 is not a prime number yet llama thinks that this function actually finds prime numbers accuracy aside you might want to run llama on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can actually take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million unless you're Apple or Google okay how do we run llama locally on an Apple silicon machine that's what I'm going to show you today if you don't have an apple silicon machine running llamas easy you go to the GitHub URL then I'm gonna leave all the links down below including the instructions and the steps you're gonna go to this one Facebook research slash Lama on GitHub you're going to clone this repository follow the instructions and boom you're done that's if you're running on Linux or Windows but this is not going to work on Apple silicon there is a C plus plus project that's also open source and\",\n",
       " \" C plus plus project that's also open source and that one will now you're gonna need a few things for this and again I'm gonna put all the steps down below you will need to clone this repository this Facebook research llama Repository I've created a folder called llama2 and I'm going to git clone this repository I'm going to do this along with you so you can do it along with me there we go clone that's this one right here llama now we're going to go to this other repository and this is the C plus Port this port is done by the same person that created whisper and I've done videos on whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker it supports all these models llama alpaca and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha with this thing is that you need to convert your model from the original to a different model structure and I'm going to show you how to do that don't worry so let's clone this one too git clone and boom done that's two out of the three tasks already done\",\n",
       " \" that's two out of the three tasks already done holy cow what's the third task well if you go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you're gonna need to click on this link that says request and new download link and I will link to this link in the description also when you go here you're going to need to fill in a little bit of information for MATA to keep track of you accept the terms and click on this accept and continue at which point they will approve you and a couple hours later at least it was a couple hours for me they sent me a link and the link looks like this this is the email I got from them this link is not going to work anymore because it's only good for 24 hours you just basically use this very special and very short-lived link and that'll allow you to download all these models how do you download the models well this repository right here the original one will tell you how to do it first of all there's this download.sh file here the email actually tells you the instructions you visit the repository you download\",\n",
       " \" tells you the instructions you visit the repository you download it and you run this download.sh script when you run that script it's going to ask you for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a checklist.chk file and then a bunch of these pth files these are the actual models but still all the other files are important as well as the params.json and when you do the download you're also going to get the tokenizer checklist check file and the tokenizer model file make sure you copy those somewhere safe as well so I put them here in my models folder now once these download and it took me a long time to download these and I have fast internet\",\n",
       " \" long time to download these and I have fast internet so maybe you want to start out with a 7 billion parameter model and just get your feet wet and then download the bigger ones later which is what I'm going to show you right now I'm going to use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here we're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it and it's going to build a bunch of binaries but the two most important ones the ones that we're gonna use Let's do an LS here it's going to be quantize and Main I'll show you how to use that momentarily you're still dependent on python because you're going to need to use a script in here called convert this script right here is a python script and this python script converts from the original models from The Source models into this new format that this program runs on so to run this python file we're going to need to install some\",\n",
       " \" python file we're going to need to install some python dependencies now you might or you might not already have python installed I do I have 3.9 this could work I could just install the requirements by by the way that's this file right here requirements.txt that contains all the requirements for the python environment to have before you can run convert all these steps I know I know I actually will add a little bit more steps for my environment but you don't have to if you already have python a decent new version of python installed but what I like to do is I have a conda environment in Anaconda environment set up and I'll create a brand new environment just for this task and if you don't know what I'm talking about I made a video on how to create and manage conda environments these are python environments these keep your python projects separate so they don't interfere with each other with different version numbers and all that I created that video and I'll link to that video down below if you don't know how to do that so I'm gonna do that real quick right now conda create and I'm going to give this environment a\",\n",
       " \" create and I'm going to give this environment a name llama2 I'm going to activate the environment using this command con to activate llama2 and now I'm going to install the dependencies into this environment well first of all let's check the version of python here I have 2.7 that's not gonna work we need a newer version of python so I'm going to say conda install python equals 3.11 and now inside this llama2 environment I'm gonna have a version a new version of python python 311 just for this environment if I check my python version 3.11 so now I can use this new version of python to install the dependencies which is this command python pip install Dash R requirements Boom the requirements installed good to go now that our python environment is set up and our python requirements are installed we can run this convert dot Pi script so I'm going to paste it in here I'm still in my content environment notice in the parentheses it says llama2 that's the name of my environment python convert and then I'm going to give it the output file which is in the models folder 7B and then the name of\",\n",
       " \" the models folder 7B and then the name of the model I'll type F16 and then where can I find the original model let's go through that real quick shall we I'm going to open up my finder here and we can take a look at where everything is first of all I'm inside my llama.cpp folder all right which is right here everything's inside my llama2 folder including the llama.cpp I'm running this convert script and I'm telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion parameter models and the 70 billion parameter models we're just gonna do this with one of the seven billion one so that's gonna be the output file it's going to go into models\",\n",
       " \" the output file it's going to go into models 7B and then it's going to be called ggml Dash model Dash F16 dot bin and where are we getting the original files from the original models well that's coming from llama 2 meta models llama 2 7B chat so this folder right here remember these are the ones that I downloaded from meta it's going to look into this folder and basically pick out the one it needs and process it so let's run that there it is it's running it okay so it's doing the conversion right now this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin it's just a different format but now this file is huge it's 13.48 gigabytes just for that model and if you get to the 13 billion parameter model is 26 and the 70 billion parameter model is even bigger it's what eight files that are 17.25 gigabytes each that's huge so there's another step and that's to quantize these models into smaller file size and we're going to use the quantize command remember when we did make\",\n",
       " \" use the quantize command remember when we did make we built Main and we built quantize now we're going to use quantize to convert this model that we've converted to quantize this model that we've converted sorry I didn't come up with this stuff okay I'm just a messenger don't kill me I'm gonna paste this command in here again commands are down there quantize models 7B this is the one right here the one we just converted we're pointing to that and we're gonna put the new one in the same folder and this is going to be the quantized version so let's run that and there it is the Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do our uh inference finally it's time for the inference to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I\",\n",
       " \" allow us to specify the number of tokens to I think it's the number of tokens correct me if I'm wrong I think it's the number of tokens that we can respond with I'm gonna set that to I don't know let's do 10 24. and doesn't have to be you can pick whatever number you want let's try this I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is but it's doing it there it is that was generated by the 7 billion parameter model and I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all and you also have this little prompts folder here well this has some examples we can send one of these files to the model as an example or as a starting prompt for example this chat with bob.txt there's a little bit\",\n",
       " \" chat with bob.txt there's a little bit of a primer and a little bit of a start of the dialogue uh part of the prompt and the understanding is that the model will continue or prompt the user to continue let's try this out I'm gonna paste a new command here and some of this will be familiar main the model and then there is something called repeat penalty 1.0 I have not looked into that I don't know what that is but it's in one of the examples in the repository if you want to read through the repository which you should probably do that it might explain it then Color dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here so I'm gonna go ahead and run that now this right now is running on the CPU how do I know that if I pop open activity monitor you'll see that our CPU is being used here I believe it's waiting for me to type something let's see hello Bob says how may I\",\n",
       " \" something let's see hello Bob says how may I assist you today hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript function that'll find prime numbers this is a little bit different than the one it gave me earlier but what the heck let's try this out also notice that while that thing is generating there was the main process name and it was using a bunch of CPUs you can actually specify you can give the main program a flag I believe it's dash dash t for the number of threads to use and you can use more threads and it'll output faster that way let's go to our JavaScript project here it's a little node project I created to try this out and looks like we pasted right in there without any modifications and I'm going to run it oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well that did not work so as far as reliability of the code that it generates not exactly usable code it's not like Chad GPT where you actually\",\n",
       " \" it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does um it's printing out the function is prime generate primes it defined is prime let's copy this code paste it in here save it and run it and find prime numbers is not defined well it's because it's a different function name so we need to change that that's my fault and there it is are these all Primes it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code if you want to use llama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parameter model and it's not consistent and sometimes it gives simple answers that work and sometimes it gives simple programs that don't work use it for Creative tasks use it for writing tasks because for completion of sentences spreading poetry I don't think it's ready for code yet so hopefully this was helpful to you if you want to try this out on\",\n",
       " \" to you if you want to try this out on your own make sure you check the steps and the links and the commands that I pasted Down Below in the description thanks for watching and I'll see you next time\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 212. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 230, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 147. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    llama thinks that this function actually finds prime numbers accuracy aside you might want to run llamas on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million . I'm gonna leave all the links down below including the instructions . and the steps you'\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    I'm going to git clone this repository this Facebook research llama Repository I've created a folder called LLama2 . this is the C plus Port this port is done by the same person that created whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you'll need to click on this link that says request and new download link and I'll link to this link in the description also when you go here you will need to fill in a bit of information for MATA to keep track of you accept the terms and click on that accept and continue at which point they will approve you and a couple hours later . the link looks like this this is the\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    'm going to show you right now I'm gonna use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here We're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it . Main I'll show you how to use that momentarily you're still dependent on\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    I have a conda environment set up and I'll create a brand new environment just for this task . if you don't know what I'm talking about I made a video on how to create and manage python environments . I've created a .txt command con to activate llama2 and now I will install the dependencies into this environment .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    's not gonna work we need a newer version of python 311 so now I'm gonna have a version anew version . I'll check my pYthon version 3.11 and now I can use this new version to install the dependencies which is this command plython pip install Dash R requirements Boom the requirements installed good to go now that our environment is set up we can run this convert dot Pi script so we're going to paste it in\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    I'm running this convert script and telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion model and the 70\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin . now this file is huge it's 13.48 gigabytes just for that model and the 70 billion parameter model is even bigger . we're going to use quantize to convert this model that we've converted to quantize this model . I'm just a messenger don't kill me.\n",
      "   Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I don't know let's do 10 24 24 .\n",
      "    I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    Color Dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript Function that'll find prime numbers\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does . it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    ama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parametri model and it's not consistent and sometimes it gives simple answers that work . Creative tasks Use it for writing tasks because for completion of sentences spreading poetry I don't think it' is ready for code yet so hopefully this was helpful to you if you want to try this out on your own check the steps and the links and the commands that I pasted\n"
     ]
    }
   ],
   "source": [
    "sum_doc = []\n",
    "for txt in texts:\n",
    "    sum_text = NewsSummarizer().summary_news(txt)\n",
    "    sum_doc.append(sum_text)\n",
    "    \n",
    "sum_doc_text = '\\n'.join(sum_doc)\n",
    "print(sum_doc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   : . :: ! !! .. .! ? !. & ! # !\\n    llama thinks that this function actually finds prime numbers accuracy aside you might want to run llamas on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million . I'm gonna leave all the links down below including the instructions . and the steps you'\\n   : . :: ! !! .. .! ? !. & ! # !\\n    I'm going to git clone this repository this Facebook research llama Repository I've created a folder called LLama2 . this is the C plus Port this port is done by the same person that created whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you'll need to click on this link that says request and new download link and I'll link to this link in the description also when you go here you will need to fill in a bit of information for MATA to keep track of you accept the terms and click on that accept and continue at which point they will approve you and a couple hours later . the link looks like this this is the\\n   : . :: ! !! .. .! ? !. & ! # !\\n    for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a\\n   : . :: ! !! .. .! ? !. & ! # !\\n    'm going to show you right now I'm gonna use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here We're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it . Main I'll show you how to use that momentarily you're still dependent on\\n   : . :: ! !! .. .! ? !. & ! # !\\n    I have a conda environment set up and I'll create a brand new environment just for this task . if you don't know what I'm talking about I made a video on how to create and manage python environments . I've created a .txt command con to activate llama2 and now I will install the dependencies into this environment .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    's not gonna work we need a newer version of python 311 so now I'm gonna have a version anew version . I'll check my pYthon version 3.11 and now I can use this new version to install the dependencies which is this command plython pip install Dash R requirements Boom the requirements installed good to go now that our environment is set up we can run this convert dot Pi script so we're going to paste it in\\n   : . :: ! !! .. .! ? !. & ! # !\\n    I'm running this convert script and telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion model and the 70\\n   : . :: ! !! .. .! ? !. & ! # !\\n    this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin . now this file is huge it's 13.48 gigabytes just for that model and the 70 billion parameter model is even bigger . we're going to use quantize to convert this model that we've converted to quantize this model . I'm just a messenger don't kill me.\\n   Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I don't know let's do 10 24 24 .\\n    I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    Color Dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript Function that'll find prime numbers\\n   : . :: ! !! .. .! ? !. & ! # !\\n    oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does . it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    ama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parametri model and it's not consistent and sometimes it gives simple answers that work . Creative tasks Use it for writing tasks because for completion of sentences spreading poetry I don't think it' is ready for code yet so hopefully this was helpful to you if you want to try this out on your own check the steps and the links and the commands that I pasted\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   I'm going to clone this repository this Facebook research slash Lama on GitHub . I've created a folder called llama2 and you're going to go to this other repository . this is the C plus Port this is done by the same person that created whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker . if you go back to the original repository you are gonna need to scroll down a little bit and there's a\\n    I'm going to run this and it's spewing out all sorts of weird stuff some of it is in German wow yeah it even has some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all and you can send one of these files to the model as an example or as a starting prompt for example this chat with bob.txt or one of the other ones in\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"web_data/sample3.txt\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(str(sum_doc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what is this that is not prime numbers I know 49 is not a prime number I know that 9 is not a prime number yet llama thinks that this function actually finds prime numbers accuracy aside you might want to run llama on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can actually take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million unless you're Apple or Google okay how do we run llama locally on an Apple silicon machine that's what I'm going to show you today if you don't have an apple silicon machine running llamas easy you go to the GitHub URL then I'm gonna leave all the links down below including the instructions and the steps you're gonna go to this one Facebook research slash Lama on GitHub you're going to clone this repository follow the instructions and boom you're done that's if you're running on Linux or Windows but this is not going to work on Apple silicon there is a C plus plus project that's also open source and that one will now you're gonna need a few things for this and again I'm gonna put all the steps down below you will need to clone this repository this Facebook research llama Repository I've created a folder called llama2 and I'm going to git clone this repository I'm going to do this along with you so you can do it along with me there we go clone that's this one right here llama now we're going to go to this other repository and this is the C plus Port this port is done by the same person that created whisper and I've done videos on whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker it supports all these models llama alpaca and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha with this thing is that you need to convert your model from the original to a different model structure and I'm going to show you how to do that don't worry so let's clone this one too git clone and boom done that's two out of the three tasks already done holy cow what's the third task well if you go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you're gonna need to click on this link that says request and new download link and I will link to this link in the description also when you go here you're going to need to fill in a little bit of information for MATA to keep track of you accept the terms and click on this accept and continue at which point they will approve you and a couple hours later at least it was a couple hours for me they sent me a link and the link looks like this this is the email I got from them this link is not going to work anymore because it's only good for 24 hours you just basically use this very special and very short-lived link and that'll allow you to download all these models how do you download the models well this repository right here the original one will tell you how to do it first of all there's this download.sh file here the email actually tells you the instructions you visit the repository you download it and you run this download.sh script when you run that script it's going to ask you for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a checklist.chk file and then a bunch of these pth files these are the actual models but still all the other files are important as well as the params.json and when you do the download you're also going to get the tokenizer checklist check file and the tokenizer model file make sure you copy those somewhere safe as well so I put them here in my models folder now once these download and it took me a long time to download these and I have fast internet so maybe you want to start out with a 7 billion parameter model and just get your feet wet and then download the bigger ones later which is what I'm going to show you right now I'm going to use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here we're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it and it's going to build a bunch of binaries but the two most important ones the ones that we're gonna use Let's do an LS here it's going to be quantize and Main I'll show you how to use that momentarily you're still dependent on python because you're going to need to use a script in here called convert this script right here is a python script and this python script converts from the original models from The Source models into this new format that this program runs on so to run this python file we're going to need to install some python dependencies now you might or you might not already have python installed I do I have 3.9 this could work I could just install the requirements by by the way that's this file right here requirements.txt that contains all the requirements for the python environment to have before you can run convert all these steps I know I know I actually will add a little bit more steps for my environment but you don't have to if you already have python a decent new version of python installed but what I like to do is I have a conda environment in Anaconda environment set up and I'll create a brand new environment just for this task and if you don't know what I'm talking about I made a video on how to create and manage conda environments these are python environments these keep your python projects separate so they don't interfere with each other with different version numbers and all that I created that video and I'll link to that video down below if you don't know how to do that so I'm gonna do that real quick right now conda create and I'm going to give this environment a name llama2 I'm going to activate the environment using this command con to activate llama2 and now I'm going to install the dependencies into this environment well first of all let's check the version of python here I have 2.7 that's not gonna work we need a newer version of python so I'm going to say conda install python equals 3.11 and now inside this llama2 environment I'm gonna have a version a new version of python python 311 just for this environment if I check my python version 3.11 so now I can use this new version of python to install the dependencies which is this command python pip install Dash R requirements Boom the requirements installed good to go now that our python environment is set up and our python requirements are installed we can run this convert dot Pi script so I'm going to paste it in here I'm still in my content environment notice in the parentheses it says llama2 that's the name of my environment python convert and then I'm going to give it the output file which is in the models folder 7B and then the name of the model I'll type F16 and then where can I find the original model let's go through that real quick shall we I'm going to open up my finder here and we can take a look at where everything is first of all I'm inside my llama.cpp folder all right which is right here everything's inside my llama2 folder including the llama.cpp I'm running this convert script and I'm telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion parameter models and the 70 billion parameter models we're just gonna do this with one of the seven billion one so that's gonna be the output file it's going to go into models 7B and then it's going to be called ggml Dash model Dash F16 dot bin and where are we getting the original files from the original models well that's coming from llama 2 meta models llama 2 7B chat so this folder right here remember these are the ones that I downloaded from meta it's going to look into this folder and basically pick out the one it needs and process it so let's run that there it is it's running it okay so it's doing the conversion right now this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin it's just a different format but now this file is huge it's 13.48 gigabytes just for that model and if you get to the 13 billion parameter model is 26 and the 70 billion parameter model is even bigger it's what eight files that are 17.25 gigabytes each that's huge so there's another step and that's to quantize these models into smaller file size and we're going to use the quantize command remember when we did make we built Main and we built quantize now we're going to use quantize to convert this model that we've converted to quantize this model that we've converted sorry I didn't come up with this stuff okay I'm just a messenger don't kill me I'm gonna paste this command in here again commands are down there quantize models 7B this is the one right here the one we just converted we're pointing to that and we're gonna put the new one in the same folder and this is going to be the quantized version so let's run that and there it is the Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do our uh inference finally it's time for the inference to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I think it's the number of tokens correct me if I'm wrong I think it's the number of tokens that we can respond with I'm gonna set that to I don't know let's do 10 24. and doesn't have to be you can pick whatever number you want let's try this I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is but it's doing it there it is that was generated by the 7 billion parameter model and I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all and you also have this little prompts folder here well this has some examples we can send one of these files to the model as an example or as a starting prompt for example this chat with bob.txt there's a little bit of a primer and a little bit of a start of the dialogue uh part of the prompt and the understanding is that the model will continue or prompt the user to continue let's try this out I'm gonna paste a new command here and some of this will be familiar main the model and then there is something called repeat penalty 1.0 I have not looked into that I don't know what that is but it's in one of the examples in the repository if you want to read through the repository which you should probably do that it might explain it then Color dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here so I'm gonna go ahead and run that now this right now is running on the CPU how do I know that if I pop open activity monitor you'll see that our CPU is being used here I believe it's waiting for me to type something let's see hello Bob says how may I assist you today hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript function that'll find prime numbers this is a little bit different than the one it gave me earlier but what the heck let's try this out also notice that while that thing is generating there was the main process name and it was using a bunch of CPUs you can actually specify you can give the main program a flag I believe it's dash dash t for the number of threads to use and you can use more threads and it'll output faster that way let's go to our JavaScript project here it's a little node project I created to try this out and looks like we pasted right in there without any modifications and I'm going to run it oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well that did not work so as far as reliability of the code that it generates not exactly usable code it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does um it's printing out the function is prime generate primes it defined is prime let's copy this code paste it in here save it and run it and find prime numbers is not defined well it's because it's a different function name so we need to change that that's my fault and there it is are these all Primes it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code if you want to use llama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parameter model and it's not consistent and sometimes it gives simple answers that work and sometimes it gives simple programs that don't work use it for Creative tasks use it for writing tasks because for completion of sentences spreading poetry I don't think it's ready for code yet so hopefully this was helpful to you if you want to try this out on your own make sure you check the steps and the links and the commands that I pasted Down Below in the description thanks for watching and I'll see you next time\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
