{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mrzaizai2k/code_Bao/LLM-with-RAG/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(file_path)\n",
    "except:\n",
    "    file_path = os.path.abspath('')\n",
    "    os.chdir(os.path.dirname(file_path))\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_text_splitters import NLTKTextSplitter, SpacyTextSplitter\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import (BSHTMLLoader, \n",
    "                                                  DirectoryLoader, \n",
    "                                                  Docx2txtLoader, \n",
    "                                                  NewsURLLoader, \n",
    "                                                  PyPDFLoader, \n",
    "                                                  PyMuPDFLoader,\n",
    "                                                  MathpixPDFLoader,\n",
    "                                                  RecursiveUrlLoader, \n",
    "                                                  SeleniumURLLoader, \n",
    "                                                  TextLoader, \n",
    "                                                  UnstructuredHTMLLoader,\n",
    "                                                UnstructuredImageLoader,\n",
    "                                                UnstructuredPowerPointLoader, \n",
    "                                                UnstructuredURLLoader, \n",
    "                                                UnstructuredWordDocumentLoader, \n",
    "                                                YoutubeLoader)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter, NLTKTextSplitter, SpacyTextSplitter\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import NougatProcessor, VisionEncoderDecoderModel, pipeline\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = config_parser(data_config_path = 'config/data_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_url_list = data.get('single_url_list')\n",
    "loader = NewsURLLoader(urls=single_url_list, \n",
    "                    post_processors=[clean_extra_whitespace],)\n",
    "ori_text = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/web_data/TopTenAlgorithms.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "ori_text = loader.load()\n",
    "ori_text =[item for item in ori_text if len(item.page_content) >= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Discrete Random\\nVariable\\nNguyen Tien Thinh\\nDiscrete Random\\nVariable\\nMultivariate Random\\nVariable\\nExpectation and\\nVariance\\nProbability Distribution\\n(Draft).11\\nDiscrete Random Variable\\nTheorem\\nP\\nx pX(x) = 1.\\nProof.\\nIt is obvious.\\n', metadata={'source': 'data/web_data/Random variable.pdf', 'file_path': 'data/web_data/Random variable.pdf', 'page': 10, 'total_pages': 38, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyen Tien Thinh', 'subject': 'Lecturer Fundamental Mathematics for Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'pdfTeX-1.40.25', 'creationDate': \"D:20231106094014+07'00'\", 'modDate': \"D:20231106094014+07'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_text[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = NougatProcessor.from_pretrained(\"facebook/nougat-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-base\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "# prepare PDF image for the model\n",
    "filepath = \"data/web_data/image/slide1.jpg\"\n",
    "image = Image.open(filepath).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['**Quicksort\\'s Average Running Time Analysis**\\n\\n* In order to have a good \"average performance,\" one can _randomize_ this algorithm by assuming that each pivot is chosen at random.\\n* Let us compute the expectation of the number \\\\(X\\\\) of comparisons needed when running the randomized version of quicksort.\\n* Recall that the input is a sequence \\\\(S\\\\) = \\\\((x_{1},...,x_{n})\\\\) of distinct elements, and that \\\\((y_{1},...,y_{n})\\\\) has the same elements sorted in increasing order.\\n* In order to compute \\\\(E(X)\\\\), we decompose \\\\(X\\\\) as a sum of indicator variables \\\\(X_{i,j}\\\\), with \\\\(X_{i,j}\\\\) = 1 iff \\\\(y_{i}\\\\) and \\\\(y_{j}\\\\) are ever compared, and \\\\(X_{i,j}\\\\) = 0 otherwise.\\n* Then, it is clear that \\\\(X\\\\) = \\\\(\\\\sum_{j=2}^{n}\\\\sum_{i=1}^{j-1}X_{i,j}\\\\) and \\\\(E(X)\\\\) = \\\\(\\\\sum_{j=2}^{n}\\\\sum_{i=1}^{j-1}E(X_{i,j})\\\\).\\n* Furthermore, since \\\\(X_{i,j}\\\\) is an indicator variable, we have \\\\(E(X_{i,j})\\\\) = \\\\(P(y_{i}\\\\) and \\\\(y_{j}\\\\) are ever compared).']\n"
     ]
    }
   ],
   "source": [
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# generate transcription (here we only generate 30 tokens)\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    min_length=1,\n",
    "    max_new_tokens=1024,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(repr(sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (repr(sequence)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# recursive_url_list = data.get('recursive_url_list')\n",
    "\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=recursive_url_list[0], max_depth=2,\n",
    "#       extractor=lambda x: Soup(x, \"html.parser\").text,\n",
    "# )\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = Docx2txtLoader(\"web_data/sample4.docx\")\n",
    "# data = loader.load()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [\n",
    "# \"https://e.vnexpress.net/news/travel-guide/binh-phuoc-boasts-jungle-treks-charming-feasts-4673392.html\",\n",
    "# 'https://e.vnexpress.net/news/culture/vietnamese-comic-artist-wins-silver-at-int-l-manga-awards-4695364.html',\n",
    "# ]\n",
    "# loader = UnstructuredURLLoader(urls=urls, \n",
    "#                     post_processors=[clean_extra_whitespace],)\n",
    "# data = loader.load()\n",
    "# print(data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube_url_list = data.get('youtube_url_list')\n",
    "\n",
    "# loader = YoutubeLoader.from_youtube_url(\n",
    "#     youtube_url_list[0], add_video_info=False\n",
    "# )\n",
    "# data = loader.load()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "class NewsSummarizer:\n",
    "    def __init__(self, summarizer = pipeline(\"summarization\", \n",
    "                                             model=\"Falconsai/text_summarization\", \n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device = take_device()),\n",
    "                #  translator = GoogleTranslator(),\n",
    "                 chunk_overlap:str = 10,\n",
    "                 max_length:int=200, \n",
    "                 min_length:int=30,\n",
    "                 ):\n",
    "        self.summarizer = summarizer\n",
    "        # self.translator = translator\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = self.load_text_splitter()\n",
    "    \n",
    "    def load_text_splitter(self):\n",
    "        text_splitter =  NLTKTextSplitter(chunk_size=1000)\n",
    "        return text_splitter\n",
    "\n",
    "    def summary_text(self,text_chunks:list):\n",
    "        '''Summary short text'''\n",
    "        sum_text= f''\n",
    "        for model_output in self.summarizer(text_chunks, batch_size=8, \n",
    "                                            truncation=\"only_first\",):\n",
    "            text = model_output['summary_text']\n",
    "            sum_text += f'\\n{text}'\n",
    "        return sum_text\n",
    "    \n",
    "    def summary_news(self, news:str)->str:\n",
    "        # trans_news = self.translator.translate(text=news, to_lang='en')\n",
    "        text_chunks = self.text_splitter.split_text(news)\n",
    "        summary_text = self.summary_text(text_chunks)\n",
    "        # summary_text = self.translator.translate(text=summary_text, to_lang='vi')\n",
    "\n",
    "        return summary_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ori_text[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE TOP10 ALGORITHMS FROM \\nTHE 20TH CENTURY\\nAlex Townsend\\nCornell University\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ori_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP10 ALGORITHMS FROM \n",
      "THE 20TH CENTURY\n",
      "Alex Townsend\n",
      "Cornell University\n",
      "\n",
      "-------\n",
      "THE TOP10 LIST\n",
      "1946: The Metropolis Algorithm \n",
      "1947: Simplex Method \n",
      "1950: Krylov Subspace Method\n",
      "1951: The Decompositional Approach to Matrix Computations \n",
      "1957: The Fortran Optimizing Compiler\n",
      "1959: QR Algorithm \n",
      "1962: Quicksort \n",
      "1965: Fast Fourier Transform\n",
      "1977: Integer Relation Detection  \n",
      "1987: Fast Multipole Method\n",
      "Dantzig von Neumann Hestenes Householder\n",
      "Backus\n",
      "Hoare\n",
      "Greengard\n",
      "\n",
      "-------\n",
      "WHAT IS AN ALGORITHM?\n",
      "Deﬁnition:\n",
      "“An algorithm is a sequence of ﬁnite computational steps that \n",
      "transforms an input into an output” [Cormen and Leiserson, 2009]\n",
      "Set of instructions\n",
      "Making tea\n",
      "Baking a cake\n",
      "Recipe\n",
      "Finite\n",
      "while(1), end\n",
      "\n",
      "-------\n",
      "NUMERICAL ANALYSIS\n",
      "“The study and development of algorithms that use numerical approximation”\n",
      "How many of the top 10 algorithms are in numerical analysis?\n",
      "Potentially all of them\n",
      "Algorithms implemented in ﬂoating point arithmetic are studied and \n",
      "developed by numerical analysts \n",
      "A deﬁnition\n",
      "Floating point arithmetic\n",
      "1/3 ⇡\n",
      "1/3 ⇡(−1)s\n",
      " \n",
      "1 +\n",
      "52\n",
      "X\n",
      "i=1\n",
      "b52−i2−i\n",
      "!\n",
      "⇥2e−1023\n",
      "\n",
      "-------\n",
      "OVERVIEW OF TALK\n",
      "A top 10 algorithm\n",
      "How it works?\n",
      "How do I use it?\n",
      "Open problem\n",
      "\n",
      "-------\n",
      "1959: QR ALGORITHM\n",
      "The Tacoma Narrows \n",
      "bridge in Nov 1940\n",
      "Collapsed in 80km/h \n",
      "winds\n",
      "\n",
      "-------\n",
      "NUMERICAL SIMULATIONS\n",
      "Resonant frequencies are eigenvalues: Av = xv\n",
      "Eigenvalue\n",
      "Eigenvector\n",
      "v 6= 0\n",
      "\n",
      "-------\n",
      "HOW DOES IT WORK?\n",
      "Francis\n",
      "for k = 1,2,...\n",
      "A = Q*R\n",
      "A = R*Q\n",
      "end\n",
      "=\n",
      "A\n",
      "Q\n",
      "R\n",
      "The ﬁnal diagonal matrix contains all the eigenvalues\n",
      "A = symmetric\n",
      "\n",
      "-------\n",
      "HOW DO I USE IT?\n",
      "A tiger’s tail\n",
      "Rootﬁnding and global optimization\n",
      "A\n",
      "characteristic \n",
      "polynomial of \n",
      "Matrix determinant\n",
      "Identity matrix\n",
      "p(x) = ±det (A −xI)\n",
      "\n",
      "-------\n",
      "OPEN PROBLEM\n",
      "Let p(x, y) be a degree (n, n) polynomial. Construct n ⇥n matrices A, B, and\n",
      "C such that\n",
      "p(x, y) = det(A + xB + yC).\n",
      "Need it to solve:\n",
      "p(x, y) = q(x, y) = 0\n",
      "Compare to:\n",
      "p(x) = ±det (A −xI)\n",
      "\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i, txt in enumerate(ori_text[:10]):\n",
    "    print(f\"{txt.page_content}\\n-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(texts):\n",
    "    for i, t in enumerate(texts):\n",
    "        print(f\"{texts[i]}\\n-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='HOW DO I USE IT?\\nAn automatic way to tell us how “complicated” a function is.\\nFFT\\n', metadata={'source': 'data/web_data/TopTenAlgorithms.pdf', 'file_path': 'data/web_data/TopTenAlgorithms.pdf', 'page': 12, 'total_pages': 20, 'format': 'PDF 1.4', 'title': 'TopTenAlgorithms.key', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Keynote', 'producer': 'Mac OS X 10.11.6 Quartz PDFContext', 'creationDate': \"D:20170325100036Z00'00'\", 'modDate': \"D:20170325100036Z00'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_text[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOW DO I USE IT?\n",
      "An automatic way to tell us how “complicated” a function is.\n",
      "FFT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ori_text[12].page_content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOW DO I USE IT?\n",
      "An automatic way to tell us how “complicated” a function is.\n",
      "FFT\n",
      "\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "token_text_splitter = TokenTextSplitter(chunk_size=150, chunk_overlap=10)\n",
    "token_texts = token_text_splitter.split_text(text)\n",
    "check(token_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOW DO I USE IT?\n",
      "\n",
      "An automatic way to tell us how “complicated” a function is.\n",
      "\n",
      "FFT\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "nltk_text_splitter = NLTKTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "nltk_texts = nltk_text_splitter.split_text(text)\n",
    "check(nltk_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_check(text_chunks):\n",
    "    sum_text = NewsSummarizer().summary_text(text_chunks)\n",
    "    print(sum_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FFT is an automatic way to tell us how “complicated” a function is . FFT has a simple function that can be accessed by a user .\n"
     ]
    }
   ],
   "source": [
    "sum_check(token_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FFT is an automatic way to tell us how “complicated” a function is . FFT has a simple function that can be accessed by a user .\n"
     ]
    }
   ],
   "source": [
    "sum_check(nltk_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
