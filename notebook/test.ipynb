{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mrzaizai2k/code_Bao/LLM-with-RAG/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(file_path)\n",
    "except:\n",
    "    file_path = os.path.abspath('')\n",
    "    os.chdir(os.path.dirname(file_path))\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_text_splitters import NLTKTextSplitter, SpacyTextSplitter\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import (BSHTMLLoader, \n",
    "                                                  DirectoryLoader, \n",
    "                                                  Docx2txtLoader, \n",
    "                                                  NewsURLLoader, \n",
    "                                                  PyPDFLoader, \n",
    "                                                  PyMuPDFLoader,\n",
    "                                                  MathpixPDFLoader,\n",
    "                                                  RecursiveUrlLoader, \n",
    "                                                  SeleniumURLLoader, \n",
    "                                                  TextLoader, \n",
    "                                                  UnstructuredHTMLLoader,\n",
    "                                                UnstructuredImageLoader,\n",
    "                                                UnstructuredPowerPointLoader, \n",
    "                                                UnstructuredURLLoader, \n",
    "                                                UnstructuredWordDocumentLoader, \n",
    "                                                YoutubeLoader)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter, NLTKTextSplitter, SpacyTextSplitter\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import NougatProcessor, VisionEncoderDecoderModel, pipeline\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = config_parser(data_config_path = 'config/data_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_url_list = data.get('single_url_list')\n",
    "loader = NewsURLLoader(urls=single_url_list, \n",
    "                    post_processors=[clean_extra_whitespace],)\n",
    "ori_text = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/web_data/Chapter 0-Course introduction.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "ori_text = loader.load()\n",
    "ori_text = combine_short_doc(ori_text, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='1987: THE FAST MULTIPOLE METHOD\\nIn 4 billion years time…\\n HOW DOES IT WORK?\\nGreengard\\nRokhlin\\n HOW DO I USE IT?\\nA\\n=\\n+\\n+\\n+ · · ·\\nFull rank form\\nLow rank form\\nOriginal\\nrank 1\\nrank 3\\nrank 10\\nrank 50\\nThe low rank format saves computational time and storage costs\\nThe SVD gives the best low rank approximations:\\n', metadata={'source': 'data/web_data/TopTenAlgorithms.pdf', 'file_path': 'data/web_data/TopTenAlgorithms.pdf', 'page': 16, 'total_pages': 20, 'format': 'PDF 1.4', 'title': 'TopTenAlgorithms.key', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Keynote', 'producer': 'Mac OS X 10.11.6 Quartz PDFContext', 'creationDate': \"D:20170325100036Z00'00'\", 'modDate': \"D:20170325100036Z00'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_text[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = NougatProcessor.from_pretrained(\"facebook/nougat-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-base\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "# prepare PDF image for the model\n",
    "filepath = \"data/web_data/image/slide1.jpg\"\n",
    "image = Image.open(filepath).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['**Quicksort\\'s Average Running Time Analysis**\\n\\n* In order to have a good \"average performance,\" one can _randomize_ this algorithm by assuming that each pivot is chosen at random.\\n* Let us compute the expectation of the number \\\\(X\\\\) of comparisons needed when running the randomized version of quicksort.\\n* Recall that the input is a sequence \\\\(S\\\\) = \\\\((x_{1},...,x_{n})\\\\) of distinct elements, and that \\\\((y_{1},...,y_{n})\\\\) has the same elements sorted in increasing order.\\n* In order to compute \\\\(E(X)\\\\), we decompose \\\\(X\\\\) as a sum of indicator variables \\\\(X_{i,j}\\\\), with \\\\(X_{i,j}\\\\) = 1 iff \\\\(y_{i}\\\\) and \\\\(y_{j}\\\\) are ever compared, and \\\\(X_{i,j}\\\\) = 0 otherwise.\\n* Then, it is clear that \\\\(X\\\\) = \\\\(\\\\sum_{j=2}^{n}\\\\sum_{i=1}^{j-1}X_{i,j}\\\\) and \\\\(E(X)\\\\) = \\\\(\\\\sum_{j=2}^{n}\\\\sum_{i=1}^{j-1}E(X_{i,j})\\\\).\\n* Furthermore, since \\\\(X_{i,j}\\\\) is an indicator variable, we have \\\\(E(X_{i,j})\\\\) = \\\\(P(y_{i}\\\\) and \\\\(y_{j}\\\\) are ever compared).']\n"
     ]
    }
   ],
   "source": [
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# generate transcription (here we only generate 30 tokens)\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    min_length=1,\n",
    "    max_new_tokens=1024,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(repr(sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (repr(sequence)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# recursive_url_list = data.get('recursive_url_list')\n",
    "\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=recursive_url_list[0], max_depth=2,\n",
    "#       extractor=lambda x: Soup(x, \"html.parser\").text,\n",
    "# )\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = Docx2txtLoader(\"web_data/sample4.docx\")\n",
    "# data = loader.load()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [\n",
    "# \"https://e.vnexpress.net/news/travel-guide/binh-phuoc-boasts-jungle-treks-charming-feasts-4673392.html\",\n",
    "# 'https://e.vnexpress.net/news/culture/vietnamese-comic-artist-wins-silver-at-int-l-manga-awards-4695364.html',\n",
    "# ]\n",
    "# loader = UnstructuredURLLoader(urls=urls, \n",
    "#                     post_processors=[clean_extra_whitespace],)\n",
    "# data = loader.load()\n",
    "# print(data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube_url_list = data.get('youtube_url_list')\n",
    "\n",
    "# loader = YoutubeLoader.from_youtube_url(\n",
    "#     youtube_url_list[0], add_video_info=False\n",
    "# )\n",
    "# data = loader.load()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "class NewsSummarizer:\n",
    "    def __init__(self, summarizer = pipeline(\"summarization\", \n",
    "                                             model=\"Falconsai/text_summarization\", \n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device = take_device()),\n",
    "                #  translator = GoogleTranslator(),\n",
    "                 chunk_overlap:str = 10,\n",
    "                 max_length:int=200, \n",
    "                 min_length:int=30,\n",
    "                 ):\n",
    "        self.summarizer = summarizer\n",
    "        # self.translator = translator\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = self.load_text_splitter()\n",
    "    \n",
    "    def load_text_splitter(self):\n",
    "        text_splitter =  NLTKTextSplitter(chunk_size=1000)\n",
    "        return text_splitter\n",
    "\n",
    "    def summary_text(self,text_chunks:list):\n",
    "        '''Summary short text'''\n",
    "        sum_text= f''\n",
    "        for model_output in self.summarizer(text_chunks, batch_size=8, \n",
    "                                            truncation=\"only_first\",):\n",
    "            text = model_output['summary_text']\n",
    "            sum_text += f'\\n{text}'\n",
    "        return sum_text\n",
    "    \n",
    "    def summary_news(self, news:str)->str:\n",
    "        # trans_news = self.translator.translate(text=news, to_lang='en')\n",
    "        text_chunks = self.text_splitter.split_text(news)\n",
    "        summary_text = self.summary_text(text_chunks)\n",
    "        # summary_text = self.translator.translate(text=summary_text, to_lang='vi')\n",
    "\n",
    "        return summary_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ori_text[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE TOP10 ALGORITHMS FROM \\nTHE 20TH CENTURY\\nAlex Townsend\\nCornell University\\n THE TOP10 LIST\\n1946: The Metropolis Algorithm \\n1947: Simplex Method \\n1950: Krylov Subspace Method\\n1951: The Decompositional Approach to Matrix Computations \\n1957: The Fortran Optimizing Compiler\\n1959: QR Algorithm \\n1962: Quicksort \\n1965: Fast Fourier Transform\\n1977: Integer Relation Detection  \\n1987: Fast Multipole Method\\nDantzig von Neumann Hestenes Householder\\nBackus\\nHoare\\nGreengard\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ori_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "DATA ENGINEERING\n",
      "COURSE OVERVIEW\n",
      "Trong Nhan Phan, PhD\n",
      " CONTACT\n",
      "❑Lecturer: Phan Trọng Nhân, PhD\n",
      "❑Faculty: Computer Science and Engineering\n",
      "❑Department: Information Systems\n",
      "❑Email: nhanpt@hcmut.edu.vn\n",
      "❑Course site: LMS\n",
      "2\n",
      "\n",
      "-------\n",
      "COURSE INTRODUCTION\n",
      "❑Subject: Data Engineering\n",
      "❑Number: 055240\n",
      "❑Credit: 3\n",
      "❑Period: 70.5\n",
      "❑Prerequisite: None\n",
      "❑Class Periods: 13 weeks\n",
      "❑Course Syllabus\n",
      "3\n",
      "\n",
      "-------\n",
      "COURSE AIMS\n",
      "◼Database analysis and design methodologies\n",
      "including\n",
      "relational\n",
      "data\n",
      "model\n",
      "and\n",
      "NoSQL\n",
      "models.\n",
      "◼Advances in data storage and retrieval methods,\n",
      "flexible query answering, query processing and\n",
      "optimization, transaction processing.\n",
      "◼Object-oriented and time series databases, as\n",
      "well as big data and applications.\n",
      "◼Research topics relevant to modern databases\n",
      "and applications will also be introduced and\n",
      "discussed.\n",
      "4\n",
      "\n",
      "-------\n",
      "REFERENCES\n",
      "[1]\n",
      "R.\n",
      "Elmasri,\n",
      "S.B.\n",
      "Navathe:\n",
      "“Fundamentals\n",
      "of\n",
      "Database\n",
      "Systems”, 7th Edition, Pearson Addison-Wesley, 2016.\n",
      "[2]\n",
      "P.A.\n",
      "Bernstein,\n",
      "E.\n",
      "Newcomer:\n",
      "\"Principles\n",
      "of\n",
      "Transaction\n",
      "Processing\", 2nd Edition, Elsevier Inc., 2009.\n",
      "[3] S. Lightstone, T. Teorey, T. Nadeau: \"Physical Database Design\",\n",
      "Elsevier Inc., 2007.\n",
      "[4] R. Kimball, M. Ross, \"The Data Warehouse ToolKit\", 3rd\n",
      "Edition, Wiley Publishing, Inc., 2013.\n",
      "[5] J. Hurwitz, A. Nugent, F. Halper, M. Kaufman: \"Big Data for\n",
      "Dummies\", John Wiley & Sons Inc., 2013.\n",
      "[6]\n",
      "Jiawei\n",
      "Han,\n",
      "Micheline\n",
      "Kamber,\n",
      "Jian\n",
      "Pei,\n",
      "“Data\n",
      "Mining:\n",
      "Concepts and Techniques”, Third Edition, Morgan Kaufmann\n",
      "Publishers, 2012.\n",
      "[7] Jure Leskovec, Anand Rajaraman, Jeffrey D. Ullman: Mining of\n",
      "Massive Datasets, 2014.\n",
      "5\n",
      "\n",
      "-------\n",
      "OUTLINE\n",
      "❑Chapter 1: Introduction and review\n",
      "❑Chapter 2: Database tuning methodologies\n",
      "❑Chapter 3: Data indexing and query optimization\n",
      "❑Chapter 4: Non-relation databases\n",
      "❑Chapter 5: Data streaming\n",
      "❑Chapter 6: Big data and applications\n",
      "❑Chapter 7: Data warehouse\n",
      "6\n",
      "\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i, txt in enumerate(ori_text[:5]):\n",
    "    print(f\"{txt.page_content}\\n-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(texts):\n",
    "    for i, t in enumerate(texts):\n",
    "        print(f\"{texts[i]}\\n-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='HOW DO I USE IT?\\nAn automatic way to tell us how “complicated” a function is.\\nFFT\\n', metadata={'source': 'data/web_data/TopTenAlgorithms.pdf', 'file_path': 'data/web_data/TopTenAlgorithms.pdf', 'page': 12, 'total_pages': 20, 'format': 'PDF 1.4', 'title': 'TopTenAlgorithms.key', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Keynote', 'producer': 'Mac OS X 10.11.6 Quartz PDFContext', 'creationDate': \"D:20170325100036Z00'00'\", 'modDate': \"D:20170325100036Z00'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_text[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOW DO I USE IT?\n",
      "An automatic way to tell us how “complicated” a function is.\n",
      "FFT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ori_text[12].page_content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOW DO I USE IT?\n",
      "An automatic way to tell us how “complicated” a function is.\n",
      "FFT\n",
      "\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "token_text_splitter = TokenTextSplitter(chunk_size=150, chunk_overlap=10)\n",
    "token_texts = token_text_splitter.split_text(text)\n",
    "check(token_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOW DO I USE IT?\n",
      "\n",
      "An automatic way to tell us how “complicated” a function is.\n",
      "\n",
      "FFT\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "nltk_text_splitter = NLTKTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "nltk_texts = nltk_text_splitter.split_text(text)\n",
    "check(nltk_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_check(text_chunks):\n",
    "    sum_text = NewsSummarizer().summary_text(text_chunks)\n",
    "    print(sum_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FFT is an automatic way to tell us how “complicated” a function is . FFT has a simple function that can be accessed by a user .\n"
     ]
    }
   ],
   "source": [
    "sum_check(token_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FFT is an automatic way to tell us how “complicated” a function is . FFT has a simple function that can be accessed by a user .\n"
     ]
    }
   ],
   "source": [
    "sum_check(nltk_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
