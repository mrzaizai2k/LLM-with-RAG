{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/code_Bao/LLM-with-RAG/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(file_path)\n",
    "except:\n",
    "    file_path = os.path.abspath('')\n",
    "    os.chdir(os.path.dirname(file_path))\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    DirectoryLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    BSHTMLLoader,\n",
    "    Docx2txtLoader,\n",
    "    TextLoader,\n",
    "    SeleniumURLLoader,\n",
    "    YoutubeLoader,\n",
    "    UnstructuredURLLoader,\n",
    ")\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "import torch\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = Docx2txtLoader(\"web_data/sample4.docx\")\n",
    "# data = loader.load()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [\n",
    "# \"https://e.vnexpress.net/news/travel-guide/binh-phuoc-boasts-jungle-treks-charming-feasts-4673392.html\",\n",
    "# 'https://e.vnexpress.net/news/culture/vietnamese-comic-artist-wins-silver-at-int-l-manga-awards-4695364.html',\n",
    "# ]\n",
    "# loader = UnstructuredURLLoader(urls=urls, \n",
    "#                     post_processors=[clean_extra_whitespace],)\n",
    "# data = loader.load()\n",
    "# print(data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"what is this that is not prime numbers I know 49 is not a prime number I know that 9 is not a prime number yet llama thinks that this function actually finds prime numbers accuracy aside you might want to run llama on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can actually take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million unless you're Apple or Google okay how do we run llama locally on an Apple silicon machine that's what I'm going to show you today if you don't have an apple silicon machine running llamas easy you go to the GitHub URL then I'm gonna leave all the links down below including the instructions and the steps you're gonna go to this one Facebook research slash Lama on GitHub you're going to clone this repository follow the instructions and boom you're done that's if you're running on Linux or Windows but this is not going to work on Apple silicon there is a C plus plus project that's also open source and that one will now you're gonna need a few things for this and again I'm gonna put all the steps down below you will need to clone this repository this Facebook research llama Repository I've created a folder called llama2 and I'm going to git clone this repository I'm going to do this along with you so you can do it along with me there we go clone that's this one right here llama now we're going to go to this other repository and this is the C plus Port this port is done by the same person that created whisper and I've done videos on whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker it supports all these models llama alpaca and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha with this thing is that you need to convert your model from the original to a different model structure and I'm going to show you how to do that don't worry so let's clone this one too git clone and boom done that's two out of the three tasks already done holy cow what's the third task well if you go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you're gonna need to click on this link that says request and new download link and I will link to this link in the description also when you go here you're going to need to fill in a little bit of information for MATA to keep track of you accept the terms and click on this accept and continue at which point they will approve you and a couple hours later at least it was a couple hours for me they sent me a link and the link looks like this this is the email I got from them this link is not going to work anymore because it's only good for 24 hours you just basically use this very special and very short-lived link and that'll allow you to download all these models how do you download the models well this repository right here the original one will tell you how to do it first of all there's this download.sh file here the email actually tells you the instructions you visit the repository you download it and you run this download.sh script when you run that script it's going to ask you for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a checklist.chk file and then a bunch of these pth files these are the actual models but still all the other files are important as well as the params.json and when you do the download you're also going to get the tokenizer checklist check file and the tokenizer model file make sure you copy those somewhere safe as well so I put them here in my models folder now once these download and it took me a long time to download these and I have fast internet so maybe you want to start out with a 7 billion parameter model and just get your feet wet and then download the bigger ones later which is what I'm going to show you right now I'm going to use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here we're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it and it's going to build a bunch of binaries but the two most important ones the ones that we're gonna use Let's do an LS here it's going to be quantize and Main I'll show you how to use that momentarily you're still dependent on python because you're going to need to use a script in here called convert this script right here is a python script and this python script converts from the original models from The Source models into this new format that this program runs on so to run this python file we're going to need to install some python dependencies now you might or you might not already have python installed I do I have 3.9 this could work I could just install the requirements by by the way that's this file right here requirements.txt that contains all the requirements for the python environment to have before you can run convert all these steps I know I know I actually will add a little bit more steps for my environment but you don't have to if you already have python a decent new version of python installed but what I like to do is I have a conda environment in Anaconda environment set up and I'll create a brand new environment just for this task and if you don't know what I'm talking about I made a video on how to create and manage conda environments these are python environments these keep your python projects separate so they don't interfere with each other with different version numbers and all that I created that video and I'll link to that video down below if you don't know how to do that so I'm gonna do that real quick right now conda create and I'm going to give this environment a name llama2 I'm going to activate the environment using this command con to activate llama2 and now I'm going to install the dependencies into this environment well first of all let's check the version of python here I have 2.7 that's not gonna work we need a newer version of python so I'm going to say conda install python equals 3.11 and now inside this llama2 environment I'm gonna have a version a new version of python python 311 just for this environment if I check my python version 3.11 so now I can use this new version of python to install the dependencies which is this command python pip install Dash R requirements Boom the requirements installed good to go now that our python environment is set up and our python requirements are installed we can run this convert dot Pi script so I'm going to paste it in here I'm still in my content environment notice in the parentheses it says llama2 that's the name of my environment python convert and then I'm going to give it the output file which is in the models folder 7B and then the name of the model I'll type F16 and then where can I find the original model let's go through that real quick shall we I'm going to open up my finder here and we can take a look at where everything is first of all I'm inside my llama.cpp folder all right which is right here everything's inside my llama2 folder including the llama.cpp I'm running this convert script and I'm telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion parameter models and the 70 billion parameter models we're just gonna do this with one of the seven billion one so that's gonna be the output file it's going to go into models 7B and then it's going to be called ggml Dash model Dash F16 dot bin and where are we getting the original files from the original models well that's coming from llama 2 meta models llama 2 7B chat so this folder right here remember these are the ones that I downloaded from meta it's going to look into this folder and basically pick out the one it needs and process it so let's run that there it is it's running it okay so it's doing the conversion right now this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin it's just a different format but now this file is huge it's 13.48 gigabytes just for that model and if you get to the 13 billion parameter model is 26 and the 70 billion parameter model is even bigger it's what eight files that are 17.25 gigabytes each that's huge so there's another step and that's to quantize these models into smaller file size and we're going to use the quantize command remember when we did make we built Main and we built quantize now we're going to use quantize to convert this model that we've converted to quantize this model that we've converted sorry I didn't come up with this stuff okay I'm just a messenger don't kill me I'm gonna paste this command in here again commands are down there quantize models 7B this is the one right here the one we just converted we're pointing to that and we're gonna put the new one in the same folder and this is going to be the quantized version so let's run that and there it is the Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do our uh inference finally it's time for the inference to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I think it's the number of tokens correct me if I'm wrong I think it's the number of tokens that we can respond with I'm gonna set that to I don't know let's do 10 24. and doesn't have to be you can pick whatever number you want let's try this I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is but it's doing it there it is that was generated by the 7 billion parameter model and I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all and you also have this little prompts folder here well this has some examples we can send one of these files to the model as an example or as a starting prompt for example this chat with bob.txt there's a little bit of a primer and a little bit of a start of the dialogue uh part of the prompt and the understanding is that the model will continue or prompt the user to continue let's try this out I'm gonna paste a new command here and some of this will be familiar main the model and then there is something called repeat penalty 1.0 I have not looked into that I don't know what that is but it's in one of the examples in the repository if you want to read through the repository which you should probably do that it might explain it then Color dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here so I'm gonna go ahead and run that now this right now is running on the CPU how do I know that if I pop open activity monitor you'll see that our CPU is being used here I believe it's waiting for me to type something let's see hello Bob says how may I assist you today hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript function that'll find prime numbers this is a little bit different than the one it gave me earlier but what the heck let's try this out also notice that while that thing is generating there was the main process name and it was using a bunch of CPUs you can actually specify you can give the main program a flag I believe it's dash dash t for the number of threads to use and you can use more threads and it'll output faster that way let's go to our JavaScript project here it's a little node project I created to try this out and looks like we pasted right in there without any modifications and I'm going to run it oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well that did not work so as far as reliability of the code that it generates not exactly usable code it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does um it's printing out the function is prime generate primes it defined is prime let's copy this code paste it in here save it and run it and find prime numbers is not defined well it's because it's a different function name so we need to change that that's my fault and there it is are these all Primes it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code if you want to use llama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parameter model and it's not consistent and sometimes it gives simple answers that work and sometimes it gives simple programs that don't work use it for Creative tasks use it for writing tasks because for completion of sentences spreading poetry I don't think it's ready for code yet so hopefully this was helpful to you if you want to try this out on your own make sure you check the steps and the links and the commands that I pasted Down Below in the description thanks for watching and I'll see you next time\", metadata={'source': 'TsVZJbnnaSs'})]\n"
     ]
    }
   ],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=TsVZJbnnaSs\", add_video_info=False\n",
    ")\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/code_Bao/LLM-with-RAG/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "class NewsSummarizer:\n",
    "    def __init__(self, summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\"),\n",
    "                 max_length:int=230, \n",
    "                 min_length:int=30,\n",
    "                 ):\n",
    "        self.summarizer = summarizer\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        \n",
    "    def summary_text(self,text:str)->str:\n",
    "        '''Summary short text'''\n",
    "        text = self.summarizer(text, max_length=self.max_length, min_length=self.min_length, do_sample=False)[0]['summary_text']\n",
    "        return text\n",
    "    \n",
    "    def summary_news(self, news:str)->str:\n",
    "        '''\n",
    "        Summary news. Because the news is too long (Loss of Information) so I will split news into 2 parts\n",
    "        '''\n",
    "        sample = news.split('. ')\n",
    "        art1 = '. '.join(sample[:int(len(sample)/2)])\n",
    "        art2 = '. '.join(sample[int(len(sample)/2):])\n",
    "        sum_art1 = self.summary_text(art1)\n",
    "        sum_art2 = self.summary_text(art2)\n",
    "        summary_text = f'   {sum_art1}\\n    {sum_art2}'\n",
    "        return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is this that is not prime numbers I know 49 is not a prime number I know that 9 is not a prime number yet llama thinks that this function actually finds prime numbers accuracy aside you might want to run llama on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can actually take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million unless you're Apple or Google okay how do we run llama locally on an Apple silicon machine that's what I'm going to show you today if you don't have an apple silicon machine running llamas easy you go to the GitHub URL then I'm gonna leave all the links down below including the instructions and the steps you're gonna go to this one Facebook research slash Lama on GitHub you're going to clone this repository follow the instructions and boom you're done that's if you're running on Linux or Windows but this is not going to work on Apple silicon there is a C plus plus project that's also open source and that one will now you're gonna need a few things for this and again I'm gonna put all the steps down below you will need to clone this repository this Facebook research llama Repository I've created a folder called llama2 and I'm going to git clone this repository I'm going to do this along with you so you can do it along with me there we go clone that's this one right here llama now we're going to go to this other repository and this is the C plus Port this port is done by the same person that created whisper and I've done videos on whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker it supports all these models llama alpaca and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha with this thing is that you need to convert your model from the original to a different model structure and I'm going to show you how to do that don't worry so let's clone this one too git clone and boom done that's two out of the three tasks already done holy cow what's the third task well if you go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you're gonna need to click on this link that says request and new download link and I will link to this link in the description also when you go here you're going to need to fill in a little bit of information for MATA to keep track of you accept the terms and click on this accept and continue at which point they will approve you and a couple hours later at least it was a couple hours for me they sent me a link and the link looks like this this is the email I got from them this link is not going to work anymore because it's only good for 24 hours you just basically use this very special and very short-lived link and that'll allow you to download all these models how do you download the models well this repository right here the original one will tell you how to do it first of all there's this download.sh file here the email actually tells you the instructions you visit the repository you download it and you run this download.sh script when you run that script it's going to ask you for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a checklist.chk file and then a bunch of these pth files these are the actual models but still all the other files are important as well as the params.json and when you do the download you're also going to get the tokenizer checklist check file and the tokenizer model file make sure you copy those somewhere safe as well so I put them here in my models folder now once these download and it took me a long time to download these and I have fast internet so maybe you want to start out with a 7 billion parameter model and just get your feet wet and then download the bigger ones later which is what I'm going to show you right now I'm going to use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here we're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it and it's going to build a bunch of binaries but the two most important ones the ones that we're gonna use Let's do an LS here it's going to be quantize and Main I'll show you how to use that momentarily you're still dependent on python because you're going to need to use a script in here called convert this script right here is a python script and this python script converts from the original models from The Source models into this new format that this program runs on so to run this python file we're going to need to install some python dependencies now you might or you might not already have python installed I do I have 3.9 this could work I could just install the requirements by by the way that's this file right here requirements.txt that contains all the requirements for the python environment to have before you can run convert all these steps I know I know I actually will add a little bit more steps for my environment but you don't have to if you already have python a decent new version of python installed but what I like to do is I have a conda environment in Anaconda environment set up and I'll create a brand new environment just for this task and if you don't know what I'm talking about I made a video on how to create and manage conda environments these are python environments these keep your python projects separate so they don't interfere with each other with different version numbers and all that I created that video and I'll link to that video down below if you don't know how to do that so I'm gonna do that real quick right now conda create and I'm going to give this environment a name llama2 I'm going to activate the environment using this command con to activate llama2 and now I'm going to install the dependencies into this environment well first of all let's check the version of python here I have 2.7 that's not gonna work we need a newer version of python so I'm going to say conda install python equals 3.11 and now inside this llama2 environment I'm gonna have a version a new version of python python 311 just for this environment if I check my python version 3.11 so now I can use this new version of python to install the dependencies which is this command python pip install Dash R requirements Boom the requirements installed good to go now that our python environment is set up and our python requirements are installed we can run this convert dot Pi script so I'm going to paste it in here I'm still in my content environment notice in the parentheses it says llama2 that's the name of my environment python convert and then I'm going to give it the output file which is in the models folder 7B and then the name of the model I'll type F16 and then where can I find the original model let's go through that real quick shall we I'm going to open up my finder here and we can take a look at where everything is first of all I'm inside my llama.cpp folder all right which is right here everything's inside my llama2 folder including the llama.cpp I'm running this convert script and I'm telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion parameter models and the 70 billion parameter models we're just gonna do this with one of the seven billion one so that's gonna be the output file it's going to go into models 7B and then it's going to be called ggml Dash model Dash F16 dot bin and where are we getting the original files from the original models well that's coming from llama 2 meta models llama 2 7B chat so this folder right here remember these are the ones that I downloaded from meta it's going to look into this folder and basically pick out the one it needs and process it so let's run that there it is it's running it okay so it's doing the conversion right now this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin it's just a different format but now this file is huge it's 13.48 gigabytes just for that model and if you get to the 13 billion parameter model is 26 and the 70 billion parameter model is even bigger it's what eight files that are 17.25 gigabytes each that's huge so there's another step and that's to quantize these models into smaller file size and we're going to use the quantize command remember when we did make we built Main and we built quantize now we're going to use quantize to convert this model that we've converted to quantize this model that we've converted sorry I didn't come up with this stuff okay I'm just a messenger don't kill me I'm gonna paste this command in here again commands are down there quantize models 7B this is the one right here the one we just converted we're pointing to that and we're gonna put the new one in the same folder and this is going to be the quantized version so let's run that and there it is the Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do our uh inference finally it's time for the inference to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I think it's the number of tokens correct me if I'm wrong I think it's the number of tokens that we can respond with I'm gonna set that to I don't know let's do 10 24. and doesn't have to be you can pick whatever number you want let's try this I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is but it's doing it there it is that was generated by the 7 billion parameter model and I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all and you also have this little prompts folder here well this has some examples we can send one of these files to the model as an example or as a starting prompt for example this chat with bob.txt there's a little bit of a primer and a little bit of a start of the dialogue uh part of the prompt and the understanding is that the model will continue or prompt the user to continue let's try this out I'm gonna paste a new command here and some of this will be familiar main the model and then there is something called repeat penalty 1.0 I have not looked into that I don't know what that is but it's in one of the examples in the repository if you want to read through the repository which you should probably do that it might explain it then Color dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here so I'm gonna go ahead and run that now this right now is running on the CPU how do I know that if I pop open activity monitor you'll see that our CPU is being used here I believe it's waiting for me to type something let's see hello Bob says how may I assist you today hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript function that'll find prime numbers this is a little bit different than the one it gave me earlier but what the heck let's try this out also notice that while that thing is generating there was the main process name and it was using a bunch of CPUs you can actually specify you can give the main program a flag I believe it's dash dash t for the number of threads to use and you can use more threads and it'll output faster that way let's go to our JavaScript project here it's a little node project I created to try this out and looks like we pasted right in there without any modifications and I'm going to run it oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well that did not work so as far as reliability of the code that it generates not exactly usable code it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does um it's printing out the function is prime generate primes it defined is prime let's copy this code paste it in here save it and run it and find prime numbers is not defined well it's because it's a different function name so we need to change that that's my fault and there it is are these all Primes it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code if you want to use llama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parameter model and it's not consistent and sometimes it gives simple answers that work and sometimes it gives simple programs that don't work use it for Creative tasks use it for writing tasks because for completion of sentences spreading poetry I don't think it's ready for code yet so hopefully this was helpful to you if you want to try this out on your own make sure you check the steps and the links and the commands that I pasted Down Below in the description thanks for watching and I'll see you next time\n"
     ]
    }
   ],
   "source": [
    "text = data[0].page_content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is this that is not prime numbers I know 49 is not a prime number I know that 9 is not a prime number yet llama thinks that this function actually finds prime numbers accuracy aside you might want to run llama on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can actually take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million unless you're Apple or Google okay how do we run llama locally on an Apple silicon machine that's what I'm going to show you today if you don't have an apple silicon machine running llamas easy you go to the GitHub URL then I'm gonna leave all the links down below including the instructions and the steps you're gonna go to this one Facebook research slash Lama on GitHub you're going to clone this repository follow the instructions and boom you're done that's if you're running on Linux or Windows but this is not going to work on Apple silicon there is a C plus plus project that's also open source and\n"
     ]
    }
   ],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=230, chunk_overlap=10)\n",
    "texts = text_splitter.split_text(text)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"what is this that is not prime numbers I know 49 is not a prime number I know that 9 is not a prime number yet llama thinks that this function actually finds prime numbers accuracy aside you might want to run llama on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can actually take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million unless you're Apple or Google okay how do we run llama locally on an Apple silicon machine that's what I'm going to show you today if you don't have an apple silicon machine running llamas easy you go to the GitHub URL then I'm gonna leave all the links down below including the instructions and the steps you're gonna go to this one Facebook research slash Lama on GitHub you're going to clone this repository follow the instructions and boom you're done that's if you're running on Linux or Windows but this is not going to work on Apple silicon there is a C plus plus project that's also open source and\",\n",
       " \" C plus plus project that's also open source and that one will now you're gonna need a few things for this and again I'm gonna put all the steps down below you will need to clone this repository this Facebook research llama Repository I've created a folder called llama2 and I'm going to git clone this repository I'm going to do this along with you so you can do it along with me there we go clone that's this one right here llama now we're going to go to this other repository and this is the C plus Port this port is done by the same person that created whisper and I've done videos on whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker it supports all these models llama alpaca and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha with this thing is that you need to convert your model from the original to a different model structure and I'm going to show you how to do that don't worry so let's clone this one too git clone and boom done that's two out of the three tasks already done\",\n",
       " \" that's two out of the three tasks already done holy cow what's the third task well if you go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you're gonna need to click on this link that says request and new download link and I will link to this link in the description also when you go here you're going to need to fill in a little bit of information for MATA to keep track of you accept the terms and click on this accept and continue at which point they will approve you and a couple hours later at least it was a couple hours for me they sent me a link and the link looks like this this is the email I got from them this link is not going to work anymore because it's only good for 24 hours you just basically use this very special and very short-lived link and that'll allow you to download all these models how do you download the models well this repository right here the original one will tell you how to do it first of all there's this download.sh file here the email actually tells you the instructions you visit the repository you download\",\n",
       " \" tells you the instructions you visit the repository you download it and you run this download.sh script when you run that script it's going to ask you for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a checklist.chk file and then a bunch of these pth files these are the actual models but still all the other files are important as well as the params.json and when you do the download you're also going to get the tokenizer checklist check file and the tokenizer model file make sure you copy those somewhere safe as well so I put them here in my models folder now once these download and it took me a long time to download these and I have fast internet\",\n",
       " \" long time to download these and I have fast internet so maybe you want to start out with a 7 billion parameter model and just get your feet wet and then download the bigger ones later which is what I'm going to show you right now I'm going to use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here we're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it and it's going to build a bunch of binaries but the two most important ones the ones that we're gonna use Let's do an LS here it's going to be quantize and Main I'll show you how to use that momentarily you're still dependent on python because you're going to need to use a script in here called convert this script right here is a python script and this python script converts from the original models from The Source models into this new format that this program runs on so to run this python file we're going to need to install some\",\n",
       " \" python file we're going to need to install some python dependencies now you might or you might not already have python installed I do I have 3.9 this could work I could just install the requirements by by the way that's this file right here requirements.txt that contains all the requirements for the python environment to have before you can run convert all these steps I know I know I actually will add a little bit more steps for my environment but you don't have to if you already have python a decent new version of python installed but what I like to do is I have a conda environment in Anaconda environment set up and I'll create a brand new environment just for this task and if you don't know what I'm talking about I made a video on how to create and manage conda environments these are python environments these keep your python projects separate so they don't interfere with each other with different version numbers and all that I created that video and I'll link to that video down below if you don't know how to do that so I'm gonna do that real quick right now conda create and I'm going to give this environment a\",\n",
       " \" create and I'm going to give this environment a name llama2 I'm going to activate the environment using this command con to activate llama2 and now I'm going to install the dependencies into this environment well first of all let's check the version of python here I have 2.7 that's not gonna work we need a newer version of python so I'm going to say conda install python equals 3.11 and now inside this llama2 environment I'm gonna have a version a new version of python python 311 just for this environment if I check my python version 3.11 so now I can use this new version of python to install the dependencies which is this command python pip install Dash R requirements Boom the requirements installed good to go now that our python environment is set up and our python requirements are installed we can run this convert dot Pi script so I'm going to paste it in here I'm still in my content environment notice in the parentheses it says llama2 that's the name of my environment python convert and then I'm going to give it the output file which is in the models folder 7B and then the name of\",\n",
       " \" the models folder 7B and then the name of the model I'll type F16 and then where can I find the original model let's go through that real quick shall we I'm going to open up my finder here and we can take a look at where everything is first of all I'm inside my llama.cpp folder all right which is right here everything's inside my llama2 folder including the llama.cpp I'm running this convert script and I'm telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion parameter models and the 70 billion parameter models we're just gonna do this with one of the seven billion one so that's gonna be the output file it's going to go into models\",\n",
       " \" the output file it's going to go into models 7B and then it's going to be called ggml Dash model Dash F16 dot bin and where are we getting the original files from the original models well that's coming from llama 2 meta models llama 2 7B chat so this folder right here remember these are the ones that I downloaded from meta it's going to look into this folder and basically pick out the one it needs and process it so let's run that there it is it's running it okay so it's doing the conversion right now this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin it's just a different format but now this file is huge it's 13.48 gigabytes just for that model and if you get to the 13 billion parameter model is 26 and the 70 billion parameter model is even bigger it's what eight files that are 17.25 gigabytes each that's huge so there's another step and that's to quantize these models into smaller file size and we're going to use the quantize command remember when we did make\",\n",
       " \" use the quantize command remember when we did make we built Main and we built quantize now we're going to use quantize to convert this model that we've converted to quantize this model that we've converted sorry I didn't come up with this stuff okay I'm just a messenger don't kill me I'm gonna paste this command in here again commands are down there quantize models 7B this is the one right here the one we just converted we're pointing to that and we're gonna put the new one in the same folder and this is going to be the quantized version so let's run that and there it is the Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do our uh inference finally it's time for the inference to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I\",\n",
       " \" allow us to specify the number of tokens to I think it's the number of tokens correct me if I'm wrong I think it's the number of tokens that we can respond with I'm gonna set that to I don't know let's do 10 24. and doesn't have to be you can pick whatever number you want let's try this I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is but it's doing it there it is that was generated by the 7 billion parameter model and I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all and you also have this little prompts folder here well this has some examples we can send one of these files to the model as an example or as a starting prompt for example this chat with bob.txt there's a little bit\",\n",
       " \" chat with bob.txt there's a little bit of a primer and a little bit of a start of the dialogue uh part of the prompt and the understanding is that the model will continue or prompt the user to continue let's try this out I'm gonna paste a new command here and some of this will be familiar main the model and then there is something called repeat penalty 1.0 I have not looked into that I don't know what that is but it's in one of the examples in the repository if you want to read through the repository which you should probably do that it might explain it then Color dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here so I'm gonna go ahead and run that now this right now is running on the CPU how do I know that if I pop open activity monitor you'll see that our CPU is being used here I believe it's waiting for me to type something let's see hello Bob says how may I\",\n",
       " \" something let's see hello Bob says how may I assist you today hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript function that'll find prime numbers this is a little bit different than the one it gave me earlier but what the heck let's try this out also notice that while that thing is generating there was the main process name and it was using a bunch of CPUs you can actually specify you can give the main program a flag I believe it's dash dash t for the number of threads to use and you can use more threads and it'll output faster that way let's go to our JavaScript project here it's a little node project I created to try this out and looks like we pasted right in there without any modifications and I'm going to run it oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well that did not work so as far as reliability of the code that it generates not exactly usable code it's not like Chad GPT where you actually\",\n",
       " \" it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does um it's printing out the function is prime generate primes it defined is prime let's copy this code paste it in here save it and run it and find prime numbers is not defined well it's because it's a different function name so we need to change that that's my fault and there it is are these all Primes it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code if you want to use llama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parameter model and it's not consistent and sometimes it gives simple answers that work and sometimes it gives simple programs that don't work use it for Creative tasks use it for writing tasks because for completion of sentences spreading poetry I don't think it's ready for code yet so hopefully this was helpful to you if you want to try this out on\",\n",
       " \" to you if you want to try this out on your own make sure you check the steps and the links and the commands that I pasted Down Below in the description thanks for watching and I'll see you next time\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 212. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n",
      "Your max_length is set to 230, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 230, but your input_length is only 147. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    llama thinks that this function actually finds prime numbers accuracy aside you might want to run llamas on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million . I'm gonna leave all the links down below including the instructions . and the steps you'\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    I'm going to git clone this repository this Facebook research llama Repository I've created a folder called LLama2 . this is the C plus Port this port is done by the same person that created whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you'll need to click on this link that says request and new download link and I'll link to this link in the description also when you go here you will need to fill in a bit of information for MATA to keep track of you accept the terms and click on that accept and continue at which point they will approve you and a couple hours later . the link looks like this this is the\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    'm going to show you right now I'm gonna use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here We're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it . Main I'll show you how to use that momentarily you're still dependent on\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    I have a conda environment set up and I'll create a brand new environment just for this task . if you don't know what I'm talking about I made a video on how to create and manage python environments . I've created a .txt command con to activate llama2 and now I will install the dependencies into this environment .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    's not gonna work we need a newer version of python 311 so now I'm gonna have a version anew version . I'll check my pYthon version 3.11 and now I can use this new version to install the dependencies which is this command plython pip install Dash R requirements Boom the requirements installed good to go now that our environment is set up we can run this convert dot Pi script so we're going to paste it in\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    I'm running this convert script and telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion model and the 70\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin . now this file is huge it's 13.48 gigabytes just for that model and the 70 billion parameter model is even bigger . we're going to use quantize to convert this model that we've converted to quantize this model . I'm just a messenger don't kill me.\n",
      "   Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I don't know let's do 10 24 24 .\n",
      "    I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    Color Dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript Function that'll find prime numbers\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does . it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code .\n",
      "   : . :: ! !! .. .! ? !. & ! # !\n",
      "    ama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parametri model and it's not consistent and sometimes it gives simple answers that work . Creative tasks Use it for writing tasks because for completion of sentences spreading poetry I don't think it' is ready for code yet so hopefully this was helpful to you if you want to try this out on your own check the steps and the links and the commands that I pasted\n"
     ]
    }
   ],
   "source": [
    "sum_doc = []\n",
    "for txt in texts:\n",
    "    sum_text = NewsSummarizer().summary_news(txt)\n",
    "    sum_doc.append(sum_text)\n",
    "    \n",
    "sum_doc_text = '\\n'.join(sum_doc)\n",
    "print(sum_doc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   : . :: ! !! .. .! ? !. & ! # !\\n    llama thinks that this function actually finds prime numbers accuracy aside you might want to run llamas on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million . I'm gonna leave all the links down below including the instructions . and the steps you'\\n   : . :: ! !! .. .! ? !. & ! # !\\n    I'm going to git clone this repository this Facebook research llama Repository I've created a folder called LLama2 . this is the C plus Port this port is done by the same person that created whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you'll need to click on this link that says request and new download link and I'll link to this link in the description also when you go here you will need to fill in a bit of information for MATA to keep track of you accept the terms and click on that accept and continue at which point they will approve you and a couple hours later . the link looks like this this is the\\n   : . :: ! !! .. .! ? !. & ! # !\\n    for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a\\n   : . :: ! !! .. .! ? !. & ! # !\\n    'm going to show you right now I'm gonna use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here We're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it . Main I'll show you how to use that momentarily you're still dependent on\\n   : . :: ! !! .. .! ? !. & ! # !\\n    I have a conda environment set up and I'll create a brand new environment just for this task . if you don't know what I'm talking about I made a video on how to create and manage python environments . I've created a .txt command con to activate llama2 and now I will install the dependencies into this environment .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    's not gonna work we need a newer version of python 311 so now I'm gonna have a version anew version . I'll check my pYthon version 3.11 and now I can use this new version to install the dependencies which is this command plython pip install Dash R requirements Boom the requirements installed good to go now that our environment is set up we can run this convert dot Pi script so we're going to paste it in\\n   : . :: ! !! .. .! ? !. & ! # !\\n    I'm running this convert script and telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion model and the 70\\n   : . :: ! !! .. .! ? !. & ! # !\\n    this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin . now this file is huge it's 13.48 gigabytes just for that model and the 70 billion parameter model is even bigger . we're going to use quantize to convert this model that we've converted to quantize this model . I'm just a messenger don't kill me.\\n   Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I don't know let's do 10 24 24 .\\n    I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    Color Dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript Function that'll find prime numbers\\n   : . :: ! !! .. .! ? !. & ! # !\\n    oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does . it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code .\\n   : . :: ! !! .. .! ? !. & ! # !\\n    ama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parametri model and it's not consistent and sometimes it gives simple answers that work . Creative tasks Use it for writing tasks because for completion of sentences spreading poetry I don't think it' is ready for code yet so hopefully this was helpful to you if you want to try this out on your own check the steps and the links and the commands that I pasted\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   I'm going to clone this repository this Facebook research slash Lama on GitHub . I've created a folder called llama2 and you're going to go to this other repository . this is the C plus Port this is done by the same person that created whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker . if you go back to the original repository you are gonna need to scroll down a little bit and there's a\\n    I'm going to run this and it's spewing out all sorts of weird stuff some of it is in German wow yeah it even has some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all and you can send one of these files to the model as an example or as a starting prompt for example this chat with bob.txt or one of the other ones in\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"web_data/sample3.txt\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(str(sum_doc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what is this that is not prime numbers I know 49 is not a prime number I know that 9 is not a prime number yet llama thinks that this function actually finds prime numbers accuracy aside you might want to run llama on your system locally without all the restrictions that you're gonna get to put all these Services popping up possibly charging money because you can actually take the Llama models run it and charge people for it if you want to at least that's my understanding of the terms of service unless you have over 700 million unless you're Apple or Google okay how do we run llama locally on an Apple silicon machine that's what I'm going to show you today if you don't have an apple silicon machine running llamas easy you go to the GitHub URL then I'm gonna leave all the links down below including the instructions and the steps you're gonna go to this one Facebook research slash Lama on GitHub you're going to clone this repository follow the instructions and boom you're done that's if you're running on Linux or Windows but this is not going to work on Apple silicon there is a C plus plus project that's also open source and that one will now you're gonna need a few things for this and again I'm gonna put all the steps down below you will need to clone this repository this Facebook research llama Repository I've created a folder called llama2 and I'm going to git clone this repository I'm going to do this along with you so you can do it along with me there we go clone that's this one right here llama now we're going to go to this other repository and this is the C plus Port this port is done by the same person that created whisper and I've done videos on whisper and how to use that on the channel before this one supports Mac OS Linux Windows Docker it supports all these models llama alpaca and so on there's mirrors for python go node Ruby c-sharp Scala the gotcha with this thing is that you need to convert your model from the original to a different model structure and I'm going to show you how to do that don't worry so let's clone this one too git clone and boom done that's two out of the three tasks already done holy cow what's the third task well if you go back to the original repository you're gonna need to scroll down a little bit and there's a download section and here you're gonna need to click on this link that says request and new download link and I will link to this link in the description also when you go here you're going to need to fill in a little bit of information for MATA to keep track of you accept the terms and click on this accept and continue at which point they will approve you and a couple hours later at least it was a couple hours for me they sent me a link and the link looks like this this is the email I got from them this link is not going to work anymore because it's only good for 24 hours you just basically use this very special and very short-lived link and that'll allow you to download all these models how do you download the models well this repository right here the original one will tell you how to do it first of all there's this download.sh file here the email actually tells you the instructions you visit the repository you download it and you run this download.sh script when you run that script it's going to ask you for this URL put that URL in select Which models you want here all the models that are available there's the 7B varieties which are 7 billion parameter models 13B varieties and 70b variety how big is all this stuff well here's where I put all my models it's in meta underscore models I copied it into that folder so I have the originals and the total size of that folder is 354 gigabytes holy cow so I created this folder called meta models and I downloaded all the original models in there now each folder has a checklist.chk file and then a bunch of these pth files these are the actual models but still all the other files are important as well as the params.json and when you do the download you're also going to get the tokenizer checklist check file and the tokenizer model file make sure you copy those somewhere safe as well so I put them here in my models folder now once these download and it took me a long time to download these and I have fast internet so maybe you want to start out with a 7 billion parameter model and just get your feet wet and then download the bigger ones later which is what I'm going to show you right now I'm going to use the 7B model let's go into that llama.cpp directory and the CPP directory is that second repository I showed you the C plus version now that we're here we're going to build this C plus project all you do is type in make in that directory it's going to use Apple's uh clang and build it and it's going to build a bunch of binaries but the two most important ones the ones that we're gonna use Let's do an LS here it's going to be quantize and Main I'll show you how to use that momentarily you're still dependent on python because you're going to need to use a script in here called convert this script right here is a python script and this python script converts from the original models from The Source models into this new format that this program runs on so to run this python file we're going to need to install some python dependencies now you might or you might not already have python installed I do I have 3.9 this could work I could just install the requirements by by the way that's this file right here requirements.txt that contains all the requirements for the python environment to have before you can run convert all these steps I know I know I actually will add a little bit more steps for my environment but you don't have to if you already have python a decent new version of python installed but what I like to do is I have a conda environment in Anaconda environment set up and I'll create a brand new environment just for this task and if you don't know what I'm talking about I made a video on how to create and manage conda environments these are python environments these keep your python projects separate so they don't interfere with each other with different version numbers and all that I created that video and I'll link to that video down below if you don't know how to do that so I'm gonna do that real quick right now conda create and I'm going to give this environment a name llama2 I'm going to activate the environment using this command con to activate llama2 and now I'm going to install the dependencies into this environment well first of all let's check the version of python here I have 2.7 that's not gonna work we need a newer version of python so I'm going to say conda install python equals 3.11 and now inside this llama2 environment I'm gonna have a version a new version of python python 311 just for this environment if I check my python version 3.11 so now I can use this new version of python to install the dependencies which is this command python pip install Dash R requirements Boom the requirements installed good to go now that our python environment is set up and our python requirements are installed we can run this convert dot Pi script so I'm going to paste it in here I'm still in my content environment notice in the parentheses it says llama2 that's the name of my environment python convert and then I'm going to give it the output file which is in the models folder 7B and then the name of the model I'll type F16 and then where can I find the original model let's go through that real quick shall we I'm going to open up my finder here and we can take a look at where everything is first of all I'm inside my llama.cpp folder all right which is right here everything's inside my llama2 folder including the llama.cpp I'm running this convert script and I'm telling it that the output file should be in the models folder let's go on this side over here give us more space CPP um models well there's nothing in that folder we don't have a 7B directory in there okay so we need to create a new directory here called 7B and this is for the 7 billion dollar processed models did I save seven billion dollar what am I what am I thinking about here I meant 7 billion parameter models you can also create folders for the 13 billion parameter models and the 70 billion parameter models we're just gonna do this with one of the seven billion one so that's gonna be the output file it's going to go into models 7B and then it's going to be called ggml Dash model Dash F16 dot bin and where are we getting the original files from the original models well that's coming from llama 2 meta models llama 2 7B chat so this folder right here remember these are the ones that I downloaded from meta it's going to look into this folder and basically pick out the one it needs and process it so let's run that there it is it's running it okay so it's doing the conversion right now this python script is included and it converted this consolidated.00.pth into this one ggml model F16 bin it's just a different format but now this file is huge it's 13.48 gigabytes just for that model and if you get to the 13 billion parameter model is 26 and the 70 billion parameter model is even bigger it's what eight files that are 17.25 gigabytes each that's huge so there's another step and that's to quantize these models into smaller file size and we're going to use the quantize command remember when we did make we built Main and we built quantize now we're going to use quantize to convert this model that we've converted to quantize this model that we've converted sorry I didn't come up with this stuff okay I'm just a messenger don't kill me I'm gonna paste this command in here again commands are down there quantize models 7B this is the one right here the one we just converted we're pointing to that and we're gonna put the new one in the same folder and this is going to be the quantized version so let's run that and there it is the Q4 underscore zero dot bin and look how much smaller that is that's 3.83 gigabytes now instead of 13.48 this is the file we're actually going to use to do our chat to do our uh inference finally it's time for the inference to do that we run main is a program now we can give it the model as the parameter so Dash M and we point it to this quantized model in the 7B folder then dash n will allow us to specify the number of tokens to I think it's the number of tokens correct me if I'm wrong I think it's the number of tokens that we can respond with I'm gonna set that to I don't know let's do 10 24. and doesn't have to be you can pick whatever number you want let's try this I'm going to run this and it's spewing out all sorts of weird stuff yeah I have no idea what all that stuff is but it's doing it there it is that was generated by the 7 billion parameter model and I have no idea what it is if you run this again it's gonna spew all sorts of things some of it is in German wow yeah it even has some code in there some python code so crazy stuff crazy stuff how do we get it to do something useful if I open this up in vs code we can take a look at the code first of all and you also have this little prompts folder here well this has some examples we can send one of these files to the model as an example or as a starting prompt for example this chat with bob.txt there's a little bit of a primer and a little bit of a start of the dialogue uh part of the prompt and the understanding is that the model will continue or prompt the user to continue let's try this out I'm gonna paste a new command here and some of this will be familiar main the model and then there is something called repeat penalty 1.0 I have not looked into that I don't know what that is but it's in one of the examples in the repository if you want to read through the repository which you should probably do that it might explain it then Color dash I if you're using a terminal that supports colors this will allow you to support uh multiple colors one for user and one for the responses and then you can send it the file which is chat with bob.txt or one of the other ones in here so I'm gonna go ahead and run that now this right now is running on the CPU how do I know that if I pop open activity monitor you'll see that our CPU is being used here I believe it's waiting for me to type something let's see hello Bob says how may I assist you today hey it's working and here is where I'm gonna hit it with my question Paste can you write a JavaScript function to find prime number let's go of course here is the JavaScript function that'll find prime numbers this is a little bit different than the one it gave me earlier but what the heck let's try this out also notice that while that thing is generating there was the main process name and it was using a bunch of CPUs you can actually specify you can give the main program a flag I believe it's dash dash t for the number of threads to use and you can use more threads and it'll output faster that way let's go to our JavaScript project here it's a little node project I created to try this out and looks like we pasted right in there without any modifications and I'm going to run it oh is prime is not defined so it gave me a reference to a function that's completely not defined at all right well that did not work so as far as reliability of the code that it generates not exactly usable code it's not like Chad GPT where you actually get usable code if I go to chatgpt and ask it that same question well let's see what it does um it's printing out the function is prime generate primes it defined is prime let's copy this code paste it in here save it and run it and find prime numbers is not defined well it's because it's a different function name so we need to change that that's my fault and there it is are these all Primes it looks to me like there are primes yeah Prime's 200 so GPT is still kind of the leader in generating code if you want to use llama do not use it for generating code or checking code or doing anything with code yet and yes I've tried this with a 13 billion parameter model and a 70 billion parameter model and it's not consistent and sometimes it gives simple answers that work and sometimes it gives simple programs that don't work use it for Creative tasks use it for writing tasks because for completion of sentences spreading poetry I don't think it's ready for code yet so hopefully this was helpful to you if you want to try this out on your own make sure you check the steps and the links and the commands that I pasted Down Below in the description thanks for watching and I'll see you next time\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
