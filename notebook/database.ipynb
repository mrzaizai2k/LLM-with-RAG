{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/code_Bao/LLM-with-RAG/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(file_path)\n",
    "except:\n",
    "    file_path = os.path.abspath('')\n",
    "    os.chdir(os.path.dirname(file_path))\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"\")\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    RecursiveUrlLoader, \n",
    "    DirectoryLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    BSHTMLLoader,\n",
    "    UnstructuredImageLoader,\n",
    "    Docx2txtLoader,\n",
    "    TextLoader,\n",
    "    SeleniumURLLoader,\n",
    "    YoutubeLoader,\n",
    "    UnstructuredURLLoader,\n",
    "    NewsURLLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    ")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "from src.utils import *\n",
    "from src.ingest import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = take_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "vector_db = VectorDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents, file_path_list = vector_db.create_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_vector_db took 0.03 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "db = vector_db.load_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_dict': {'50cede06-5159-4e2e-8f58-04534892deeb': Document(page_content='NP-COMPLETENESS\\nNguyen An Khuong, CSE-HCMUT\\nAdvanced Algorithms Course (CO5127) Lecture on September 20, 2023', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 0}),\n",
       "  '41f86118-cecb-4e9e-883e-189464abb62b': Document(page_content='Outline\\n1Overview\\n2Turing Machines, P, and NP\\n3Reductions and NP-completeness\\n4Some popular NP-complete problems\\n5Further Reading and Reeferences', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 1}),\n",
       "  '2ddd69ed-1e15-4f8b-b4d8-6ef3393572bd': Document(page_content='1. Overview', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 2}),\n",
       "  'a1660e51-e904-4dba-babe-71296cadcfbe': Document(page_content='Overview 1: Polynomial vs. NP-complete\\n•Polynomial-time algorithm: There exists a constant c∈Nsuch that the\\nalgorithm has (worst-case) running time O(nc), where nis the size of the\\ninput.\\n•Examples:\\nPolynomial: n;n2log2n;n3;n20\\nSuper-polynomial: nlog2n; 2√n; 1.001n; 2n;n!\\n•Central Question: Which computational problems have polynomial-time\\nalgorithms?\\n•Million-dollar question: Intriguing class of problems: NP-complete\\nproblems.\\n•NP-complete problems: It is unknown whether NP-complete problems have\\npolynomial-time algorithms. A polynomial-time algorithm for only one\\nNP-complete problem would imply polynomial-time algorithms for all\\nproblems in NP . (see Gerhard Woeginger’s P vs NP page:\\nhttp://www.win.tue.nl/gwoegi/P-versus-NP .htm)', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 3}),\n",
       "  '76b6d551-849b-4f30-8ce5-e629f74bd82a': Document(page_content='Overview 2: Some Poly. and NPC Problems\\n•Polynomial:\\nShortest Path: Given a graph G, two vertices aandbofG, and an integer k,\\ndoes Ghave a simple a−b-path of length at most k?\\nEuler Tour: Given a graph G, does Ghave a cycle that traverses each edge\\nofGexactly once?\\n2-CNF SAT: Given a propositional formula Fin 2-CNF , is Fsatisfiable? A\\nk-CNF formula is a conjunction (AND) of clauses, and each clause is a\\ndisjunction (OR) of at most k literals, which are negated or unnegated Boolean\\nvariables.\\n•NP-complete:\\nLongest Path: Given a graph Gand an integer k, does Ghave a simple path\\nof length at least k?\\nHamiltonian Cycle: Given a graph G, does Ghave a simple cycle that visits\\neach vertex of G?\\n3-CNF SAT: Given a propositional formula Fin 3-CNF , is Fsatisfiable?\\nExample: (x∨¬y∨z)∧(¬x�', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 4}),\n",
       "  '5d340434-a94e-4ebd-af13-1b2ec5236b22': Document(page_content='�z)∧(¬x∨z)∧(¬y∨¬z).', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 4}),\n",
       "  'd1853027-5c8b-4ffb-bc62-57120402b283': Document(page_content='2. Turing Machines, P , and NP', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 5}),\n",
       "  '9414b4e6-2c4e-423d-a474-ebd3542815d4': Document(page_content='Turing Machines, P, andNP: Decision probs and\\nEncodings\\n•We are going to\\nFormally define P , NP , and NP-complete (NPC), and then\\nLearn how to show that a problem is NP-complete\\n•<Name of Decision Problem >\\nInput: <What constitutes an instance >\\nQuestion: <Yes/No question >\\n•We want to know which decision problems can be solved in polynomial time -\\npolynomial in the size of the input n .\\nAssume a ”reasonable” encoding of the input\\nMany encodings are polynomial-time equivalent; i.e., one encoding can be\\ncomputed from another in polynomial time.\\nImportant exception: unary versus binary encoding of integers.\\nAn integer xtakes⌈log2x⌉bits in binary and x= 2log2xbits in unary.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 6}),\n",
       "  '161a7ff7-622f-4850-8e33-b5c6f98ddb1e': Document(page_content='Turing Machines, P, andNP: Formal-language framework\\nWe can view decision problems as languages.\\n•Alphabetσ: a finite set of symbols. W.l.o.g., σ={0,1}\\n•Language Loverσ: set of strings made with symbols from σ:L⊆σ∗\\n•Fix an encoding of instances of a decision problem intoσ\\n•Define the language L⊆σ∗such that\\nx∈L⇔xis a Y es-instance for', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 7}),\n",
       "  'cd657473-6fb8-49de-85ed-b763dfc06ae5': Document(page_content='Non-deterministic Turing Machine (NTM)\\n•input word x∈σ∗placed on an\\ninfinite tape (memory)\\n•read-write head initially placed on\\nthe first symbol of x\\n•computation step: if the machine is\\nin state sand reads a, it can move\\ninto state s′, writing b, and moving\\nthe head into direction D∈{L,R}if\\n((s,a),(s′,b,D))∈δ.\\n•Q: a finite, non-empty set of states\\n•γ: a finite, non-empty set of tape\\nsymbols\\n•∈γ: blank symbol (the only\\nsymbol allowed to occur on the\\ntape infinitely often)\\n•σ⊆γ\\\\{b}: set of input symbols\\n•q0∈Q: start state\\n•A⊆Q: set of accepting (final)\\nstates\\n•δ⊆(Q\\\\A×γ)×(Q×γ×{L,R}) :\\ntransition relation, where L(R)\\nstands for a move to the left (right.)', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 8}),\n",
       "  'bfc7186a-377e-448f-ad43-131c62e2616d': Document(page_content='Accepted Language, Accept and Decide in poly. time\\n•Definition 1. A NTM accepts a word x∈σ∗if there exists a sequence of\\ncomputation steps starting in the start state and ending in an accepted state.\\n•Definition 2. The language accepted by an NTM is the set of words it\\naccepts.\\n•Video (The LEGO Turing Machine ): https://youtu.be/cYw2ewoO6c4\\n•Definition 3. A language Lisaccepted in polynomial time by an NTM Mif\\nLis accepted by M, and\\nthere is a constant ksuch that for any word x∈L, the NTM Maccepts xin\\nO(\\n|x|k)\\ncomputation steps.\\n•Definition 4. A language Lisdecided in polynomial time by an NTM Mif\\nthere is a constant ksuch that for any word x∈L, the NTM Maccepts xin\\nO(\\n|x|k)\\ncomputation steps, and\\nthere is a constant k′such that for any word x∈σ∗\\\\L, on input xthe NTM', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 9}),\n",
       "  '3972c649-a8e7-4571-af59-cdb0bdbea0fe': Document(page_content='\\\\L, on input xthe NTM M\\nhalts in a non-accepting state ( Q\\\\A) inO(\\n|x|k′)\\ncomputation steps.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 9}),\n",
       "  '95905978-ff4c-48c8-876d-eb0a444ec427': Document(page_content='DTM and DTM equivalents\\n•Definition 5. ADeterministic Turing Machine (DTM) is a Non-deterministic\\nTuring Machine where the transition relation contains at most one tuple\\n((s,a),(·,·,·)) for each s∈Q\\\\Aanda∈γ.\\n•The transition relation δcan be viewed as a function\\nδ:Q\\\\A×γ→Q×γ×{L,R}.\\n•⇒ For a given input word x∈σ∗, there is exactly one sequence of\\ncomputation steps starting in the start state.\\n•Many computational models are polynomial-time equivalent to DTMs:\\nRandom Access Machine (RAM, used for algorithms in the textbook)\\nvariants of Turing machines (multiple tapes, infinite only in one direction, ...)\\n...', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 10}),\n",
       "  '698ada80-7624-4d6e-8da1-2068b135be45': Document(page_content='Definitions of Pand NP\\n•Definition 6. (P)·P={L⊆σ∗: there is a DTM accepting Lin polynomial\\ntime}\\n•Definition 7. (NP).NP={L⊆σ∗: there is a NTM accepting Lin polynomial\\ntime}\\n•Definition 8. (coNP). coNP ={L⊆σ∗:σ∗\\\\L∈NP}\\ncoP?\\n•Theorem 9. P={L⊆σ∗: there is a DTM deciding Lin polynomial time }\\n•Proof (sketch). Need to show: if Lis accepted by a DTM Min polynomial\\ntime, then there is a DTM that decides Lin polynomial time. Idea: design a\\nDTM M′that simulates Mforc·nksteps, where c·nkis the running time of\\nM. (Note that this proof is nonconstructive: we might not know the running\\ntime of M.)', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 11}),\n",
       "  '0f0ed39d-6f71-451b-9438-7d6a28ef8d9a': Document(page_content='NP and certificates: Non-deterministic choices\\n•A NTM for an NP-language Lmakes a polynomial number of\\nnon-deterministic choices on input x∈L. We can encode these\\nnon-deterministic choices into a certificate c, which is a polynomial-length\\nword. Now, there exists a DTM, which, given xandc, verifies that x∈Lin\\npolynomial time.\\n•Thus, L∈NP iff there is a DTM Vand for each x∈Lthere exists a\\npolynomial-length certificate csuch that V(x,c) = 1, but V(y,·) = 0 for each\\ny/∈L.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 12}),\n",
       "  '3a14a54d-4f0f-4110-836d-5531dc0f8ea9': Document(page_content='CNF-SAT is in NP\\n•A CNF formula is a propositional formula in conjunctive normal form:\\nconjunction (AND) of clauses; each clause is a disjunction (OR) of literals;\\neach literal is a negated or unnegated Boolean variable.\\n•An assignment α: var( F)→{0,1}satisfies a clause Cif it sets a literal of C\\nto true, and it satisfies Fif it satisfies all clauses in F.\\n•CNF-SAT Input: CNF formula F\\nQuestion: Does Fhave a satisfying assignment?\\n•Example: (x∨¬y∨z)∧(¬x∨z)∧(¬y∨¬z).\\n•Lemma. CNF-SAT∈NP.\\n•Proof. Certificate: assignment αto the variables. Given a certificate, it can\\nbe checked whether all clauses are satisfied.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 13}),\n",
       "  '96152594-4ccb-476d-b534-017fc64326ab': Document(page_content='Brute-force algorithms for problems in NP\\n•Theorem. Every problem in NP can be solved in exponential time.\\n•Proof. Let be an arbitrary problem in NP . [Use certificate-based definition\\nof NP] We know that ∃a polynomial pand a polynomial-time verification\\nalgorithm Vsuch that:\\nfor every x∈(i.e., every Y es-instance for )∃string c∈{0,1}∗,|c|≤p(|x|),\\nsuch that V(x,y) = 1, and\\nfor every x/∈(i.e., every No-instance for ) and every string\\nc∈{0,1}∗,V(x,c) = 0.\\n•Now, we can prove that there exists an exponential-time algorithm for with\\ninput x:\\nFor each string c∈{0,1}∗with|c|≤p(|x|), evaluate V(x,c) and return Y es if\\nV(x,c) = 1.\\nReturn No.\\n•Running time: 2p(|x|)·n', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 14}),\n",
       "  'f8236b6a-c88f-4e0e-b0c7-e0953d56fe92': Document(page_content=': 2p(|x|)·nO(1)⊆2O(2·p(|x|))= 2O(p(|x|)), but nonconstructive.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 14}),\n",
       "  '10ed6018-0d4d-4d19-b540-4f5ff3a1903c': Document(page_content='3. Reductions and NP-completeness', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 15}),\n",
       "  '90bee8e2-7509-49fd-af4f-6ccabf862a3c': Document(page_content='Polynomial-time reduction and NP-completeness\\n•Definition 12. A language L1ispolynomial-time reducible to a language L2,\\nwritten L1≤PL2, if there exists a polynomial-time computable function\\nf:σ∗→σ∗such that for all x∈σ∗,\\nx∈L1⇔f(x)∈L2.\\n•A polynomial time algorithm computing fis areduction algorithm.\\n•Lemma 13 ( New polynomial-time algorithms via reductions ).If\\nL1,L2∈σ∗are languages such that L1≤PL2, then L2∈Pimplies L1∈P.\\n•Definition 14 ( NP-hard ).A language L⊆σ∗is NP-hard if\\nL′≤PLfor every L′∈NP.\\n•Definition 15 ( NP-complete ).A language L⊆σ∗is NP-complete (in NPC) if\\n1L∈NP, and\\n2Lis NP-hard.\\n•\\n•Theorem (The first NP-complete', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 16}),\n",
       "  'ef197fea-8230-4ce3-98e5-2e1b133102e5': Document(page_content='\\n•Theorem (The first NP-complete problem). CNF-SAT is NP-complete.\\n•Proved by encoding NTMs into SAT Coo71; Lev73 and then CNF-SAT Kar72.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 16}),\n",
       "  '9c8736de-10e8-4338-b143-9673539f2c43': Document(page_content='Strategy for proving NP-completeness\\n•Lemma. IfLis a language such that L′≤PLfor some L′∈NPC , then Lis\\nNP-hard. If, in addition, L∈NP, then L∈NPC .\\n•Proof. For all L′′∈NP, we have L′′≤PL′≤PL. By transitivity, we have\\nL′′≤PL. Thus, Lis NP-hard.\\n•Method to prove that a language Lis NP-complete:\\n1Prove L∈NP\\n2Prove Lis NP-hard.\\nSelect a known NP-complete language L′.\\nDescribe an algorithm that computes a function fmapping every instance x∈σ∗\\nofL′to an instance f(x) ofL.\\nProve that x∈L′⇔f(x)∈Lfor all x∈σ∗.\\nProve that the algorithm computing fruns in polynomial time.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 17}),\n",
       "  '6483121d-97ed-40b9-a67f-978501d47957': Document(page_content='4. Some popular NP-complete problems', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 18}),\n",
       "  '39fcaf70-a321-4d3a-9c90-475452070574': Document(page_content='NP-complete problems: 3-CNF SAT is NP-hard\\n•Theorem. 3-CNF SAT is NP-complete.\\n•Proof. 3-CNF SAT is in NP , since it is a special case of CNF-SAT. To show\\nthat 3-CNF SAT is NP-hard, we give a polynomial reduction from CNF-SAT.\\nLetFbe a CNF formula. The reduction algorithm constructs a 3 -CNF\\nformula F′as follows. For each clause CinF:\\nIfChas at most 3 literals, then copy CintoF′.\\nOtherwise, denote C=(ℓ1∨ℓ2∨···∨ℓk). Create k−3 new variables\\ny1,..., yk−3, and add the clauses\\n(ℓ1∨ℓ2∨y1),(¬y1∨ℓ3∨y2),(¬y2∨ℓ4∨y3),..., (¬yk−3∨ℓk−1∨ℓk).\\n•Show that', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 19}),\n",
       "  '0bd2f21a-a7d5-4ffb-a248-a4228338c109': Document(page_content='�ℓk).\\n•Show that Fis satisfiable⇔F′is satisfiable. Show that F′can be computed\\nin polynomial time (trivial; use a RAM).', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 19}),\n",
       "  '1bf754b3-7cdf-44af-976d-77fc59dc8e08': Document(page_content='Clique and NP-Completeness\\n•Definition. Aclique in a graph\\nG= (V,E) is a subset of vertices\\nS⊆Vsuch that every two vertices\\nofSare adjacent in G.\\n•Clique Problem:\\nInput: Graph G, integer k\\nQuestion: Does Ghave a clique of\\nsize k?\\n•Theorem. Clique is NP-complete.\\n', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 20}),\n",
       "  '32eb6d10-b6d7-4134-8658-c4372a56d6bb': Document(page_content='Proving Clique is in NP\\n•Clique is in NP - Let F=C1∧C2∧...Ckbe a 3-CNF formula\\n•Construct a graph Gthat has a clique of size kiffFis satisfiable\\n•For each clause Cr=(ℓr\\n1∨···∨ℓr\\nw),1≤r≤k, create wnew vertices\\nvr\\n1,..., vr\\nw\\n•Add an edge between vr\\niandvs\\njif\\nr̸=s and\\nℓr\\ni̸=¬ℓs\\nj where¬¬x=x.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 21}),\n",
       "  '7955db5e-9a91-460a-b026-3cc751f3e849': Document(page_content='Proving Clique is in NP (cont.)\\n•Check correctness and polynomial running time\\n•Correctness: Fhas a satisfying assignment iff Ghas a clique of size k.\\n•(⇒) : Letαbe a sat. assignment for F. For each clause Cr, choose a literal\\nℓr\\niwithα(ℓr\\ni)= 1, and denote by srthe corresponding vertex in G. Now,\\n{sr: 1≤r≤k}is a clique of size kinGsinceα(x)̸=α(¬x).\\n•(⇐) : Let Sbe a clique of size kinG. Then, Scontains exactly one vertex\\nsr∈{vr\\n1,..., vr\\nw}for each r∈{1,..., k}. Denote by lrthe corresponding\\nliteral. Now, for any r,r′, it is not the case that lr=¬lr′. Therefore, there is an\\nassignment αto var( F) such that α(lr)= 1 for each r∈{1,..., k}andα\\nsatisf', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 22}),\n",
       "  '04767d3d-1934-4752-b4b3-ce7d917c6a50': Document(page_content='1,..., k}andα\\nsatisfies F.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 22}),\n",
       "  'e0f8e64a-e1d8-4f6d-9c8f-3f153cfacb8e': Document(page_content='Vertex Cover\\n•Definition. Avertex cover in a graph G= (V,E) is a subset of vertices\\nS⊆Vsuch that every edge of Ghas an endpoint in S.\\n•Vertex Cover Problem :\\nInput: Graph G,integer k\\nQuestion: Does Ghave a vertex cover of size k?\\n•Theorem. VERTEX COVER is NP-complete.\\n•Proof. HW3.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 23}),\n",
       "  '7d04ec6e-89c6-42b7-ac85-f27b290ceb74': Document(page_content='Hamiltonian Cycle\\n•Definition. AHamiltonian Cycle in a graph G= (V,E) is a cycle visiting each\\nvertex exactly once.\\n(Alternatively, a permutation of Vsuch that every two consecutive vertices\\nare adjacent and the first and last vertex in the permutation are adjacent.)\\n•Hamiltonian CyCle Problem :\\nInput: Graph G,integer k\\nQuestion: Does GHamiltonian Cycle?\\n•Theorem. Hamiltonian Cycle is NP- complete .\\n•Proof. HW3.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 24}),\n",
       "  '02b6f69f-e42b-4f7a-b464-37e16a29b0d5': Document(page_content='5. Further Reading and Reeferences', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 25}),\n",
       "  '9c7c8046-bb2f-41ed-96a1-5002bfa311fa': Document(page_content='Further Reading and ereferences\\n•Further Reading:\\nCLRS22’s Chapter 34, NP-Completeness and\\nGarey and Johnson’s influential reference book, GJ79\\n•References:\\n[CLRS22] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.\\nIntroduction to Algorithms. 4th ed. The MIT Press, 2022.\\n[GJ79] Michael R. Garey and David S. Johnson. Computers and Intractability: A Guide\\nto the Theory of NP-Completeness. W. H. Freeman & Co., 1979.\\n[Coo71] Stephen A. Cook. ”The Complexity of Theorem-Proving Procedures”. In:\\nProceedings of the 3rd Annual ACM Symposium on Theory of Computing\\n(STOC 1971). 1971, pp. 151-158.\\n[Kar72] Richard M. Karp. ”Reducibility among combinatorial problems.” In: Complexity\\nof computer computations (Proc. Sympos., IBM Thomas J. Watson Res.\\nCenter, Y orktown Heights,', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 26}),\n",
       "  'c4fca76f-a625-40fa-af13-615db2867bd8': Document(page_content='.\\nCenter, Y orktown Heights, N.Y ., 1972). New Y ork: Plenum, 1972, pp. 85-103.\\n[Lev73] Leonid Levin. ”Universal sequential search problems.” In: Problems of\\nInformation Transmission 9.3 (1973), pp. 265-266.', metadata={'source': 'data/web_data//Slides__NP_completeness__Advanced_Algorithms___CO55127_.pdf', 'page': 26}),\n",
       "  '3e91a770-109d-4470-b889-d298dbd661bb': Document(page_content='Chapter 4:\\nRelational Data Model and \\nER/EER -to-Relational Mapping', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 0}),\n",
       "  '6f5221cd-faf9-4a6a-8099-416b8ef8073f': Document(page_content='Contents\\n1Relational Data Model\\n2Main Phases of Database Design\\n3ER-/EER -to-Relational Mapping\\n2 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 1}),\n",
       "  '6f636fb2-922b-411e-bb2f-b4c1c0529683': Document(page_content='Contents\\n1Relational Data Model\\n2Main Phases of Database Design\\n3ER-/EER -to-Relational Mapping\\n3 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 2}),\n",
       "  '3a1277f1-443b-49a8-8ed1-b6d6762f049f': Document(page_content='Relational Data Model\\n◼Basic Concepts: relational data model, \\nrelation schema, domain, tuple , cardinality & \\ndegree, database schema, etc.\\n◼Relational Integrity Constraints\\n❑key, primary key & foreign key\\n❑entity integrity constraint\\n❑referential integrity\\n◼Update Operations on Relations\\n4 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 3}),\n",
       "  '837ea7b9-f77d-4c28-b4e2-784a18c3f982': Document(page_content='Basic Concepts\\n◼The relational model of data is based on the \\nconcept of a relation\\n◼A relation is a mathematical concept based \\non the ideas of sets\\n◼The model was first proposed by Dr. E.F. \\nCodd of IBM in 1970 in the following paper:\\n\"A Relational Model for Large Shared Data \\nBanks,\" Communications of the ACM, June \\n1970\\n5 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 4}),\n",
       "  '36af631b-2a37-46b7-9b75-0a7f73881e50': Document(page_content='Basic Concepts\\n◼Relational data model: represents a database \\nin the form of relations -2-dimensional table \\nwith rows and columns of data. A database may \\ncontain one or more such tables. A relation \\nschema is used to describe a relation\\n◼Relation schema: R(A1, A2,…, An) is made up \\nof a relation name R and a list of attributes A1, \\nA2, . . ., An. Each attribute Ai is the name of a \\nrole played by some domain D in the relation \\nschema R. R is called the name of this relation\\n6 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 5}),\n",
       "  'ec2aa738-eed7-40df-86ad-b7cfddd6dd5a': Document(page_content='Basic Concepts\\n◼The degree of a relation is the number of \\nattributes n of its relation schema.\\n◼Domain D : D is called the domain of Ai and \\nis denoted by dom(Ai). It is a set of atomic \\nvalues and a set of integrity constraints\\n❑STUDENT(Name, SSN, HomePhone , Address, \\nOfficePhone , Age, GPA) \\n❑Degree = ??\\n❑dom(GPA) = ??\\n7 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 6}),\n",
       "  '543cc83c-a525-40d2-83c1-33e4c9ea09da': Document(page_content='Basic Concepts\\n◼Tuple : row/record in table\\n◼Cardinality : number of tuples in a table\\n◼Database schema S = {R1, R2,…, Rm}\\n8\\n Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 7}),\n",
       "  'deb95d09-fc74-4236-85e2-df9551c030bb': Document(page_content='Basic Concepts\\n◼A relation r (or relation state, relation \\ninstance ) of the relation schema R(A1, A2, . \\n. ., An), also denoted by r(R), is a set of n-\\ntuples r = {t1, t2, . . ., tm}. \\n❑Each n -tuple t is an ordered list of n values t = \\n<v1, v2, . . ., vn>, where each value vi, i=1..n, is \\nan element of dom(Ai) or is a special nullvalue. \\nThe ithvalue in tuple t, which corresponds to the \\nattribute Ai, is referred to as t[Ai]\\n9 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 8}),\n",
       "  '1ae914cf-9b94-43f0-b1db-79fabfeb4ca6': Document(page_content='Basic Concepts\\nRelational data model\\nDatabase schema\\nRelation schema\\nRelation\\nTuple\\nAttribute\\n10 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 9}),\n",
       "  '0e49ac97-7503-4ee8-94cb-3dcac1e783ea': Document(page_content='Basic Concepts\\n◼A relation can be conveniently represented \\nby a table, as the example shows\\n◼The columns of the tabular relation represent \\nattributes\\n◼Each attribute has a distinct name, and is \\nalways referenced by that name, never by its \\nposition\\n◼Each row of the table represents a tuple. The \\nordering of the tuples is immaterial and all \\ntuples must be distinct\\nJan -2015 11', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 10}),\n",
       "  'ee31ccb2-5dd8-410a-b975-1d24cd07d39f': Document(page_content='Basic Concepts\\nJan -2015 12', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 11}),\n",
       "  'afaa82e5-b530-42a2-9653-3eb93b83085c': Document(page_content='Basic Concepts\\nFormal Terms Informal Terms \\nRelation Table\\nAttribute Column Header\\nDomain All possible Column Values\\nTuple Row\\nSchema of a Relation Table Definition\\nState of the Relation Populated Table\\n13\\uf0a3Alternative Terminology for Relational Model\\nJan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 12}),\n",
       "  '5de6a2c1-ef33-4729-b001-249a6b01ef83': Document(page_content='Relational Integrity Constraints\\n◼Constraints are conditions that must hold on \\nallvalid relation instances. There are three \\nmain types of constraints:\\n❑Key constraints\\n❑Entity integrity constraints\\n❑Referential integrity constraints\\n14 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 13}),\n",
       "  'adae41a7-c9ac-4a47-8377-9fc8c54f6f85': Document(page_content='Relational Integrity Constraints\\n◼Null value\\n❑Represents value for an attribute that is currently \\nunknown or inapplicable for tuple\\n❑Deals with incomplete or exceptional data\\n❑Represents the absence of a value and is not the \\nsame as zero or spaces, which are values\\n15 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 14}),\n",
       "  '3b846d2f-a477-4532-b48b-b09794f32f4a': Document(page_content='Relational Integrity Constraints -\\nKey Constraints\\n◼Superkey of R: A set of attributes SK of R \\nsuch that no two tuples in any valid relation \\ninstance r(R)  will have the same value for \\nSK.  That is, for any distinct tuples t1 and t2 \\nin r(R), t1[SK] \\uf0b9t2[SK]\\n◼Key of R: A \"minimal\" superkey ; that is, a \\nsuperkey K such that removal of any attribute \\nfrom K results in a set of attributes that is not \\na superkey\\n16 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 15}),\n",
       "  '599c1205-8389-419f-bb54-cf3183705d00': Document(page_content='Relational Integrity Constraints -\\nKey Constraints\\nExample: The CAR relation schema:\\nCAR(State, Reg#, SerialNo , Make, Model, Year) has two \\nkeys\\nKey1 = {State, Reg#}\\nKey2 = { SerialNo }, which are also superkeys . {SerialNo , \\nMake} is a superkey but not  a key\\n◼If a relation has several candidate keys, one \\nis chosen arbitrarily to be the primary key. \\nThe primary key attributes are underlined .\\n17 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 16}),\n",
       "  '640ed1a5-7f93-4b1b-9aea-d2dee0b5c414': Document(page_content='Relational Integrity Constraints -\\nKey Constraints\\n◼The CAR relation, with two candidate keys: \\nLicense_Number and Engine_Serial_Number\\n18\\n Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 17}),\n",
       "  '76b94434-0fa0-434c-9696-b83d66447896': Document(page_content='Relational Integrity Constraints -\\nEntity Integrity\\n19 Jan -2015◼Relational Database Schema : A set S of relation \\nschemas that belong to the same database. S is the \\nname  of the database: S = {R1, R2, ..., Rn}\\n◼Entity Integrity: primary key attributes  PK of each \\nrelation schema R in S cannot have null values in \\nany tuple of r(R) because primary key values are \\nused to identify  the individual tuples: t[PK] \\uf0b9null for \\nany tuple t in r(R)\\n❑Note: Other attributes of R may be similarly \\nconstrained  to disallow null values, even though they \\nare not members of the primary key', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 18}),\n",
       "  '4866ee8b-efd4-484b-9a4d-8b30c6bb35d1': Document(page_content='Relational Integrity Constraints -\\nReferential Integrity\\n20 Jan -2015◼A constraint involving two  relations (the previous \\nconstraints involve a single relation)\\n◼Used to specify a relationship among tuples in two \\nrelations: the referencing relation and the referenced \\nrelation\\n◼Tuples in the referencing relation R1have attributes FK \\n(called foreign key attributes) that reference the primary \\nkey attributes PK of the referenced relation R2. A tuple t1\\nin R1is said to reference a tuple t2in R2if t1[FK] = t2[PK]\\n◼A referential integrity constraint can be displayed in a \\nrelational database schema as a directed arc from R1.FK \\nto R2', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 19}),\n",
       "  'bb8078e0-55ff-443c-86f9-0ea9a3b79349': Document(page_content='Relational Integrity Constraints -\\nReferential Integrity\\n21\\n Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 20}),\n",
       "  '7a2f5a52-5b31-4f06-b380-103ed4405b2b': Document(page_content='Relational Integrity Constraints -\\nReferential Integrity\\n22 Jan -2015Statement of the constraint\\n◼The value in the foreign key column (or \\ncolumns) FK of the the referencing relation \\nR1can be either :\\n❑(1) a value of an existing primary key value of the \\ncorresponding primary key PK in the referenced \\nrelation R2,, or\\n❑(2) a NULL\\n◼In case (2), the FK in R1 should notbe a part \\nof its own primary key', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 21}),\n",
       "  'fa20db4c-e972-43ae-aa10-d01a1c500400': Document(page_content='23 Jan -2015Referential integrity constraints displayed on the \\nCOMPANY relational database schema', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 22}),\n",
       "  '37a661a2-dc3b-411f-81c3-11c1125af5bc': Document(page_content='Relational Integrity Constraints -\\nOther Types of Constraints\\n◼Semantic Integrity Constraints:\\n-based on application semantics and cannot be \\nexpressed by the model per se\\n-E.g., “the max. no. of hours per employee for all \\nprojects he or she works on is 56 hrsper week”\\n-A constraint specification language may have to \\nbe used to express these\\n-SQL-99 allows triggers and ASSERTIONS to \\nallow for some of these\\n◼State/static constraints (so far)\\n◼Transition/dynamic constraints: e.g., “the \\nsalary of an employee can only increase”\\n24 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 23}),\n",
       "  'e0c477e2-c8cd-4de9-82e9-2733f02c0c9c': Document(page_content='Update Operations on Relations\\n◼INSERT a tuple\\n◼DELETE a tuple\\n◼MODIFY a tuple\\n◼Integrity constraints should not be violated by \\nthe update operations\\n25 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 24}),\n",
       "  '32b21a09-79da-4b58-b6e2-966ec8f6ca8d': Document(page_content='Update Operations on Relations\\n26 Jan -2015◼Insertion : to insert a new tuple t into a relation \\nR. When inserting a new tuple, it should make \\nsure that the database constraints are not \\nviolated:\\n❑The value of an attribute should be of the correct data \\ntype (i.e. from the appropriate domain).\\n❑The value of a prime attribute (i.e. the key attribute) \\nmust not be null\\n❑The key value(s) must not be the same as that of an \\nexisting tuple in the same relation\\n❑The value of a foreign key (if any) must refer to an \\nexisting tuple in the corresponding relation\\n◼Options if the constraints are violated: \\nHomework !!', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 25}),\n",
       "  '20db2f6a-f84f-4cb8-bfbe-f3dc320b842c': Document(page_content='Update Operations on Relations\\n◼Deletion : to remove an existing tuple t from a \\nrelation R. When deleting a tuple, the \\nfollowing constraints must not be violated:\\n❑The tuple must already exist in the database\\n❑The referential integrity constraint is not violated\\n◼Modification : to change values of some \\nattributes of an existing tuple t in a relation R\\n27 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 26}),\n",
       "  '4e601021-9696-4217-88eb-aebebbeaa960': Document(page_content='Update Operations on Relations\\n◼In case of integrity violation, several actions \\ncan be taken:\\n❑Cancel the operation that causes the violation \\n(REJECT option)\\n❑Perform the operation but inform the user of the \\nviolation\\n❑Trigger additional updates so the violation is \\ncorrected (CASCADE option, SET NULL option)\\n❑Execute a user -specified error -correction routine\\n◼Again, homework !!\\n28 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 27}),\n",
       "  '02cc6973-6fed-4da1-9c6c-c18036b36543': Document(page_content='Contents\\n1Relational Data Model\\n2Main Phases of Database Design\\n3ER-/EER -to-Relational Mapping\\n29 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 28}),\n",
       "  '12999037-d630-4cb0-a74f-c2302c06a792': Document(page_content='Main Phases of Database Design\\n◼Three main phases\\n❑Conceptual database design\\n❑Logical database design\\n❑Physical database design\\n30 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 29}),\n",
       "  '70ac27f9-5d06-44eb-9983-f750d195255a': Document(page_content='31 Jan -2015A simplified diagram to illustrate \\nthe main phases of database design', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 30}),\n",
       "  '14efa61d-cb9f-421b-97e6-3406ddb9cb37': Document(page_content='Main Phases of Database Design\\n◼Conceptual database design\\n❑The process of constructing a model of the data \\nused in an enterprise, independent of allphysical \\nconsiderations\\n❑Model comprises entity types, relationship types, \\nattributes and attribute domains, primary and \\nalternate keys, structural and integrity constraints\\n32 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 31}),\n",
       "  '1b766dda-64ae-4a30-ba30-3371971b9516': Document(page_content='Main Phases of Database Design\\n◼Logical database design\\n❑The process of constructing a model of the data \\nused in an enterprise based on a specific data \\nmodel (e.g. relational), but independent of a \\nparticular DBMS and other physical \\nconsiderations\\n❑ER-& EER -to-Relational Mapping\\n❑Normalization\\n33 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 32}),\n",
       "  'ce3b5a2b-3139-4a77-99cc-86126c4cf39e': Document(page_content='Main Phases of Database Design\\n◼Physical database design\\n❑The process of producing a description of the \\nimplementation of the database on secondary \\nstorage; it describes the base relations, file \\norganizations, and indexes design used to \\nachieve efficient access to the data, and any \\nassociated integrity constraints and security \\nmeasures\\n34 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 33}),\n",
       "  'd2b1fa83-3ca1-44d4-aded-2249d0012b85': Document(page_content='35 Jan -2015The ERD for the COMPANY database', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 34}),\n",
       "  '71b781cf-0f04-4e1c-8ae8-ef8a7f4b4251': Document(page_content='36 Jan -2015Result of mapping the COMPANY ER schema \\ninto a relational schema', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 35}),\n",
       "  '62718fd6-1ac5-480a-a794-4cfb8dddad38': Document(page_content='Contents\\n1Relational Data Model\\n2Main Phases of Database Design\\n3ER-/EER -to-Relational Mapping\\n37 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 36}),\n",
       "  '8af34c7d-ebe6-45f1-a32d-af592d60cd4f': Document(page_content='ER-& EER -to-Relational Mapping\\n◼ER-\\n❑Step 1: Mapping of Regular Entity Types\\n❑Step 2: Mapping of Weak Entity Types\\n❑Step 3: Mapping of Binary 1:1 Relationship Types\\n❑Step 4: Mapping of Binary 1:N Relationship Types\\n❑Step 5: Mapping of Binary M:N Relationship Types\\n❑Step 6: Mapping of Multivalued attributes\\n❑Step 7: Mapping of N -aryRelationship Types\\n◼EER-\\n❑Step 8: Options for Mapping Specialization or \\nGeneralization.\\n❑Step 9: Mapping of Union Types (Categories)\\n38 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 37}),\n",
       "  '466f1b92-a6d8-4c96-bf5b-3e2aa7998766': Document(page_content='ER-to-Relational Mapping\\n39 Jan -2015◼Step 1: Mapping of Regular (strong) Entity \\nTypes\\n❑Entity --> Relation\\n❑Attribute of entity --> Attribute of relation\\n❑Primary key of entity --> Primary key of relation\\n❑Example: We create the relations EMPLOYEE, \\nDEPARTMENT, and PROJECT in the relational \\nschema corresponding to the regular entities in the ER \\ndiagram. SSN, DNUMBER, and PNUMBER are the \\nprimary keys for the relations EMPLOYEE, \\nDEPARTMENT, and PROJECT as shown', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 38}),\n",
       "  'f4151441-9316-4d10-9b34-014b3e77fbbc': Document(page_content='Strong Entity \\nTypes\\n40 Jan -2015The ERD for the COMPANY database', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 39}),\n",
       "  '7b1810de-3aaa-4beb-89f3-314a6c31907f': Document(page_content='ER-to-Relational Mapping\\n◼Step 2: Mapping of Weak Entity Types\\n❑For each weak entity type W in the ER schema with owner entity \\ntype E, create a relation R and include all simple attributes (or \\nsimple components of composite attributes) of W as attributes of \\nR\\n❑In addition, include as foreign key attributes of R the primary key \\nattribute(s) of the relation(s) that correspond to the owner entity \\ntype(s)\\n❑The primary key of R is the combination of the primary key(s) of \\nthe owner(s) and the partial key of the weak entity type W, if any\\n❑Example: Create the relation DEPENDENT in this step to \\ncorrespond to the weak entity type DEPENDENT. Include the \\nprimary key SSN of the EMPLOYEE relation as a foreign key \\nattribute of DEPENDENT (renamed to ESSN)\\nThe primary key of the DEPENDENT relation is the combination \\n{ESSN, DEPENDENT_NAME} because DEPENDENT_NAME is \\nthe partial key of DEPENDENT', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 40}),\n",
       "  '0f8e919c-77db-4d52-adc1-54af24931f03': Document(page_content=' \\nthe partial key of DEPENDENT\\n❑Note: CASCADE option as implemented\\n41 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 40}),\n",
       "  '38a248cc-9341-4f29-931a-36b4e4cfacc6': Document(page_content='Weak Entity \\nTypesPartial keyOwner’s PK\\nPK\\n42 Jan -2015The ERD for the COMPANY database', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 41}),\n",
       "  '4cdf7b11-c1e7-4b33-9fe3-d1fe8381f97d': Document(page_content='43 Jan -2015Result of mapping the COMPANY ER schema \\ninto a relational schema', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 42}),\n",
       "  '3b45e6ea-6b64-4e48-bd3d-b110b19c3646': Document(page_content='ER-to-Relational Mapping\\n◼ER-\\n❑\\nStep 1: Mapping of Regular Entity Types\\n❑\\nStep 2: Mapping of Weak Entity Types\\n❑Step 3: Mapping of Binary 1:1 Relationship Types\\n❑Step 4: Mapping of Binary 1:N Relationship Types\\n❑Step 5: Mapping of Binary M:N Relationship \\nTypes\\n❑Step 6: Mapping of Multivalued attributes\\n❑Step 7: Mapping of N -ary Relationship Types\\n◼Transformation of binary relationships -\\ndepends on functionality of relationship and \\nmembership class of participating entity types\\n44 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 43}),\n",
       "  'b2b24d0e-640b-41a9-adbc-b6f8d41271bd': Document(page_content='ER-to-Relational Mapping\\n45 Jan -2015◼Mandatory membership class\\n❑For two entity types E1 and E2: If E2 is a mandatory \\nmember of an N:1 (or 1:1) relationship with E1, then \\nthe relation for E2 will include the prime attributes of \\nE1 as a foreign key to represent the relationship\\n❑1:1 relationship: If the membership class for E1 and \\nE2 are both mandatory, a foreign key can be used in \\neither relation\\n❑N:1 relationship: If the membership class of E2, which \\nis at the N -side of the relationship, is optional (i.e. \\npartial), then the above guideline is not applicable', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 44}),\n",
       "  'f120e2ff-3514-459b-888b-ad920393d07d': Document(page_content='ER-to-Relational Mapping\\n◼Assume every module must be offered by a department, \\nthen the entity type MODULE is a mandatory member \\nof the relationship OFFER. The relation for MODULE is:\\nMODULE( MDL -NUMBER , TITLE, TERM, ..., DNAME )DEPARTMENT OFFERMODULE1 N\\n46 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 45}),\n",
       "  '51b7797c-e55d-495b-8abb-37945412366f': Document(page_content='Relationships \\nTypes\\n47 Jan -2015The ERD for the COMPANY database', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 46}),\n",
       "  '082e89a8-230f-4b9e-b3a0-52a959b03f3e': Document(page_content='48 Jan -2015Result of mapping the COMPANY ER schema \\ninto a relational schema', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 47}),\n",
       "  '1a594f65-84da-45ba-90f2-bb86e83a4c6a': Document(page_content='ER-to-Relational Mapping\\n◼Optional membership classes\\n❑If entity type E2 is an optional member of the N:1 \\nrelationship with entity type E1 (i.e. E2 is at the N -\\nside of the relationship), then the relationship is \\nusually represented by a new relation containing \\nthe prime attributes of E1 and E2, together with \\nany attributes of the relationship. The key of the \\nentity type at the N -side (i.e. E2) will become the \\nkey of the new relation\\n❑If both entity types in a 1:1 relationship have the \\noptional membership, a new relation is created \\nwhich contains the prime attributes of both entity \\ntypes, together with any attributes of the \\nrelationship. The prime attribute(s) of either entity \\ntype will be the key of the new relation\\n49 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 48}),\n",
       "  'f0138efc-4ddb-48f0-b3c5-ab044fc7e4c3': Document(page_content='ER-to-Relational Mapping\\n◼One possible representation of the relationship:\\nBORROWER( BNUMBER , NAME, ADDRESS, ...)\\nBOOK( ISBN , TITLE, ..., BNUMBER )\\n◼A better alternative:\\nBORROWER( BNUMBER , NAME, ADDRESS, ...)\\nBOOK( ISBN , TITLE, ...)\\nON_LOAN( ISBN , BNUMBER)\\n \\nON_LOAN  BORROWER  BOOK  1 N  \\nJan -2015 50', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 49}),\n",
       "  'b75e02eb-1993-4f48-a9df-d2466b2efe6d': Document(page_content='1:N (both optional)\\n51 Jan -2015The ERD for the COMPANY database', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 50}),\n",
       "  '4d4a52f9-4060-4f00-8952-d8044afdc676': Document(page_content='???\\n52 Jan -2015Result of mapping the COMPANY ER schema \\ninto a relational schema', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 51}),\n",
       "  'b483b494-721f-469c-b1bd-bde70b72181a': Document(page_content='ER-to-Relational Mapping\\n◼N:M binary relationships: \\n❑An N:M relationship is always represented by a \\nnew relation which consists of the prime attributes \\nof both participating entity types together with any \\nattributes of the relationship\\n❑The combination of the prime attributes will form \\nthe primary key of the new relation\\n◼Example: ENROL is an M:N relationship \\nbetween STUDENT and MODULE. To \\nrepresent the relationship, we have a new \\nrelation: \\nENROL( SNUMBER, MDL -NUMBER , DATE)\\n53 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 52}),\n",
       "  '1f70fd0e-fd5b-46f8-a8d2-9a8988e8ab08': Document(page_content='The ERD for the COMPANY database\\nM:N\\n54 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 53}),\n",
       "  '3c308692-f906-4864-b273-3f7b20cb2a4b': Document(page_content='55 Jan -2015Result of mapping the COMPANY ER schema \\ninto a relational schema', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 54}),\n",
       "  '6e0b062f-6d84-4971-9f08-bd17354e255f': Document(page_content='◼Transformation of recursive/involuted relationships\\n❑Relationship among different instances of the same entity\\n❑The name(s) of the prime attribute(s) needs to be changed to reflect \\nthe role each entity plays in the relationship\\nPERSON MARRY1\\n1\\n \\nEMPLOYEE  SUPERVISE  N \\n1  \\nPART COMPRISEM\\nN\\n56 Jan -2015ER-to-Relational Mapping', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 55}),\n",
       "  '61500d33-960f-4557-b932-b3fed77bccab': Document(page_content='ER-to-Relational Mapping\\n◼Example 1: 1:1 involuted relationship, in \\nwhich the memberships for both entities are \\noptional\\nPERSON( ID, NAME, ADDRESS, ...)\\nMARRY( HUSBAND -ID, WIFE_ID, \\nDATE_OF_MARRIAGE)\\n57 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 56}),\n",
       "  'b4af3d5f-92d5-4abd-a235-d434240e4608': Document(page_content='ER-to-Relational Mapping\\n◼Example 2: 1:N involuted relationship\\n❑If the relationship is mandatory or almost \\nmandatory:\\nEMPLOYEE( ID, ENAME, ..., SUPERVISOR_ID )\\n❑If the relationship is optional:\\nEMPLOYEE( ID, ENAME, ...)\\nSUPERVISE( ID, START_DATE, ..., \\nSUPERVISOR_ID )\\n◼Example 3: N:M involuted relationship\\nPART( PNUMBER , DESCRIPTION, ...)\\nCOMPRISE( MAJOR -PNUMBER, MINOR -PNUMBER , QUANTITY)\\n58 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 57}),\n",
       "  'ab008164-6bd0-48e5-82d3-8ec6c3b085e4': Document(page_content='ER-to-Relational Mapping\\n◼ER-\\n❑\\nStep 1: Mapping of Regular Entity Types\\n❑\\nStep 2: Mapping of Weak Entity Types\\n❑\\nStep 3: Mapping of Binary 1:1 Relationship Types\\n❑\\nStep 4: Mapping of Binary 1:N Relationship Types\\n❑\\nStep 5: Mapping of Binary M:N Relationship \\nTypes\\n❑Step 6: Mapping of Multivalued attributes\\n❑Step 7: Mapping of N -ary Relationship Types\\n59 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 58}),\n",
       "  'd66adf05-af4c-4464-84a0-f4d459365b84': Document(page_content='ER-to-Relational Mapping\\n◼Step 6: Mapping of Multivalued attributes\\n❑For each multivalued attribute A, create a new \\nrelation R. This relation R will include an attribute \\ncorresponding to A, plus the primary key attribute \\nK-as a foreign key in R -of the relation that \\nrepresents the entity type or relationship type that \\nhas A as an attribute\\n❑The primary key of R is the combination of A and \\nK. If the multivalued attribute is composite, we \\ninclude its simple components\\nExample: The relation DEPT_LOCATIONS is created. The \\nattribute DLOCATION represents the multivalued attribute \\nLOCATIONS of DEPARTMENT, while DNUMBER -as foreign \\nkey-represents the primary key of the DEPARTMENT relation. \\nThe primary key of R is the combination of {DNUMBER, \\nDLOCATION}\\n60 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 59}),\n",
       "  'b4df16cc-a1f8-4345-aee2-6cb2f17cae28': Document(page_content='Multivalued \\nAttribute\\n61 Jan -2015The ERD for the COMPANY database', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 60}),\n",
       "  'f76c9395-d26c-49e5-bf0b-3ee7beeb2b18': Document(page_content='62 Jan -2015Result of mapping the COMPANY ER schema \\ninto a relational schema', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 61}),\n",
       "  'dccf43be-da95-410a-988d-b7621b5c136f': Document(page_content='ER-to-Relational Mapping\\n◼Step 7: Mapping of N -aryRelationship Types\\n❑For each n -aryrelationship type R, where n>2, create \\na new relationship S to represent R\\n❑Include as foreign key attributes in S the primary keys \\nof the relations that represent the participating entity \\ntypes\\n❑Also include any simple attributes of the n -ary\\nrelationship type (or simple components of composite \\nattributes) as attributes of S\\nExample: The relationship type SUPPY in the ER below. This can \\nbe mapped to the relation SUPPLY shown in the relational \\nschema, whose primary key is the combination of the three \\nforeign keys {SNAME, PARTNO, PROJNAME}\\n63 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 62}),\n",
       "  '88605ca8-4bac-495f-a820-6a488ac790ec': Document(page_content='Note: if the cardinality \\nconstraint on any of the entity \\ntypes E participating in the \\nrelationship is 1, the PK \\nshould not include the FK \\nattributes that reference the \\nrelation E’ corresponding to E\\n64ER-to-Relational Mapping\\nTernary relationship types: The SUPPLY relationship\\nJan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 63}),\n",
       "  '6f22e79a-1d1c-423a-ac16-428cf7586a0a': Document(page_content='ER-to-Relational Mapping\\nCorrespondence between ER and Relational Models\\nER Model Relational Model\\nEntity type “Entity” relation\\n1:1 or 1:N relationship type Foreign key (or “relationship” relation)\\nM:N relationship type “Relationship” relation & 2 foreign keys\\nn-aryrelationship type “Relationship” relation & n foreign keys\\nSimple attribute Attribute\\nComposite attribute Set of simple component attributes\\nMultivalued attribute Relation and foreign key\\nValue set Domain\\nKey attribute Primary (or secondary) key\\n65 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 64}),\n",
       "  'dce397ea-ba4d-481a-bd5e-28c75c73f917': Document(page_content='ER-& EER -to-Relational Mapping\\n◼\\nER\\n-\\n❑\\nStep 1: Mapping of Regular Entity Types\\n❑\\nStep 2: Mapping of Weak Entity Types\\n❑\\nStep 3: Mapping of Binary 1:1 Relationship Types\\n❑\\nStep 4: Mapping of Binary 1:N Relationship Types\\n❑\\nStep 5: Mapping of Binary M:N Relationship Types\\n❑\\nStep 6: Mapping of \\n Multivalued\\n attributes\\n❑\\nStep 7: Mapping of N\\n -\\nary\\nRelationship Types\\n◼EER-\\n❑Step 8: Options for Mapping Specialization or \\nGeneralization.\\n❑Step 9: Mapping of Union Types (Categories)\\n66 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 65}),\n",
       "  '60348c41-1607-4ca4-a3fd-e817bd658956': Document(page_content='EER -to-Relational Mapping\\n◼Step8: Options for Mapping Specialization \\nor Generalization.\\nConvert each specialization with m subclasses {S1, \\nS2,….,Sm} and generalized superclass C, where \\nthe attributes of C are {k,a1,…an} and k is the \\n(primary) key, into relational schemas using one of \\nthe four following options:\\n◼Option 8A: Multiple relations -Superclass and subclasses\\n◼Option 8B: Multiple relations -Subclass relations only\\n◼Option 8C: Single relation with one type attribute\\n◼Option 8D: Single relation with multiple type attributes\\n6767 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 66}),\n",
       "  '04e86fba-9d2d-4e9c-8fb9-16fbcde47e14': Document(page_content='EER -to-Relational Mapping\\n◼Option 8A: Multiple relations -Superclass and \\nsubclasses\\nCreate a relation L for C with attributes Attrs(L) = {k,a1,…an} \\nand PK(L) = k. Create a relation Li for each subclass Si, 1 < i \\n< m, with the attributesAttrs (Li) = {k} U {attributes of Si} and \\nPK(Li)=k. This option works for any specialization (total or \\npartial, disjoint or over -lapping). \\n◼Option 8B: Multiple relations -Subclass relations only\\nCreate a relation Li for each subclass Si, 1 < i < m, with the \\nattributes Attr(Li) = {attributes of Si} U {k,a1…,an} and PK(Li) \\n= k. This option only works for a  specialization whose \\nsubclasses are total (every entity in the superclass must \\nbelong to (at least) one of the subclasses).\\n68 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 67}),\n",
       "  '4aa484a6-1aa8-404d-b83a-1cd886e4d6bd': Document(page_content='Example: \\nOption 8A\\nJan -2015 69\\n', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 68}),\n",
       "  '676915ff-209f-4486-9998-3e4b2424341d': Document(page_content='Tonnage\\n70 Jan -2015Example: \\nOption 8B', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 69}),\n",
       "  'e036c288-665c-458e-ba70-dfe60a452261': Document(page_content='◼Option 8C: Single relation with one type attribute\\nCreate a single relation L with attributes Attrs(L) = {k,a1,…an} U \\n{attributes of S1} U…U {attributes of Sm} U {t} and PK(L) = k. \\nThe attribute t is called a type (or discriminating ) attribute that \\nindicates the subclass to which each tuple belongs. This \\noption works only for specialization whose subclass are \\ndisjoint.\\n◼Option 8D: Single relation with multiple type \\nattributes\\nCreate a single relation schema L with attributes Attrs(L) = \\n{k,a1,…an} U {attributes of S1} U…U {attributes of Sm} U {t1, \\nt2,…,tm} and PK(L) = k. Each ti, 1 < I < m, is a Boolean type \\nattribute indicating whether a tuple belongs to the subclass Si.\\n71 Jan -2015EER -to-Relational Mapping', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 70}),\n",
       "  '97df63ea-6de0-4f06-9aca-c1f40608370e': Document(page_content='EngType\\n72 Jan -2015Example: \\nOption 8C\\nd', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 71}),\n",
       "  'be167585-d75c-4aac-9c45-38361192e6b2': Document(page_content='73 Jan -2015Example: \\nOption 8D', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 72}),\n",
       "  'ee85bd4c-a478-4d43-87e9-ee332cc483ed': Document(page_content='EER -to-Relational Mapping\\n◼Mapping of Shared Subclasses (Multiple Inheritance)\\n❑A shared subclass, such as STUDENT_ASSISTANT, is a \\nsubclass of several classes, indicating multiple inheritance. \\nThese classes must all have the same key attribute; \\notherwise, the shared subclass would be modeled as a \\ncategory.\\n❑We can apply any of the options discussed in Step 8 to a \\nshared subclass, subject to the restriction discussed in \\nStep 8 of the mapping algorithm. Below both 8C and 8D \\nare used for the shared class STUDENT_ASSISTANT.\\n74 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 73}),\n",
       "  '802e8a25-9d72-430f-b1c4-544e3f65c612': Document(page_content='Example: \\nMapping \\nof Shared \\nSubclasses\\n75 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 74}),\n",
       "  '8210ec21-d7e3-4b85-87fd-aa3cf1be55e4': Document(page_content='Example: Mapping of Shared Subclasses\\nJan -2015 76\\n', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 75}),\n",
       "  'c009486f-92ee-45ef-a4ec-4611c3a6721b': Document(page_content='EER -to-Relational Mapping\\n◼Step 9: Mapping of Union Types \\n(Categories).\\n❑For mapping a category whose defining \\nsuperclass have different keys, it is customary to \\nspecify a new key attribute, called a surrogate\\nkey, when creating a relation to correspond to the \\ncategory. \\n❑In the example below we can create a relation \\nOWNER to correspond to the OWNER category \\nand include any attributes of the category in this \\nrelation. The primary key of the OWNER relation \\nis the surrogate key, which we called OwnerId .\\n77 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 76}),\n",
       "  '8d645c23-3ca0-4033-8947-e63a0fb99d33': Document(page_content='Example: Mapping of Union Types\\nJan -2015 78', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 77}),\n",
       "  'c50f887e-8cc4-40d4-b96d-9ea90c549949': Document(page_content='Contents\\n1Relational Data Model\\n2Main Phases of Database Design\\n3ER-/EER -to-Relational Mapping\\n79 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 78}),\n",
       "  'fa7f30f3-1145-431e-ad21-14df5cc0299f': Document(page_content='80 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 79}),\n",
       "  '364394da-9a01-4308-82fd-b23e70af7c46': Document(page_content='Exercise 1\\n81 Jan -2015\\n', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 80}),\n",
       "  '7a57b349-c933-40e8-b938-668fdaebaeca': Document(page_content='Exercise 2\\n82 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 81}),\n",
       "  '6fb89545-ca15-4b92-861c-852069caa6c0': Document(page_content='Review questions\\n1)Define the following terms as they apply to \\nthe relational model of data : domain , \\nattribute, n -tuple, relation schema, relation \\nstate, degree of a relation, relational \\ndatabase schema, and relational database \\nstate .\\n2)Discuss the entity integrity and referential \\nintegrity constraints . Why is each \\nconsidered important?\\n83 Jan -2015', metadata={'source': 'data/web_data//Chapter_4-RelationalDataModelAndRelationalMapping.pdf', 'page': 82}),\n",
       "  'ed5161ff-27ca-45cc-8b9a-d69ce53c9986': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nBài giảng tóm tắt chuyên đề\\nKí hiệu O-lớn, Độ tăng của hàm và\\nPhân tích thuật toán\\nNguyễn An Khương\\nNgày 6 tháng 9 năm 2023\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 0}),\n",
       "  '1dcd93fe-8620-4be9-b8b3-65caa5424425': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nĐặt vấn đề\\nMột số nhận xét\\n•Ta thường thấy rằng đối với một bài toán cụ thể (bài toán\\nsắp xếp hay tìm kiếm chẳng hạn), có những thuật toán nhanh\\nhơn rất nhiều so với những thuật toán khác cũng dùng để giải\\nbài toán đó, đặc bi', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 1}),\n",
       "  '2f7923fe-02cd-49f7-94bd-3ed0bbf65538': Document(page_content='�ó, đặc biệt là cho các trường hợp mà dữ kiện đầu\\nvào có kích thước lớn.\\n•Ta cần tìm mô hình toán học để diễn tả tượng này.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 1}),\n",
       "  '8ac719ae-30e7-482c-9649-8b160d9e9c5c': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nĐặt vấn đề\\nMột số nhận xét\\n•Mô hình cần tìm về thời gian chạy của thuật toán sẽ so sánh\\ncác sự khác biệt LỚN giữa các thuật toán.\\n•Do đó ta sẽ bỏ qua các thừa số hằng và số hạng có bậc thấp.\\n(Tuy nhiên, điều này không có nghĩa', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 2}),\n",
       "  'e032fc72-6a2e-437a-a381-362f63f0e91b': Document(page_content=' không có nghĩa là các thừa số hằng\\nkhông đóng vai trò quan trọng trong thực hành.)\\n•Mô hình của ta sẽ cho thấy rằng thuật toán nào sẽ hiệu quả\\nhơn hoặc kém hiệu quả một cách đáng kể so với các thuật\\ntoán khác khi dữ kiện đầu vào có kích thước đủ lớn.\\nNguyễn An Khương O-lớn, Độ tăng của hàm', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 2}),\n",
       "  '95e5e461-ee6c-4271-b76a-f27edb2ab7a4': Document(page_content='�ng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 2}),\n",
       "  'b6df4940-cff6-4d5b-9ad2-306a12f5edab': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nĐặt vấn đề\\nVD minh họa\\n•Giả sử ta có hai thuật toán có cùng dữ kiện đầu vào kích\\nthước n.\\n•Thuật toán đầu có thời gian chạy là T1(n) =n3,\\n•Thuật toán thứ hai có thời gian chạy là T2(n) =5n2+5.\\n•Thuật toán nào nhanh hơn?\\nNguyễn An Khương O-lớn, Độ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 3}),\n",
       "  'f631e59b-1dd1-4092-bc67-bdcd4b37df79': Document(page_content='ớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 3}),\n",
       "  'b4a48a6c-d974-49a3-91f8-eb8cd3dc7dff': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nĐặt vấn đề\\nVD minh họa\\n•Dễ thấy rằng với nđủ lớn, T1chậm hơn nhiều so với T2.\\n(Nhưng nó chỉ chậm hơn kể từ một giá trị bước ngoặc nào đó\\n(kể từ N=?).)\\n•Vớinđủ lớn, ta có thể bỏ qua thừa số hằng và các số h�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 4}),\n",
       "  'bf8a711e-1e96-462b-9b89-422f6da87a43': Document(page_content='à các số hạng\\nbậc thấp trong T2(n) =5n2+5.\\n•Thậm chí nếu T2(n) =106n2+103nthì với nthuật toán thứ\\nhai vẫn sẽ chạy nhanh hơn.\\n•Những quan sát này sẽ dẫn ta đến định nghĩa chính xác cho\\nbức tranh \"LỚN\" về thời gian chạy của thuật toán như sau.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân t', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 4}),\n",
       "  'e10dab37-798b-4452-bb89-f5478cc7aa42': Document(page_content='�a hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 4}),\n",
       "  '1ef0c407-c066-41c2-a829-d689055d1c6f': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nMô hình\\nĐịnh nghĩa O-lớn\\nDefinition\\nHàm T(n)được gọi là O-lớn của f(n), kí hiệu T(n)∈O(f(n))\\nnếu tồn tại các hằng số dương cvàNsao cho với mọi n≥Nta có\\nT(n)≤cf(n).\\n•Bài tập (dễ): CMR khái niệm O-lớn này thật sự nêu được bả', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 5}),\n",
       "  'c39f6b1c-84af-4e76-9f4b-f0d8a15155d5': Document(page_content='ược bản\\nchất về độ tăng của hàm khi dữ kiện đầu vào có kích thước\\nlớn và bỏ qua được các nhân tử hằng và các số hạng bậc thấp.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 5}),\n",
       "  '102062bb-70b0-47bb-9fe9-bdd0b66de10e': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nMô hình\\nMột số tính chất của O-lớn\\nVề mặt toán học, ta có thể chứng minh được rằng:\\n•Với hằng số dương bất kì d∈Rta có f(n)∈O(df(n))và\\ndf(n)∈O(f(n));và•Nếu limn→∞h(n)\\ng(n)=0 thì g(n) +h(n)∈O(g(n)).\\nNguyễn An Khương O-lớn, Đ�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 6}),\n",
       "  'c9f3f1b0-752a-4e38-9fdc-b4da72d8bbc3': Document(page_content='-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 6}),\n",
       "  '27e306cd-d393-49db-b31a-1ccfac89b346': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nMô hình\\nMột số tính chất của O-lớn\\nVề mặt toán học, ta có thể chứng minh được rằng:\\n•Với hằng số dương bất kì d∈Rta có f(n)∈O(df(n))và\\ndf(n)∈O(f(n));và\\n•Nếu limn→∞h(n)\\ng(n)=0 thì g(n) +h(n)∈O(g(n)).\\nNguyễn An Khương O-lớn, Đ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 7}),\n",
       "  '88c8d29b-4872-4db4-a9d1-2915daa7cb8f': Document(page_content=' O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 7}),\n",
       "  '5d2f2019-e3e1-4a6d-bf9e-6d7fda2012e2': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nMô hình\\nMột số tính chất của O-lớn\\n•Khái niệm O-lớn cho phép ta xếp hạng được tính hiệu quả\\ncủa thuật toán với những dữ liệu đầu vào có kích thước lớn:•O(1)⊂O(log(n))⊂O(n)⊂O(nlog(n))⊂O(n2)⊂\\nO(n3)⊂O(n4)⊂ · ⊂ O(', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 8}),\n",
       "  '3a0d4bac-eae3-4ca8-b495-6ec659db3780': Document(page_content=')⊂ · ⊂ O(2n)⊂O(n!)⊂O(nn).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 8}),\n",
       "  '6ca47f90-3259-4b17-8090-5f4932360d35': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nMô hình\\nMột số tính chất của O-lớn\\n•Khái niệm O-lớn cho phép ta xếp hạng được tính hiệu quả\\ncủa thuật toán với những dữ liệu đầu vào có kích thước lớn:\\n•O(1)⊂O(log(n))⊂O(n)⊂O(nlog(n))⊂O(n2)⊂\\nO(n3)⊂O(n4)⊂ · ⊂ O', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 9}),\n",
       "  '2bfad0ae-d135-4d69-89eb-ffab6845e34c': Document(page_content='4)⊂ · ⊂ O(2n)⊂O(n!)⊂O(nn).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 9}),\n",
       "  '92f08f61-c22d-4d88-9d43-3e57b80a71ad': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nCần đếm những gì\\n•Bây giờ, khi đã có phương pháp xếp hạng thời gian chạy của\\ncác thuật toán thì ta cần phải xem xét thời gian chạy của một\\nthuật toán cụ thể thuộc vào lớp hàm nào.•Việc phân tích mã của chương trình có liên qu', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 10}),\n",
       "  'a8b1b5fc-d47a-422f-982a-fa65f1e8c7d0': Document(page_content='ình có liên quan đến việc xác\\nđịnh một cách thức thích hợp để đếm số lượng phép tính cần\\nthực hiện (kết quả sẽ là một hàm theo kích thước của dữ kiện\\nđầu vào.)\\n•Ví dụ, trong vòng lặp sau đây, cách đếm thích hợp là đếm số\\nlượng phép cộng cần thực hiện, và kích thư�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 10}),\n",
       "  '59e4f0e4-0702-4fc5-8495-f55490f36d9d': Document(page_content=', và kích thước của dữ liệu đầu\\nvào sẽ là my_list.\\nfor i in range(len(my_list)):\\nsum += my_list(i)\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 10}),\n",
       "  '71b55ff1-f45b-4e28-b57c-bd2b2f12b662': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nCần đếm những gì\\n•Bây giờ, khi đã có phương pháp xếp hạng thời gian chạy của\\ncác thuật toán thì ta cần phải xem xét thời gian chạy của một\\nthuật toán cụ thể thuộc vào lớp hàm nào.\\n•Việc phân tích mã của chương trình có liên', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 11}),\n",
       "  'b60cf878-3521-4086-8910-fc8976d7471a': Document(page_content=' trình có liên quan đến việc xác\\nđịnh một cách thức thích hợp để đếm số lượng phép tính cần\\nthực hiện (kết quả sẽ là một hàm theo kích thước của dữ kiện\\nđầu vào.)•Ví dụ, trong vòng lặp sau đây, cách đếm thích hợp là đếm số\\nlượng phép cộng cần thực hiện, và kích thư�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 11}),\n",
       "  '51af87ed-f0a1-4322-b824-381c3828d3e5': Document(page_content=', và kích thước của dữ liệu đầu\\nvào sẽ là my_list.\\nfor i in range(len(my_list)):\\nsum += my_list(i)\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 11}),\n",
       "  '75de634b-f301-4f07-bf56-17c492004e47': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nCần đếm những gì\\n•Bây giờ, khi đã có phương pháp xếp hạng thời gian chạy của\\ncác thuật toán thì ta cần phải xem xét thời gian chạy của một\\nthuật toán cụ thể thuộc vào lớp hàm nào.\\n•Việc phân tích mã của chương trình có liên', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 12}),\n",
       "  'bf9cf1e2-1a3c-4bb3-b056-02994796274d': Document(page_content=' trình có liên quan đến việc xác\\nđịnh một cách thức thích hợp để đếm số lượng phép tính cần\\nthực hiện (kết quả sẽ là một hàm theo kích thước của dữ kiện\\nđầu vào.)\\n•Ví dụ, trong vòng lặp sau đây, cách đếm thích hợp là đếm số\\nlượng phép cộng cần thực hiện, và kích thư', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 12}),\n",
       "  '827aa545-3ae3-49ca-993d-daefd9ab43fe': Document(page_content='n, và kích thước của dữ liệu đầu\\nvào sẽ là my_list.for i in range(len(my_list)):\\nsum += my_list(i)\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 12}),\n",
       "  '53008138-1ae7-4c9f-8907-d8c0f95b9b13': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nCần đếm những gì\\n•Bây giờ, khi đã có phương pháp xếp hạng thời gian chạy của\\ncác thuật toán thì ta cần phải xem xét thời gian chạy của một\\nthuật toán cụ thể thuộc vào lớp hàm nào.\\n•Việc phân tích mã của chương trình có liên', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 13}),\n",
       "  '76dc8813-3158-45d9-9b2b-8977498936f4': Document(page_content=' trình có liên quan đến việc xác\\nđịnh một cách thức thích hợp để đếm số lượng phép tính cần\\nthực hiện (kết quả sẽ là một hàm theo kích thước của dữ kiện\\nđầu vào.)\\n•Ví dụ, trong vòng lặp sau đây, cách đếm thích hợp là đếm số\\nlượng phép cộng cần thực hiện, và kích thư', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 13}),\n",
       "  '4a2590de-a289-4dbd-9af2-5414612e20bb': Document(page_content='n, và kích thước của dữ liệu đầu\\nvào sẽ là my_list.\\nfor i in range(len(my_list)):\\nsum += my_list(i)\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 13}),\n",
       "  'f89a0157-1db6-4fcc-90d9-1de42b8a2130': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nCần đếm những gì\\n•Khi đã biết cần tính những gì thì việc còn lại chỉ là đếm!\\nfor i in range(len(my_list)):\\nsum += my_list(i)•Trong đoạn mã trên, nếu danh sách đầu vào có độ dài nthì sẽ\\ncónphép cộng được thực hiện, và do đó độ phức', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 14}),\n",
       "  'd5467b93-4b08-4763-aea2-eb1b7470bfd6': Document(page_content=' độ phức tạp sẽ là\\nO(n).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 14}),\n",
       "  '0586baf8-988f-46a6-8f4f-3d5a82278ccd': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nCần đếm những gì\\n•Khi đã biết cần tính những gì thì việc còn lại chỉ là đếm!\\nfor i in range(len(my_list)):\\nsum += my_list(i)\\n•Trong đoạn mã trên, nếu danh sách đầu vào có độ dài nthì sẽ\\ncónphép cộng được thực hiện, và do đó độ phứ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 15}),\n",
       "  '34f861c0-e5af-4835-a2d1-f85c56431faa': Document(page_content='ó độ phức tạp sẽ là\\nO(n).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 15}),\n",
       "  'ab43bd9a-f881-4ad8-b97f-360a53fce89e': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích vòng lặp\\n•Nói chung, để tìm thời gian chạy của một vòng lặp ta cần xác\\nđịnh thời gian chạy bên trong thân của vòng lặp rồi nhân nó\\nvới số lần thực thi của vòng lặp đó.(Luôn nhớ đến kích thước của dữ li�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 16}),\n",
       "  '1a772545-7635-49e4-bb81-4f30d7455ce4': Document(page_content='ủa dữ liệu đầu vào khi tính độ\\nphức tạp!)\\nfor i in range(len(my_list)):\\nsum += my_list(i)•Chẳng hạn trong đoạn mã trên, mỗi lần vòng lặp thực thi một\\nphép toán cộng, do đó trong thân của vòng lặp có độ phức\\ntạp là O(1);\\n•Nếu danh sách đầu vào có kích thước nthì vòng lặp thực thi\\nnlần, nên tổng cộng th', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 16}),\n",
       "  '3abff6de-6529-403a-bb56-2b40482a890d': Document(page_content='ổng cộng thời gian chạy của vòng lặp sẽ là\\n1·n∈O(n).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 16}),\n",
       "  'ca9ebf12-ecf9-4ac9-873f-2b75707f5ba9': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích vòng lặp\\n•Nói chung, để tìm thời gian chạy của một vòng lặp ta cần xác\\nđịnh thời gian chạy bên trong thân của vòng lặp rồi nhân nó\\nvới số lần thực thi của vòng lặp đó.\\n(Luôn nhớ đến kích thước của dữ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 17}),\n",
       "  'ec1e4413-9728-43de-a34a-e4363f49d045': Document(page_content='c của dữ liệu đầu vào khi tính độ\\nphức tạp!)\\nfor i in range(len(my_list)):\\nsum += my_list(i)•Chẳng hạn trong đoạn mã trên, mỗi lần vòng lặp thực thi một\\nphép toán cộng, do đó trong thân của vòng lặp có độ phức\\ntạp là O(1);\\n•Nếu danh sách đầu vào có kích thước nthì vòng lặp thực thi\\nnlần, nên tổng cộ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 17}),\n",
       "  'c708adef-30cf-4b67-afb5-4bc66d0592d9': Document(page_content='n tổng cộng thời gian chạy của vòng lặp sẽ là\\n1·n∈O(n).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 17}),\n",
       "  '2773b19f-f8b5-4e19-a4c5-833bf6cf8f0b': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích vòng lặp\\n•Nói chung, để tìm thời gian chạy của một vòng lặp ta cần xác\\nđịnh thời gian chạy bên trong thân của vòng lặp rồi nhân nó\\nvới số lần thực thi của vòng lặp đó.\\n(Luôn nhớ đến kích thước của dữ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 18}),\n",
       "  'd78c326c-7bb3-4878-ac51-5bebffcfae4f': Document(page_content='c của dữ liệu đầu vào khi tính độ\\nphức tạp!)\\nfor i in range(len(my_list)):\\nsum += my_list(i)\\n•Chẳng hạn trong đoạn mã trên, mỗi lần vòng lặp thực thi một\\nphép toán cộng, do đó trong thân của vòng lặp có độ phức\\ntạp là O(1);•Nếu danh sách đầu vào có kích thước nthì vòng lặp thực thi\\nnlần, nên tổng cộ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 18}),\n",
       "  'db19e73a-defe-4d0e-b210-ceed3ebdf0a1': Document(page_content='n tổng cộng thời gian chạy của vòng lặp sẽ là\\n1·n∈O(n).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 18}),\n",
       "  '77fa0c5a-2b74-4bd2-a7a4-77054500a3eb': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích vòng lặp\\n•Nói chung, để tìm thời gian chạy của một vòng lặp ta cần xác\\nđịnh thời gian chạy bên trong thân của vòng lặp rồi nhân nó\\nvới số lần thực thi của vòng lặp đó.\\n(Luôn nhớ đến kích thước của dữ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 19}),\n",
       "  '742c2d91-4557-4d54-9ce4-b1ba9a97170a': Document(page_content='c của dữ liệu đầu vào khi tính độ\\nphức tạp!)\\nfor i in range(len(my_list)):\\nsum += my_list(i)\\n•Chẳng hạn trong đoạn mã trên, mỗi lần vòng lặp thực thi một\\nphép toán cộng, do đó trong thân của vòng lặp có độ phức\\ntạp là O(1);\\n•Nếu danh sách đầu vào có kích thước nthì vòng lặp thực thi\\nnlần, nên tổng c�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 19}),\n",
       "  'b9000e53-a7aa-43b6-9122-8677ad3aa5c3': Document(page_content='ên tổng cộng thời gian chạy của vòng lặp sẽ là\\n1·n∈O(n).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 19}),\n",
       "  '976c3568-7fca-4d99-863b-ba7c3367a548': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích vòng lặp\\n•Đối với các vòng lặp lồng nhau ta cũng làm theo nguyên tắc\\ntương tự: Bắt đầu tính từ bên trong ra ngoài.\\nfor i in range(len(my_list)):\\nfor j in range(len(my_list)):\\ndouble_sum += my_list(i)•Tại mỗi bước, vòng lặp bên trong thực thi một phép toán\\ncộ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 20}),\n",
       "  '07267b90-e108-4ddb-875c-9baf870a45bd': Document(page_content=' phép toán\\ncộng, do đó thân của vòng lặp bên trong có độ phức tạp là\\nO(1).\\n•Nếu danh sách đầu vào có kích thước nthì vòng lặp bên\\ntrong thực thi nlần. Do đó độ phức tạp của vòng lặp bên\\ntrong là 1 ·n∈O(n).\\n•Vòng lặp ngoài cũng sẽ thực thi ncho danh sách đầu vào kích\\nthước nvà do đó độ phức tạ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 20}),\n",
       "  '00e26709-3d16-4262-bb14-bf78fec1db57': Document(page_content='� phức tạp của toàn bộ hai vòng lặp sẽ là\\nn·n∈O(n2).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 20}),\n",
       "  '77a76645-e402-47d7-8b9f-607ce88df269': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích vòng lặp\\n•Đối với các vòng lặp lồng nhau ta cũng làm theo nguyên tắc\\ntương tự: Bắt đầu tính từ bên trong ra ngoài.\\nfor i in range(len(my_list)):\\nfor j in range(len(my_list)):\\ndouble_sum += my_list(i)\\n•Tại mỗi bước, vòng lặp bên trong thực thi một phép toán\\nc�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 21}),\n",
       "  '9d08817b-92bf-405d-9d3d-ed32365d6d86': Document(page_content='t phép toán\\ncộng, do đó thân của vòng lặp bên trong có độ phức tạp là\\nO(1).•Nếu danh sách đầu vào có kích thước nthì vòng lặp bên\\ntrong thực thi nlần. Do đó độ phức tạp của vòng lặp bên\\ntrong là 1 ·n∈O(n).\\n•Vòng lặp ngoài cũng sẽ thực thi ncho danh sách đầu vào kích\\nthước nvà do đó độ phức tạ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 21}),\n",
       "  '89ff0f79-d19d-4927-9b45-126ceaffae11': Document(page_content='� phức tạp của toàn bộ hai vòng lặp sẽ là\\nn·n∈O(n2).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 21}),\n",
       "  '164f04c3-17dc-4293-9a5a-98cb9f6d3b5b': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích vòng lặp\\n•Đối với các vòng lặp lồng nhau ta cũng làm theo nguyên tắc\\ntương tự: Bắt đầu tính từ bên trong ra ngoài.\\nfor i in range(len(my_list)):\\nfor j in range(len(my_list)):\\ndouble_sum += my_list(i)\\n•Tại mỗi bước, vòng lặp bên trong thực thi một phép toán\\nc�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 22}),\n",
       "  'c736ea77-e392-4b45-8b3f-3cce7f73ebfc': Document(page_content='t phép toán\\ncộng, do đó thân của vòng lặp bên trong có độ phức tạp là\\nO(1).\\n•Nếu danh sách đầu vào có kích thước nthì vòng lặp bên\\ntrong thực thi nlần. Do đó độ phức tạp của vòng lặp bên\\ntrong là 1 ·n∈O(n).•Vòng lặp ngoài cũng sẽ thực thi ncho danh sách đầu vào kích\\nthước nvà do đó độ phức tạ', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 22}),\n",
       "  '4e77b962-7486-4ac9-a72a-576ee337b984': Document(page_content='� phức tạp của toàn bộ hai vòng lặp sẽ là\\nn·n∈O(n2).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 22}),\n",
       "  'f0a14aa6-071d-4039-bf8b-71863a04a0a5': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích vòng lặp\\n•Đối với các vòng lặp lồng nhau ta cũng làm theo nguyên tắc\\ntương tự: Bắt đầu tính từ bên trong ra ngoài.\\nfor i in range(len(my_list)):\\nfor j in range(len(my_list)):\\ndouble_sum += my_list(i)\\n•Tại mỗi bước, vòng lặp bên trong thực thi một phép toán\\nc�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 23}),\n",
       "  '7fb19b9b-2b0f-4652-ae7d-6f66dbab3a12': Document(page_content='t phép toán\\ncộng, do đó thân của vòng lặp bên trong có độ phức tạp là\\nO(1).\\n•Nếu danh sách đầu vào có kích thước nthì vòng lặp bên\\ntrong thực thi nlần. Do đó độ phức tạp của vòng lặp bên\\ntrong là 1 ·n∈O(n).\\n•Vòng lặp ngoài cũng sẽ thực thi ncho danh sách đầu vào kích\\nthước nvà do đó độ phức t�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 23}),\n",
       "  '803c29f5-dca8-4eea-9595-2574e37881d0': Document(page_content='�� phức tạp của toàn bộ hai vòng lặp sẽ là\\nn·n∈O(n2).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 23}),\n",
       "  'cf2adb6a-6d03-405e-bf66-cb8032ab8a44': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích tổ hợp các vòng lặp\\n•Điều gì sẽ xảy ra khi vòng lặp không thực thi cùng số lần\\ntrong mỗi bước lặp?\\narbitrary_number = 0\\nfor i in range(len(my_list)):\\nfor j in range(i):\\narbitrary_number += 10•Ta cũng sẽ tuân thủ nguyên tắc chung như trường hợp trên,\\nnhưng tính toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 24}),\n",
       "  '8340d133-a942-4cd2-9f93-690f875621e1': Document(page_content='nhưng tính toán phức tạp hơn một chút.\\n•Vòng lặp bên trong thực thi O(1)lần\\n•Ở lần lặp lại thứ inó thực thi ilần. Do đó độ phức tạp của\\nnó sẽ là O(i·1) =O(i).\\n•Vòng lặp ngoài thực thi nlần, do đó tổng số phép toán cần\\nthực hiện sẽ là 1 +2+3+. . .+n=Pn\\ni=1i=n(n+1)\\n2.\\nNguyễn An Khư', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 24}),\n",
       "  'be5f437d-d0e8-4f79-b3d7-c64e92442ca4': Document(page_content='Nguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 24}),\n",
       "  'd1d14250-5c73-4fe5-b73f-190d906dcef6': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích tổ hợp các vòng lặp\\n•Điều gì sẽ xảy ra khi vòng lặp không thực thi cùng số lần\\ntrong mỗi bước lặp?\\narbitrary_number = 0\\nfor i in range(len(my_list)):\\nfor j in range(i):\\narbitrary_number += 10\\n•Ta cũng sẽ tuân thủ nguyên tắc chung như trường hợp trên,\\nnhưng tính to', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 25}),\n",
       "  '0061ff67-a17b-4082-a63a-dc99d93cfa26': Document(page_content='\\nnhưng tính toán phức tạp hơn một chút.•Vòng lặp bên trong thực thi O(1)lần\\n•Ở lần lặp lại thứ inó thực thi ilần. Do đó độ phức tạp của\\nnó sẽ là O(i·1) =O(i).\\n•Vòng lặp ngoài thực thi nlần, do đó tổng số phép toán cần\\nthực hiện sẽ là 1 +2+3+. . .+n=Pn\\ni=1i=n(n+1)\\n2.\\nNguyễn An Khư', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 25}),\n",
       "  '5ae4ec1b-617c-4298-a638-c86f47f8f4df': Document(page_content='Nguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 25}),\n",
       "  '9c7fc029-5b9b-4dbb-ba43-17621ce6c97d': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích tổ hợp các vòng lặp\\n•Điều gì sẽ xảy ra khi vòng lặp không thực thi cùng số lần\\ntrong mỗi bước lặp?\\narbitrary_number = 0\\nfor i in range(len(my_list)):\\nfor j in range(i):\\narbitrary_number += 10\\n•Ta cũng sẽ tuân thủ nguyên tắc chung như trường hợp trên,\\nnhưng tính to', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 26}),\n",
       "  'e7f2c969-ac39-4474-8555-2f480041c1ed': Document(page_content='\\nnhưng tính toán phức tạp hơn một chút.\\n•Vòng lặp bên trong thực thi O(1)lần•Ở lần lặp lại thứ inó thực thi ilần. Do đó độ phức tạp của\\nnó sẽ là O(i·1) =O(i).\\n•Vòng lặp ngoài thực thi nlần, do đó tổng số phép toán cần\\nthực hiện sẽ là 1 +2+3+. . .+n=Pn\\ni=1i=n(n+1)\\n2.\\nNguyễn An Khư', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 26}),\n",
       "  'f657e497-b508-45df-9679-7c619e10535a': Document(page_content='Nguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 26}),\n",
       "  'c7ebf265-8ef8-4816-abea-e04814838307': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích tổ hợp các vòng lặp\\n•Điều gì sẽ xảy ra khi vòng lặp không thực thi cùng số lần\\ntrong mỗi bước lặp?\\narbitrary_number = 0\\nfor i in range(len(my_list)):\\nfor j in range(i):\\narbitrary_number += 10\\n•Ta cũng sẽ tuân thủ nguyên tắc chung như trường hợp trên,\\nnhưng tính to', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 27}),\n",
       "  '26c4c862-0544-463d-bad0-d8b8beed6a8b': Document(page_content='\\nnhưng tính toán phức tạp hơn một chút.\\n•Vòng lặp bên trong thực thi O(1)lần\\n•Ở lần lặp lại thứ inó thực thi ilần. Do đó độ phức tạp của\\nnó sẽ là O(i·1) =O(i).•Vòng lặp ngoài thực thi nlần, do đó tổng số phép toán cần\\nthực hiện sẽ là 1 +2+3+. . .+n=Pn\\ni=1i=n(n+1)\\n2.\\nNguyễn An Khư', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 27}),\n",
       "  'f78f7eb9-6a93-42e4-9862-936fb3e5bb59': Document(page_content='Nguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 27}),\n",
       "  'eb424c6c-0558-461a-b11e-b58d09a6071d': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích tổ hợp các vòng lặp\\n•Điều gì sẽ xảy ra khi vòng lặp không thực thi cùng số lần\\ntrong mỗi bước lặp?\\narbitrary_number = 0\\nfor i in range(len(my_list)):\\nfor j in range(i):\\narbitrary_number += 10\\n•Ta cũng sẽ tuân thủ nguyên tắc chung như trường hợp trên,\\nnhưng tính to', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 28}),\n",
       "  '1013ec3f-1dae-4655-bee6-6ed9cd300822': Document(page_content='\\nnhưng tính toán phức tạp hơn một chút.\\n•Vòng lặp bên trong thực thi O(1)lần\\n•Ở lần lặp lại thứ inó thực thi ilần. Do đó độ phức tạp của\\nnó sẽ là O(i·1) =O(i).\\n•Vòng lặp ngoài thực thi nlần, do đó tổng số phép toán cần\\nthực hiện sẽ là 1 +2+3+. . .+n=Pn\\ni=1i=n(n+1)\\n2.\\nNguyễn An Kh�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 28}),\n",
       "  '7c4b28c4-f06e-4de0-baad-14152971baf6': Document(page_content='\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 28}),\n",
       "  'fcb3573a-bd1c-4211-ac73-cebbe60fe5bb': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích tổ hợp các vòng lặp\\n•Nhắc lại rằng các nhân tử hằng và các hạng tử bậc thấp có\\nthể được bỏ qua trong việc đánh giá O-lớn.•Cách đơn giản nhất để mô tả thuật toán có độ phức tạp\\nO(n(n+1)\\n2)là g', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 29}),\n",
       "  '701823ad-8ce5-44d4-a0c2-1cc86d222f0e': Document(page_content='n+1)\\n2)là gì?\\n•(Gợi ý:n(n+1)\\n2=n2\\n2+n\\n2.)\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 29}),\n",
       "  'd4a62a52-e8b7-43f3-9ea8-6b071c360963': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích tổ hợp các vòng lặp\\n•Nhắc lại rằng các nhân tử hằng và các hạng tử bậc thấp có\\nthể được bỏ qua trong việc đánh giá O-lớn.\\n•Cách đơn giản nhất để mô tả thuật toán có độ phức tạp\\nO(n(n+1)\\n2)là', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 30}),\n",
       "  '75b4fe0c-4e48-4641-8452-39eb97f81e6b': Document(page_content='(n+1)\\n2)là gì?•(Gợi ý:n(n+1)\\n2=n2\\n2+n\\n2.)\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 30}),\n",
       "  '11d593f8-e3ea-40b1-983b-49408979a700': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích tổ hợp các vòng lặp\\n•Nhắc lại rằng các nhân tử hằng và các hạng tử bậc thấp có\\nthể được bỏ qua trong việc đánh giá O-lớn.\\n•Cách đơn giản nhất để mô tả thuật toán có độ phức tạp\\nO(n(n+1)\\n2)là', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 31}),\n",
       "  '0cbbeded-9348-4e82-ae96-07eb9440db7c': Document(page_content='(n+1)\\n2)là gì?\\n•(Gợi ý:n(n+1)\\n2=n2\\n2+n\\n2.)\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 31}),\n",
       "  '49848715-9cd2-4b4d-8ea6-d2c1674206a9': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các khối mã tiếp nối nhau\\n•Khi phân tích độ phức tạp của một dãy liên tiếp các khối mã\\nta chỉ đơn giản là lấy tổng các O-lớn của từng khối này để\\nđược độ phức tạp của toàn bộ.\\narbitrary_number = 0\\nfor i in range(len(', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 32}),\n",
       "  'f9f1e1dd-b6d7-46c7-b910-183cf2ebdda6': Document(page_content=' = 0\\nfor i in range(len(my_list)):\\narbitrary_number += 10\\nfor j in range(len(my_list)**2)\\nfor i in range(10):\\narbitrary_number -= 20BTVN:\\n•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ\\nnhất?\\n•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ hai?\\n•Độ phức tạp của đoạn mã trên sẽ là tổng các O-lớn của hai\\nvòng lặp trên.\\nNguyễn', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 32}),\n",
       "  '778d13f8-7ea9-4b17-bc1c-eb6386a7bbe4': Document(page_content='ên.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 32}),\n",
       "  '4d813894-0989-4941-b43f-109ead34a508': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các khối mã tiếp nối nhau\\n•Khi phân tích độ phức tạp của một dãy liên tiếp các khối mã\\nta chỉ đơn giản là lấy tổng các O-lớn của từng khối này để\\nđược độ phức tạp của toàn bộ.\\narbitrary_number = 0\\nfor i in range(len(', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 33}),\n",
       "  'aa521642-5586-44c8-b1e5-632a32bad9a4': Document(page_content=' = 0\\nfor i in range(len(my_list)):\\narbitrary_number += 10\\nfor j in range(len(my_list)**2)\\nfor i in range(10):\\narbitrary_number -= 20\\nBTVN:•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ\\nnhất?\\n•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ hai?\\n•Độ phức tạp của đoạn mã trên sẽ là tổng các O-lớn của hai\\nvòng lặp trên.\\nNguyễn', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 33}),\n",
       "  '67d45bfe-39c2-42cf-a7e6-f799de829aa9': Document(page_content='ên.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 33}),\n",
       "  'f2b262d0-6b1d-4f83-82df-c4eb71b300dd': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các khối mã tiếp nối nhau\\n•Khi phân tích độ phức tạp của một dãy liên tiếp các khối mã\\nta chỉ đơn giản là lấy tổng các O-lớn của từng khối này để\\nđược độ phức tạp của toàn bộ.\\narbitrary_number = 0\\nfor i in range(len(', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 34}),\n",
       "  '9efe9484-6a04-4b33-a996-5fe6df13915a': Document(page_content=' = 0\\nfor i in range(len(my_list)):\\narbitrary_number += 10\\nfor j in range(len(my_list)**2)\\nfor i in range(10):\\narbitrary_number -= 20\\nBTVN:\\n•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ\\nnhất?•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ hai?\\n•Độ phức tạp của đoạn mã trên sẽ là tổng các O-lớn của hai\\nvòng lặp trên.\\nNguyễn', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 34}),\n",
       "  '00557b13-bc0c-45a5-aaaf-a0aa260d5795': Document(page_content='ên.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 34}),\n",
       "  '852f5857-6214-4010-b9b5-21a2f4ca6e6b': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các khối mã tiếp nối nhau\\n•Khi phân tích độ phức tạp của một dãy liên tiếp các khối mã\\nta chỉ đơn giản là lấy tổng các O-lớn của từng khối này để\\nđược độ phức tạp của toàn bộ.\\narbitrary_number = 0\\nfor i in range(len(', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 35}),\n",
       "  'e20078b2-a127-40ed-849f-a4f13f71b56a': Document(page_content=' = 0\\nfor i in range(len(my_list)):\\narbitrary_number += 10\\nfor j in range(len(my_list)**2)\\nfor i in range(10):\\narbitrary_number -= 20\\nBTVN:\\n•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ\\nnhất?\\n•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ hai?•Độ phức tạp của đoạn mã trên sẽ là tổng các O-lớn của hai\\nvòng lặp trên.\\nNguyễn', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 35}),\n",
       "  '243138db-811d-4353-8b61-d3ea5c0b2f80': Document(page_content='ên.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 35}),\n",
       "  '117df730-931e-49bb-a698-c59270d9d577': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các khối mã tiếp nối nhau\\n•Khi phân tích độ phức tạp của một dãy liên tiếp các khối mã\\nta chỉ đơn giản là lấy tổng các O-lớn của từng khối này để\\nđược độ phức tạp của toàn bộ.\\narbitrary_number = 0\\nfor i in range(len(', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 36}),\n",
       "  'ac96f9c8-68a7-4e36-a728-986c0c09df94': Document(page_content=' = 0\\nfor i in range(len(my_list)):\\narbitrary_number += 10\\nfor j in range(len(my_list)**2)\\nfor i in range(10):\\narbitrary_number -= 20\\nBTVN:\\n•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ\\nnhất?\\n•Tìm số lượng phép toán cần thực hiện trong vòng lặp thứ hai?\\n•Độ phức tạp của đoạn mã trên sẽ là tổng các O-lớn của hai\\nvòng lặp trên. Nguyễn', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 36}),\n",
       "  '6025a6c5-2e7b-45b4-9645-f2fc41db1bf3': Document(page_content=' trên. Nguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 36}),\n",
       "  '15c3b36b-f1af-4b90-8b49-b476ca4ab396': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích cấu trúc “if else”\\n•Khi phân tích độ phức tạp của một đoạn mã có cấu\\ntrúcif else ta lấy số lượng phép toán là số lớn nhất trong số\\ncác trường hợp xảy ra.\\narbitrary_number = 0\\nif something_happened:\\nfor i in range(len(my_list)):\\nfor j in range(i):\\narbitrary_number += -0.1\\nelse:\\nfor i in', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 37}),\n",
       "  '5455426b-0b3a-4c80-ae07-9c03e3e2778b': Document(page_content='0.1\\nelse:\\nfor i in range(10000):\\narbitrary_number /= 2.0•Tính số lượng phép toán trong đoạn mã của khối if?\\n•Tính số lượng phép toán trong đoạn mã của khối else?\\n•Độ phức tạp của đoạn mã nói trên sẽ là kết quả lớn hơn khi n\\nđủ lớn.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 37}),\n",
       "  '890bab4b-7a23-41eb-8022-6ce893f243ba': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích cấu trúc “if else”\\n•Khi phân tích độ phức tạp của một đoạn mã có cấu\\ntrúcif else ta lấy số lượng phép toán là số lớn nhất trong số\\ncác trường hợp xảy ra.\\narbitrary_number = 0\\nif something_happened:\\nfor i in range(len(my_list)):\\nfor j in range(i):\\narbitrary_number += -0.1\\nelse:\\nfor i in', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 38}),\n",
       "  '57381be6-ad96-4f7b-9755-d785cdcec50c': Document(page_content='0.1\\nelse:\\nfor i in range(10000):\\narbitrary_number /= 2.0\\n•Tính số lượng phép toán trong đoạn mã của khối if?•Tính số lượng phép toán trong đoạn mã của khối else?\\n•Độ phức tạp của đoạn mã nói trên sẽ là kết quả lớn hơn khi n\\nđủ lớn.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 38}),\n",
       "  'cbf416e7-b4b3-4d29-9a78-f07624a489e4': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích cấu trúc “if else”\\n•Khi phân tích độ phức tạp của một đoạn mã có cấu\\ntrúcif else ta lấy số lượng phép toán là số lớn nhất trong số\\ncác trường hợp xảy ra.\\narbitrary_number = 0\\nif something_happened:\\nfor i in range(len(my_list)):\\nfor j in range(i):\\narbitrary_number += -0.1\\nelse:\\nfor i in', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 39}),\n",
       "  '6305b2aa-d5a7-4e31-8465-2005484d0ea1': Document(page_content='0.1\\nelse:\\nfor i in range(10000):\\narbitrary_number /= 2.0\\n•Tính số lượng phép toán trong đoạn mã của khối if?\\n•Tính số lượng phép toán trong đoạn mã của khối else?•Độ phức tạp của đoạn mã nói trên sẽ là kết quả lớn hơn khi n\\nđủ lớn.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 39}),\n",
       "  '04299f2e-8a92-4f47-bfcf-865330eb5c7c': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích cấu trúc “if else”\\n•Khi phân tích độ phức tạp của một đoạn mã có cấu\\ntrúcif else ta lấy số lượng phép toán là số lớn nhất trong số\\ncác trường hợp xảy ra.\\narbitrary_number = 0\\nif something_happened:\\nfor i in range(len(my_list)):\\nfor j in range(i):\\narbitrary_number += -0.1\\nelse:\\nfor i in', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 40}),\n",
       "  'd8cf0bc0-5f8b-419f-aa63-720060112e31': Document(page_content='0.1\\nelse:\\nfor i in range(10000):\\narbitrary_number /= 2.0\\n•Tính số lượng phép toán trong đoạn mã của khối if?\\n•Tính số lượng phép toán trong đoạn mã của khối else?\\n•Độ phức tạp của đoạn mã nói trên sẽ là kết quả lớn hơn khi n\\nđủ lớn.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 40}),\n",
       "  '729e843f-dc57-40a7-a018-9a129878b934': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm “call”\\n•Nếu trong đoạn mã của ta có chứa hàm “call” thì hàm O-lớn\\ncủa toàn bộ chương trình sẽ bằng tổng kích thước của các\\nhàm đầu vào.\\ndef add_a_list(some_list):\\nsum = 0\\nfor item in some_list:\\nsum += item\\nreturn sum\\narbitrary_number = 0\\nadd_a_list(input_list)\\nadd_a_list(input_list[', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 41}),\n",
       "  'f695c03c-c2dc-4264-aee0-9504edfb8878': Document(page_content='add_a_list(input_list[:1])\\nadd_a_list(input_list(:len(input_list)/2))\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 41}),\n",
       "  'afeb5de1-f1a4-4e6e-9b5c-cb3b27b16367': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm “call”\\n•Các hàm “call” add_a_list lần lượt có độ phức tạp O(n),\\nO(1), vàO(n\\n2).•Do đó độ phức tạp của toàn bộ thuật toán sẽ là\\nO(n+1+n\\n2) =O(n)).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 42}),\n",
       "  '75dd017c-0551-4712-ad5a-3a635299119b': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm “call”\\n•Các hàm “call” add_a_list lần lượt có độ phức tạp O(n),\\nO(1), vàO(n\\n2).\\n•Do đó độ phức tạp của toàn bộ thuật toán sẽ là\\nO(n+1+n\\n2) =O(n)).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 43}),\n",
       "  'de70ca38-1679-4070-814a-a5704ff0432d': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nÔn lại\\n•Ta đã tìm hiểu xong cách đánh giá độ phức tạp của các đoạn\\nmã có chứa:•Các vòng lặp lồng nhau hoặc liên tiếp.\\n•Các hàm “call”.\\n•Cấu trúc if else.\\n•Điều này sẽ giúp ta đánh giá tiệm cận được độ phức của m', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 44}),\n",
       "  'acdb27d4-fce6-4ca2-b913-b5924d3633d3': Document(page_content='ức của một\\nsố thuật toán sắp xếp quen thuộc trong trong trường hợp xấu\\nnhất ở phần sau.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 44}),\n",
       "  '8d75dd70-7ff0-4958-abeb-f34f70f0e760': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nÔn lại\\n•Ta đã tìm hiểu xong cách đánh giá độ phức tạp của các đoạn\\nmã có chứa:\\n•Các vòng lặp lồng nhau hoặc liên tiếp.•Các hàm “call”.\\n•Cấu trúc if else.\\n•Điều này sẽ giúp ta đánh giá tiệm cận được độ phức của m', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 45}),\n",
       "  '0bd60216-e716-499b-8f81-d8f44670806f': Document(page_content='ức của một\\nsố thuật toán sắp xếp quen thuộc trong trong trường hợp xấu\\nnhất ở phần sau.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 45}),\n",
       "  '8f538592-a1f8-4634-8fcb-495a75946199': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nÔn lại\\n•Ta đã tìm hiểu xong cách đánh giá độ phức tạp của các đoạn\\nmã có chứa:\\n•Các vòng lặp lồng nhau hoặc liên tiếp.\\n•Các hàm “call”.•Cấu trúc if else.\\n•Điều này sẽ giúp ta đánh giá tiệm cận được độ phức của m', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 46}),\n",
       "  '5096245d-e532-4b8a-803d-22a6e157dc1e': Document(page_content='ức của một\\nsố thuật toán sắp xếp quen thuộc trong trong trường hợp xấu\\nnhất ở phần sau.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 46}),\n",
       "  '2f04c9d0-e61a-477d-9b84-4b0ec1249620': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nÔn lại\\n•Ta đã tìm hiểu xong cách đánh giá độ phức tạp của các đoạn\\nmã có chứa:\\n•Các vòng lặp lồng nhau hoặc liên tiếp.\\n•Các hàm “call”.\\n•Cấu trúc if else.•Điều này sẽ giúp ta đánh giá tiệm cận được độ phức của m', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 47}),\n",
       "  '5c011834-fa98-4c7d-813c-5fb6ec7a5cb7': Document(page_content='ức của một\\nsố thuật toán sắp xếp quen thuộc trong trong trường hợp xấu\\nnhất ở phần sau.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 47}),\n",
       "  'e715916d-20bc-475b-9116-41eb0d99f907': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nÔn lại\\n•Ta đã tìm hiểu xong cách đánh giá độ phức tạp của các đoạn\\nmã có chứa:\\n•Các vòng lặp lồng nhau hoặc liên tiếp.\\n•Các hàm “call”.\\n•Cấu trúc if else.\\n•Điều này sẽ giúp ta đánh giá tiệm cận được độ phức của', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 48}),\n",
       "  'b66154b5-9753-484d-b007-b0da57b099a5': Document(page_content=' phức của một\\nsố thuật toán sắp xếp quen thuộc trong trong trường hợp xấu\\nnhất ở phần sau.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 48}),\n",
       "  '49a78cd4-5496-4baf-a1c1-209511acfc9c': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Về cơ bản, không có sự khác biệt giữa việc phân tích mã đệ\\nquy và các cấu trúc if ... else thông thường•Tuy nhiên vấn đề đếm số phép toán sẽ phức tạp hơn một chút.\\ndef int_to_bin(n):\\nif n < 2:\\nreturn str(n)\\nelse:\\nreturn int_to_bin(n / 2)', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 49}),\n",
       "  'ac4a1070-c8ef-4478-a72c-d53901bd79c3': Document(page_content=' int_to_bin(n / 2) + str(n % 2)•Ta sẽ đếm số phép toán trong đoạn mã trên và đo kích thước\\ncủa dữ kiện đầu vào như thế nào?\\n•Gọi thời gian chạy của thuật toán int_to_bin là hàm T(n).\\n•Khi đó dùng các quy tắc đếm đã xét ở trên, độ phức tạp của\\nthuật toán sẽ là O(max{1,T(n/2) +1}).\\nNguyễn An Khương O-l', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 49}),\n",
       "  '0dc9b2dd-a204-4bfd-83c0-f63efd212fa9': Document(page_content=' An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 49}),\n",
       "  '749a1d94-53a3-4bfe-ab2c-c0c1082bcf21': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Về cơ bản, không có sự khác biệt giữa việc phân tích mã đệ\\nquy và các cấu trúc if ... else thông thường\\n•Tuy nhiên vấn đề đếm số phép toán sẽ phức tạp hơn một chút.\\ndef int_to_bin(n):\\nif n < 2:\\nreturn str(n)\\nelse:\\nreturn int_to_bin(n / 2', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 50}),\n",
       "  '7115dc33-279e-40b7-acdd-48ad12bd85bd': Document(page_content='return int_to_bin(n / 2) + str(n % 2)•Ta sẽ đếm số phép toán trong đoạn mã trên và đo kích thước\\ncủa dữ kiện đầu vào như thế nào?\\n•Gọi thời gian chạy của thuật toán int_to_bin là hàm T(n).\\n•Khi đó dùng các quy tắc đếm đã xét ở trên, độ phức tạp của\\nthuật toán sẽ là O(max{1,T(n/2) +1}).\\nNguyễn An Khương O-', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 50}),\n",
       "  '36b97e5f-3415-4e3a-87f3-d53c44f6dcf6': Document(page_content='n An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 50}),\n",
       "  'a8d3e605-7337-41f0-af6c-8d0e2d874501': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Về cơ bản, không có sự khác biệt giữa việc phân tích mã đệ\\nquy và các cấu trúc if ... else thông thường\\n•Tuy nhiên vấn đề đếm số phép toán sẽ phức tạp hơn một chút.\\ndef int_to_bin(n):\\nif n < 2:\\nreturn str(n)\\nelse:\\nreturn int_to_bin(n / 2', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 51}),\n",
       "  'b12feb6e-1f9d-4337-ad2b-a8e2208031e2': Document(page_content='return int_to_bin(n / 2) + str(n % 2)\\n•Ta sẽ đếm số phép toán trong đoạn mã trên và đo kích thước\\ncủa dữ kiện đầu vào như thế nào?•Gọi thời gian chạy của thuật toán int_to_bin là hàm T(n).\\n•Khi đó dùng các quy tắc đếm đã xét ở trên, độ phức tạp của\\nthuật toán sẽ là O(max{1,T(n/2) +1}).\\nNguyễn An Khương O-', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 51}),\n",
       "  '600d9ce9-4956-494e-a146-53a432003d96': Document(page_content='n An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 51}),\n",
       "  '1086210e-866f-4d7c-9c9d-7556857b5cf6': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Về cơ bản, không có sự khác biệt giữa việc phân tích mã đệ\\nquy và các cấu trúc if ... else thông thường\\n•Tuy nhiên vấn đề đếm số phép toán sẽ phức tạp hơn một chút.\\ndef int_to_bin(n):\\nif n < 2:\\nreturn str(n)\\nelse:\\nreturn int_to_bin(n / 2', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 52}),\n",
       "  '615cb98f-a4de-4ffd-9a97-005053652610': Document(page_content='return int_to_bin(n / 2) + str(n % 2)\\n•Ta sẽ đếm số phép toán trong đoạn mã trên và đo kích thước\\ncủa dữ kiện đầu vào như thế nào?\\n•Gọi thời gian chạy của thuật toán int_to_bin là hàm T(n).•Khi đó dùng các quy tắc đếm đã xét ở trên, độ phức tạp của\\nthuật toán sẽ là O(max{1,T(n/2) +1}).\\nNguyễn An Khương O-', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 52}),\n",
       "  '935193ce-f4c5-4980-9097-0dc6f7cb0ba3': Document(page_content='n An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 52}),\n",
       "  '6b6b325f-da88-487b-b3c5-d15da4d42939': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Về cơ bản, không có sự khác biệt giữa việc phân tích mã đệ\\nquy và các cấu trúc if ... else thông thường\\n•Tuy nhiên vấn đề đếm số phép toán sẽ phức tạp hơn một chút.\\ndef int_to_bin(n):\\nif n < 2:\\nreturn str(n)\\nelse:\\nreturn int_to_bin(n / 2', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 53}),\n",
       "  'c5048ea2-87e4-45fd-a580-0e289e454b6c': Document(page_content='return int_to_bin(n / 2) + str(n % 2)\\n•Ta sẽ đếm số phép toán trong đoạn mã trên và đo kích thước\\ncủa dữ kiện đầu vào như thế nào?\\n•Gọi thời gian chạy của thuật toán int_to_bin là hàm T(n).\\n•Khi đó dùng các quy tắc đếm đã xét ở trên, độ phức tạp của\\nthuật toán sẽ là O(max{1,T(n/2) +1}).\\nNguyễn An Khương O', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 53}),\n",
       "  'c08d1ac0-57f2-4e0a-baf1-6c605ad7d234': Document(page_content='�n An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 53}),\n",
       "  'ac13cdbe-e7b7-45e3-b047-8607aa6b208f': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Hiển nhiên ta có T(n) = max {1,T(n/2) +1}) =T(n/2) +1.•Vậy ta sẽ thu được điều gì?\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 54}),\n",
       "  'b4d3dbef-db8b-46e4-966e-ec0ec6237c6d': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Hiển nhiên ta có T(n) = max {1,T(n/2) +1}) =T(n/2) +1.\\n•Vậy ta sẽ thu được điều gì?\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 55}),\n",
       "  'c8b0f440-33f9-4d63-abd6-fdebd0e819c6': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Để đơn giản ta xét trường hợp tồn tại ksao chon\\n2k=1.•VìT(n) =T(n/2) +1 nên ta có\\nT(n) = ( T(n/4) +1) +1\\n=T(n/4) +2\\n=T(n/8) +3\\n...\\n=T(n\\n2k) +k\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật to', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 56}),\n",
       "  '77d743d8-1c9f-48c1-ba1c-cc1dca41bf6a': Document(page_content=' tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 56}),\n",
       "  'e7edeea1-7c19-490e-b527-b71d8224bc82': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Để đơn giản ta xét trường hợp tồn tại ksao chon\\n2k=1.\\n•VìT(n) =T(n/2) +1 nên ta có\\nT(n) = ( T(n/4) +1) +1\\n=T(n/4) +2\\n=T(n/8) +3\\n...\\n=T(n\\n2k) +k\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 57}),\n",
       "  '6a6177bf-c26b-4ebf-bffe-6ca8a953edb2': Document(page_content='n tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 57}),\n",
       "  'b9277a62-3fff-41a9-ad43-ab875b39f10e': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Do tồn tại ksao chon\\n2k=1, nên\\nT(n) =T(1) +k•Nhưng T(1) =1, vàn\\n2k=1⇒n=2k⇒log2(n) =k\\n•Thế ngược vào phương trình trên, ta được\\nT(n) = log2(n) +1∈O(log(n)).\\nBTVN: CMR trong trường hợp không tồn tại kđển\\n2', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 58}),\n",
       "  'ba2f50a5-8dec-49ae-bdcd-3929fb6fa06e': Document(page_content='i kđển\\n2k=1 thì ta\\ncũng có T(n)∈O(log(n)).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 58}),\n",
       "  'e0160d91-29a3-484c-853e-85d8ff4627e0': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Do tồn tại ksao chon\\n2k=1, nên\\nT(n) =T(1) +k\\n•Nhưng T(1) =1, vàn\\n2k=1⇒n=2k⇒log2(n) =k•Thế ngược vào phương trình trên, ta được\\nT(n) = log2(n) +1∈O(log(n)).\\nBTVN: CMR trong trường hợp không tồn tại kđển\\n2', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 59}),\n",
       "  '7a61b81f-d355-4af9-90fc-d705d9b7e4eb': Document(page_content='i kđển\\n2k=1 thì ta\\ncũng có T(n)∈O(log(n)).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 59}),\n",
       "  '2641d33f-3e74-486f-9452-7510a6a0c672': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nPhân tích thuật toán\\nPhân tích các hàm đệ quy\\n•Do tồn tại ksao chon\\n2k=1, nên\\nT(n) =T(1) +k\\n•Nhưng T(1) =1, vàn\\n2k=1⇒n=2k⇒log2(n) =k\\n•Thế ngược vào phương trình trên, ta được\\nT(n) = log2(n) +1∈O(log(n)).\\nBTVN: CMR trong trường hợp không tồn tại kđển\\n', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 60}),\n",
       "  'd0161dcd-bb2c-4360-85ce-489650dcbcec': Document(page_content='�i kđển\\n2k=1 thì ta\\ncũng có T(n)∈O(log(n)).\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 60}),\n",
       "  '54240588-7cda-4d9c-ab8b-f2c70366fb24': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nBTVN\\nHãy tính chính xác số phép toán cần thực thi T(n)đối với từng\\nthuật toán sau.\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 61}),\n",
       "  '50613d62-448c-4a87-aa56-05e837e5d5e9': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nBTVN\\n•quick_sort (sắp xếp nhanh),•merge_sort (sắp xếp trộn),\\n•selection_sort (sắp xếp lựa chọn trực tiếp),\\n•insertion_sort (sắp xếp chèn trực tiếp),\\n•bubble_sort (sắp xếp nổi bọt)\\n•...\\n•các thuật toán tìm kiếm: nhị phân, theo chiều sâu, theo lựa\\nchọn tốt nhất, tu�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 62}),\n",
       "  'b8ac8e7b-c65b-406f-89b7-f6c61fcd6d2b': Document(page_content=' nhất, tuần tự...\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 62}),\n",
       "  '9b09c274-a0e2-4f71-94ec-1e4032ed24b0': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nBTVN\\n•quick_sort (sắp xếp nhanh),\\n•merge_sort (sắp xếp trộn),•selection_sort (sắp xếp lựa chọn trực tiếp),\\n•insertion_sort (sắp xếp chèn trực tiếp),\\n•bubble_sort (sắp xếp nổi bọt)\\n•...\\n•các thuật toán tìm kiếm: nhị phân, theo chiều sâu, theo lựa\\nchọn tốt nhất, tu�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 63}),\n",
       "  'f37272d9-5ae2-4180-b9a3-08efb3d945a2': Document(page_content=' nhất, tuần tự...\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 63}),\n",
       "  'b1b31f9b-0c62-4a72-ab10-56830cd22785': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nBTVN\\n•quick_sort (sắp xếp nhanh),\\n•merge_sort (sắp xếp trộn),\\n•selection_sort (sắp xếp lựa chọn trực tiếp),•insertion_sort (sắp xếp chèn trực tiếp),\\n•bubble_sort (sắp xếp nổi bọt)\\n•...\\n•các thuật toán tìm kiếm: nhị phân, theo chiều sâu, theo lựa\\nchọn tốt nhất, tu�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 64}),\n",
       "  'de185f7b-df45-417d-9d80-fe377495d066': Document(page_content=' nhất, tuần tự...\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 64}),\n",
       "  '0b6f98fc-cde5-4b8c-9219-788290e72a43': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nBTVN\\n•quick_sort (sắp xếp nhanh),\\n•merge_sort (sắp xếp trộn),\\n•selection_sort (sắp xếp lựa chọn trực tiếp),\\n•insertion_sort (sắp xếp chèn trực tiếp),•bubble_sort (sắp xếp nổi bọt)\\n•...\\n•các thuật toán tìm kiếm: nhị phân, theo chiều sâu, theo lựa\\nchọn tốt nhất, tu�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 65}),\n",
       "  '0fbaac32-8e8e-4cec-b27b-e1beec51f125': Document(page_content=' nhất, tuần tự...\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 65}),\n",
       "  '23a9d5b4-86e7-467f-bf24-d9bfc0d00950': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nBTVN\\n•quick_sort (sắp xếp nhanh),\\n•merge_sort (sắp xếp trộn),\\n•selection_sort (sắp xếp lựa chọn trực tiếp),\\n•insertion_sort (sắp xếp chèn trực tiếp),\\n•bubble_sort (sắp xếp nổi bọt)•...\\n•các thuật toán tìm kiếm: nhị phân, theo chiều sâu, theo lựa\\nchọn tốt nhất, tu�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 66}),\n",
       "  'c02a4813-3944-4aae-a439-6e1790f67581': Document(page_content=' nhất, tuần tự...\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 66}),\n",
       "  '26703d08-5d04-4bb1-bc6f-f2f30047d056': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nBTVN\\n•quick_sort (sắp xếp nhanh),\\n•merge_sort (sắp xếp trộn),\\n•selection_sort (sắp xếp lựa chọn trực tiếp),\\n•insertion_sort (sắp xếp chèn trực tiếp),\\n•bubble_sort (sắp xếp nổi bọt)\\n•...•các thuật toán tìm kiếm: nhị phân, theo chiều sâu, theo lựa\\nchọn tốt nhất, tu�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 67}),\n",
       "  'bbb2e02c-844a-48d4-b4c1-160ca65d66cc': Document(page_content=' nhất, tuần tự...\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 67}),\n",
       "  '135abdd9-eeee-4591-a234-3c63c2de4425': Document(page_content='O-lớn, Độ tăng của hàm, Phân tích thuật toán\\nBTVN\\n•quick_sort (sắp xếp nhanh),\\n•merge_sort (sắp xếp trộn),\\n•selection_sort (sắp xếp lựa chọn trực tiếp),\\n•insertion_sort (sắp xếp chèn trực tiếp),\\n•bubble_sort (sắp xếp nổi bọt)\\n•...\\n•các thuật toán tìm kiếm: nhị phân, theo chiều sâu, theo lựa\\nchọn tốt nhất, tu�', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 68}),\n",
       "  'eab2feee-47c0-4984-91d7-706302c27518': Document(page_content='t nhất, tuần tự...\\nNguyễn An Khương O-lớn, Độ tăng của hàm, Phân tích thuật toán', metadata={'source': 'data/web_data//Growth_of_Functions.pdf', 'page': 68}),\n",
       "  '9885704f-b6f7-478d-bb13-daf13372c782': Document(page_content='Chapter 8\\n\\nApproximation Algorithms\\n\\n<number>\\n\\n\\n\\nOutline\\n\\nWhy approximation algorithms?\\n\\nThe vertex cover problem\\n\\nThe set cover problem\\n\\nTSP\\n\\nScheduling Independent tasks\\n\\nBin packing\\n\\n<number>\\n\\n\\n\\nWhy Approximation Algorithms ?\\n\\nMany problems of practical significance are NP-complete but are too important to abandon merely because obtaining an optimal solution is intractable. \\n\\nIf a problem is NP-complete, we are unlikely to find a polynomial time algorithm for solving it exactly, but it may still be possible to find near-optimal solution in polynomial time.\\n\\n In practice, near-optimality is often good enough. \\n\\nAn algorithm that returns near-optimal solutions is called an approximation algorithm.\\n\\n<number>\\n\\n\\n\\nPerformance bounds for approximation algorithms\\n\\n<number>\\n\\n\\n\\nNote that\\xa0 p(n) is always greater than or equal to 1. \\n\\nIf p(n) = 1 then the approximate algorithm is an optimal algorithm.\\n\\nThe larger p(n), the worst algorithm\\n\\nRelative error\\n\\nWe define the relative error of the', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  'bc069897-6e15-4cca-80ae-ada3a89666a1': Document(page_content=\" error\\n\\nWe define the relative error of the approximate algorithm for any input size as\\n\\n                  |c(i) - c*(i)|/ c*(i)\\n\\nWe say that an approximate algorithm has a relative error bound of ε(n) if\\n\\n                  |c(i)-c*(i)|/c*(i)≤\\xa0ε(n)\\x0b\\x0b\\n\\n<number>\\n\\n\\n\\n1. The Vertex-Cover Problem\\n\\nVertex cover: given an undirected graph G=(V,E), then a subset V'\\uf0cdV such that if \\uf022(u,v)\\uf0ceE, then u\\uf0ceV' or v \\uf0ceV' (or both).\\n\\nSize of a vertex cover: the number of vertices in it.\\n\\nVertex-cover problem: find a vertex-cover of minimal size.\\n\\nThis problem is NP-complete\\n\\n<number>\\n\\n\\n\\nApproximate vertex-cover algorithm\\n\\n<number>\\n\\n\\n\\n\", metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '98040503-56b4-4f8d-95f4-e12b851f99f8': Document(page_content='cover algorithm\\n\\n<number>\\n\\n\\n\\n<number>\\n\\n\\n\\nTheorem:\\n\\nAPPROXIMATE-VERTEX-COVER has a ratio bound of 2, i.e., the size of returned vertex cover set is at most twice of the size of optimal vertex-cover.\\n\\nProof:\\n\\nIt runs in poly time\\n\\nThe returned C is a vertex-cover.\\n\\nLet A be the set of edges picked in line 4 and C* be the optimal vertex-cover.\\n\\nThen C* must include at least one end of each edge in A and no two edges in A are covered by the same vertex in C*, so |C*|\\uf0b3|A|. \\n\\nMoreover, |C|=2|A|, so |C|\\uf0a32|C*|.\\n\\n<number>\\n\\n\\n\\nThe Set Covering Problem \\n\\nThe set covering problem is an optimization problem that models many resource-selection problems. \\n\\nAn instance (X, F) of the set-covering problem consists of  a finite set X and a family F of subsets of X, such that every element of X belongs to at least one subset', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  'a7edf4e1-2aa0-46c0-9128-ca7bae5c4610': Document(page_content=' every element of X belongs to at least one subset in F:\\n\\n        X  = \\uf0c8 S\\n\\n                 S\\uf0ceF\\n\\n     We say that a subset S\\uf0ceF covers its elements. \\n\\nThe problem is to find a minimum-size subset C \\uf0cd F whose members cover all of X:\\n\\n          X  = \\uf0c8 S\\n\\n                  S\\uf0ceC\\n\\nWe say that any C satisfying the above equation covers  X.\\n\\n<number>\\n\\n\\n\\nFigure 6.2  An instance {X, F} of the set covering problem, where X consists of the 12 black points and F = { S1, S2, S3, S4, S5, S6}. A minimum size set cover is C = { S3, S4, S5}. The greedy algorithm produces the set C’ = {S1, S4, S5, S3} in order.\\n\\n<number>\\n\\n\\n', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  'f660526e-c17c-4538-9757-c1fe3ca26bbc': Document(page_content=' in order.\\n\\n<number>\\n\\n\\n\\nApplications of Set-covering problem\\n\\nAssume that X is a set of skills that are needed to solve a problem and we have a set of people available to work on it. We wish to form a team, containing as few people as possible, s.t. for every requisite skill in X, there is a member in the team having that skill.\\n\\nAssign emergency stations (fire stations) in a city.\\n\\nAllocate sale branch offices for a company.\\n\\nSchedule for bus drivers.\\n\\n<number>\\n\\n\\n\\nAn Example:  Fire stations\\n\\nS1 = {x1, x2, x3, x4}\\n\\nS2 = { x1, x2, x3, x4, x5}\\n\\nS3 = { x1, x2, x3, x4, x5, x6} \\n\\nS4 = { x1, x3, x4, x6, x7}\\n\\nS5 = { x2, x3, x5, x6, x8, x9}\\n\\nS6 = { x3, x4, x5,', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  'a80df6c4-bf34-487e-b4b3-f411ac8acfa3': Document(page_content=' { x3, x4, x5, x6, x7, x8} \\n\\nS7 = { x4, x6, x7, x8 }\\n\\nS8 = { x5, x6, x7, x8, x9, x10}\\n\\nS9 =  { x5, x8, x9, x10, x11}\\n\\nS10 = { x8, x9, x10, x11}\\n\\nS11 = { x9, x10, x11}\\n\\nThe optimal solution:\\n\\nS = {S3, S8, S9}\\n\\nThe map of a city\\n\\n<number>\\n\\n\\n\\nA greedy approximation algorithm\\n\\n<number>\\n\\n\\n\\nRatio bound of Greedy-set-cover\\n\\nLet denote the dth harmonic number\\n\\n      Hd = \\uf053di-11/i\\n\\nTheorem:  Greedy-set-cover has a ratio bound H(max{|S|: S \\uf0ceF})\\n\\nCorollary: Greedy-set-cover has a ratio bound of (ln|X| +1)\\n\\n', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '0f9738cf-5da2-4905-82a4-76eb0d4353a3': Document(page_content=\" of (ln|X| +1)\\n\\n     (Refer to the text book for the proofs)\\n\\n<number>\\n\\n\\n\\n3. The Traveling Salesman Problem\\n\\nSince finding the shortest tour for TSP requires so much computation, we may consider to find a tour that is almost as short as the shortest. That is, it may be possible to find near-optimal solution.\\n\\nExample: We can use an approximation algorithm for the HCP. It's relatively easy to find a tour that is longer by at most a factor of two than the optimal tour. The method is based on \\n\\nthe algorithm for finding the minimum spanning tree and\\n\\nan observation that it is always cheapest to go directly from a vertex u to a vertex w; going by way of any intermediate stop v can’t be less expensive.\\n\\n                      C(u,w) \\uf0a3 C(u,v)+ C(v,w)\\n\\n<number>\\n\\n\\n\\nAPPROX-TSP-TOUR\\n\\n<number>\\n\\n\\n\\nThí dụ minh h�\", metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '00f9d2f2-659d-491c-9b91-f32e75630c92': Document(page_content='Thí dụ minh họa giải thuật APPROX-TSP-TOUR\\n\\n<number>\\n\\n\\n\\nThe preorder tree walk is not simple tour, since a node be visited many times, but it can be fixed, the tree walk visits the vertices in the order a, b, c, b, h, b, a, d, e, f, e, g, e, d, a. From this order, we can arrive to the hamiltonian cycle H: a, b, c, h, d, e ,f, g, a.\\n\\n<number>\\n\\n\\n\\nThe optimal tour\\n\\nThe total cost of H is approximately 19.074. An optimal tour H* has the total cost of approximately 14.715.\\n\\nThe running time of APPROX-TSP-TOUR is the running time of the Prim algorithm. If the graph is implemented by adjacency matrix, it’s O(V2lgV)\\uf0bb O(E.lgV) since the input graph is a complete graph.\\n\\n<number>\\n\\n\\n\\nRat', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '70ecd10e-3b23-4a5e-8c43-1df01b500093': Document(page_content='.\\n\\n<number>\\n\\n\\n\\nRatio bound of APPROX-TSP-TOUR\\n\\n<number>\\n\\n\\n\\nBut W is not a tour, since it visits some vertices more than once. By the triangle inequality, we can delete a visit to any vertex from W. By repeatedly applying this operation, we can remove from W all but the first visit to each vertex.\\n\\nLet H be the cycle corresponding to this preorder walk. It is a hamiltonian cycle, since every vertex is visited exactly once. Since H is obtained by deleting vertices from W, we have\\n\\n           c(H) \\uf0a3 c(W)                          (4)\\n\\nFrom (3) and (4), we conclude:\\n\\n          c(H) \\uf0a3 2c(H*)\\n\\nSo, APPROX-TSP-TOUR returns a tour whose cost is not more than twice the cost of an optimal tour.\\n\\n<number>\\n\\n\\n\\nScheduling independent tasks\\n\\n', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '5b648b43-2f21-4037-8bdf-0002527856b4': Document(page_content='\\n\\n\\n\\nScheduling independent tasks\\n\\nAn instance of the scheduling problem is defined by a set of n task times, ti, 1≤ i ≤ n, and m, the number of processors.\\n\\nObtaining minimum finish time schedules is NP-complete.\\n\\nThe scheduling rule we will use is called the LPT (longest processing time) rule.\\n\\nDefinition: An LPT schedule is one that is the result of an algorithm, which, whenever a processor becomes free, assigns to that processor a task whose time is the largest of those tasks not yet  assigned.\\n\\n<number>\\n\\n\\n\\nExample\\n\\nLet m = 3, n = 6 and (t1, t2, t3, t4, t5, t6) = (8, 7, 6, 5, 4, 3). In an LPT schedule tasks 1, 2 and 3 respectively. Tasks 1, 2, and 3 are assigned to processors 1, 2 and 3. Tasks 4, 5 and 6 are respectively assigned to the processors 3, 2, and 1.\\n\\nThe finish time is 11. Since \\uf053 ti /3 = 11, the schedule is also optimal. \\n\\n    ', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  'fb27b595-6027-4e8a-85bb-60e2361cd513': Document(page_content=' is also optimal. \\n\\n            6 7 8        11\\n\\nP1\\n\\nP2\\n\\nP3\\n\\n            1                     6\\n\\n            2                    5 \\n\\n            3                   4\\n\\n<number>\\n\\n\\n\\nExample\\n\\nLet m = 3, n = 6 and (t1, t2, t3, t4, t5, t6, t7) = (5, 5, 4, 4, 3, 3, 3). The LPT schedule is shown in the following figure. This has a finish time is 11. The optimal schedule is 9. Hence, for this instance |F*(I) – F(I)|/F*(I) = (11-9)/9=2/9', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '9d2469d0-320c-4fa2-955a-5462a4061e1d': Document(page_content=' (11-9)/9=2/9.\\n\\n                 4  5     8       11\\n\\n                   9 \\n\\nP1\\n\\nP2\\n\\nP3\\n\\n     1                 5        7\\n\\n         1                     3\\n\\n        2              6 \\n\\n        2                      4 \\n\\n        3             4\\n\\n        5               6          7\\n\\n(a) ', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '68843bfd-8b2d-45dd-a43d-f89109f1a95c': Document(page_content='    7\\n\\n(a)  LPT schedule\\n\\n(b) Optimal schedule\\n\\n<number>\\n\\n\\n\\nWhile the LPT rule may generate optimal schedules for some problem instances, it does not do so for all instances. How bad can LPT schedules be relative to optimal schedules?\\n\\nTheorem: [Graham] Let F*(I) be the finish time of an optimal m processor schedule for instance I of the task scheduling problem. Let F(I) be the finish time of an LPT schedule for the same instance, then\\n\\n         |F*(I)-F(I)|/F*(I) ≤ 1/3 – 1/(3m)\\n\\nThe proof of this theorem can be referred to the book\\n\\n“Fundamentals of Computer Algorithms”, E. Horowitz and S. Sahni, Pitman Publishing, 1978.\\n\\n<number>\\n\\n\\n\\nBin Packing\\n\\n We are given n objects which have to be placed in bins of equal capacity L.\\n\\nObject i requires li units of bin capacity.\\n\\nThe objective is to determine the minimum number of bins needed to accommodate all n objects.\\n', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '63ebf626-6b78-45e1-99f8-16e194c798ea': Document(page_content=' of bins needed to accommodate all n objects.\\n\\nExample: Let L = 10, n = 6 and (l1, l2, l3, l4, l5, l6) = (5, 6, 3, 7, 5,4)\\n\\nThe bin packing problem is NP-complete.\\n\\n<number>\\n\\n\\n\\nFour heuristics\\n\\nOne can derive many simple heuristics for the bin packing problem. In general, they will not obtain optimal packings.\\n\\nHowever, they obtain packings that use only a “small” fraction of bins more than an optimal packing.\\n\\nFour heuristics:\\n\\nFirst fit (FF)\\n\\nBest fit (BF)\\n\\nFirst fit Decreasing (FFD)\\n\\nBest fit Decreasing (BFD)\\n\\n<number>\\n\\n\\n\\nFirst-fit and Best-fit\\n\\nFirst-fit\\n\\nIndex the bins 1, 2, 3,… All bins are initially filled to level 0. Objects are considered for packing in the order 1, 2, …, n. To pack object i, find the least index j such that bin j is filled to the level r (r ≤ L – li).', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '6684f9ea-f91e-407c-8e50-d908873c667b': Document(page_content=' the level r (r ≤ L – li). Pack I into bin j. Bin j is now filled to level r + li.\\n\\nBest-fit\\n\\nThe initial conditions are the same as for FF. When object i is being considered, find the least j such that bin j is filled to a level r (r ≤ L – li) and r is as large as possible. Pack i into bin j. Bin j is now filled to level r + li.\\n\\n<number>\\n\\n\\n\\nExample\\n\\n6\\n\\n2\\n\\n4\\n\\n3\\n\\n5\\n\\n(a) First Fit\\n\\n1\\n\\n    5\\n\\n3\\n\\n2\\n\\n4\\n\\n6\\n\\n(b) Best Fit\\n\\n1\\n\\n<number>\\n\\n\\n\\nFirst-fit decreasing and Best-fit decreasing\\n\\nFirst-fit decreasing (FFD)\\n\\nReorder the objects so that li \\uf0b3 li+1, 1 ≤ i ≤ n. Now use First-fit to pack the objects.\\n\\nBest-fit decreasing (BFD)\\n\\nReorder the objects so that li \\uf0b3 li+1, 1 ≤ i ≤ n. Now use First-fit to pack', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  'd3c67a18-d16e-4c11-990a-b6eccec98caa': Document(page_content=' ≤ n. Now use First-fit to pack the objects.\\n\\n<number>\\n\\n\\n\\nExample\\n\\n5\\n\\n1\\n\\n3\\n\\n4\\n\\n6\\n\\n2\\n\\n(c) First Fit Decreasing and  Best Fit Decreasing\\n\\nFFD and BFD do better than either FF or BF on this instance. While FFD and BFD obtain optimal packings on this example, they do not in general obtain such a packings.\\n\\n<number>\\n\\n\\n\\nTheorem\\n\\nLet I be an instance of the bin packing problem and let F*(I) be the minimum number of bins needed for this instance. The packing generated by either FF or BF uses no more than (17/10)F*(I)+2 bins. The packings generated by either FFD or BFD uses no more than (11/9)F*(I)+4 bins.\\n\\nThis proof of this theorem is long and complex (given by Johnson et al., 1974).\\n\\n<number>\\n\\n\\n\\nAppendix: A Taxonomy of Algorithm Design Strategies\\n\\nStrategy name\\t\\t           \\tExamples\\n\\n----------------------------------------------------------------------------------------\\n\\n', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '6b3a0dd1-b95f-4ff8-9311-e7dd58770e16': Document(page_content='  \\tExamples\\n\\n----------------------------------------------------------------------------------------\\n\\nBruce-force                                 Sequential search, selection sort\\n\\nDivide-and-conquer\\t\\tQuicksort, mergesort, binary search\\n\\nDecrease-and-conquer\\t\\tInsertion sort, DFS, BFS\\n\\nTransform-and-conquer\\t\\theapsort, Gauss elimination\\n\\nGreedy                \\t\\tPrim’s, Dijkstra’s\\n\\nDynamic Programming\\t\\tFloyd’s\\n\\nBacktracking\\t\\t\\n\\nBranch-and-Bound\\n\\nApproximate algorithms\\n\\nHeuristics \\n\\nMeta-heuristics\\n\\n<number>\\n\\n\\n\\nprocedure MST-PRIM (G, w, r);\\n\\n/* G = (V,E) is weighted graph with the weight function w, and r is an arbitrary root vertex */ \\n\\nbegin\\n\\n    Q: = V[G];  /* Q is a', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '88f2f6ab-d997-4782-b864-5768e5eb84ca': Document(page_content=' = V[G];  /* Q is a priority queue  */\\n\\n    for each u \\uf0ce Q do key[u]: = \\uf0a5;\\n\\n    key[r]: = 0; p[r]: = NIL;\\n\\n    while Q is not empty do \\n\\n    begin\\n\\n        u: = EXTRACT-MIN(Q);\\n\\n        for each v \\uf0ce Q and w(u, v) < key[v] then\\n\\n        / * update the key field of vertice v */\\n\\n        begin\\n\\n            p[v] := u;  key[v]: = w(u, v)\\n\\n        end \\n\\n    end\\n\\nend;\\n\\n<number>', metadata={'source': 'data/web_data//CHAP8_En.pptx'}),\n",
       "  '24c87a7c-fbfb-4e2e-8846-160508f45e30': Document(page_content='1DATA ENGINEERING\\nLECTURE 1\\n', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 0}),\n",
       "  'e5911b6f-b2cd-49d4-9497-6b904f5bd923': Document(page_content='DATABASE SYSTEM REVIEW\\n2', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 1}),\n",
       "  'c512ced0-f8d5-4591-a00d-a77394866b2d': Document(page_content='DATA APPROACH (1/2)\\n◼File based approach\\n3\\nCustomer\\nInvoicing\\nPurchase\\nOrdersCustomer\\nOrders\\nOrder\\nFileCustomer\\nFileCustomer\\nFile\\nStock\\nFile\\nStock\\nFileOrder\\nFile\\nSupplier\\nFileStock\\nFileApplicationsFiles\\nStock\\nControlStock\\nFile\\nOrder\\nFile\\nApplicationsFiles\\nPurchase\\nOrders\\nStock\\nControlCustomer\\nOrdersCustomer\\nFile\\nStock\\nFile\\nOrder\\nFileCustomer\\nInvoicing\\nSupplier\\nFileShared file approach', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 2}),\n",
       "  '057f1d75-bced-4dfe-ab5e-30330a07024f': Document(page_content='DATA APPROACH (2/2)\\n◼Database approach\\n4\\n', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 3}),\n",
       "  'c9081027-4416-4edb-a7ea-fa29952741f9': Document(page_content='KEY CONCEPTS\\n◼Data vs. Metadata vs. Information\\n◼Data Model\\n◼Database\\n◼Database Management System\\n◼Database System = DB + DBMS + applications\\n◼Database Schema\\n◼Database State\\n◼Database Normalization\\n◼Keys of all kinds\\n5', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 4}),\n",
       "  'd00eeb8a-a296-4d16-ac8b-5a9a7ce3e861': Document(page_content='DBMS ARCHITECTURE\\n6\\n', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 5}),\n",
       "  '63e5edaf-d3fa-410f-b0f3-a223d2cb8581': Document(page_content='DATABASE SYSTEM FRAMEWORK\\n7Visualization, Collaborative Computing, Mobile Computing, \\nKnowledge -based Systems\\nLayer 3: information extraction & sharing\\nData Warehousing, Data Mining, Internet DBs, Collaborative, P2P & \\nGrid Data Management\\nLayer 2: interoperability & migration\\nHeterogeneous DB Systems, Client/Server DBs, Multimedia DB \\nSystems, Migrating Legacy DBs\\nLayer 1: DB technologies\\nDB Systems, Distributed DB Systems\\nNetworking, Mass Storage, Agents, Grid Computing Infrastructure, \\nParallel & Distributed Processing, Distributed Object ManagementData \\nManagement \\nLayerApplication \\nLayer\\nSupporting \\nLayer', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 6}),\n",
       "  'dd81158f-4681-4eed-9b90-51f5bd709459': Document(page_content='DATABASE DESIGN PROCESS\\n8\\n', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 7}),\n",
       "  '997033e9-c118-4cdb-834e-03092cf533ac': Document(page_content='BUSINESS REQUIREMENTS (1/2)\\n◼The COMPANY database :keeps track ofemployees,\\ndepartments, andprojects .\\n◼The company isorganized into DEPARTMENTs .Each\\ndepartment hasaunique name ,aunique number ,anda\\nparticular employee who manages thedepartment .We\\nkeep track ofthestart date when thatemployee began\\nmanaging the department .Adepartment may have\\nseveral locations .\\n◼Adepartment controls anumber ofPROJECTs, each of\\nwhich has aunique name ,aunique number ,and a\\nsingle location .\\n9', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 8}),\n",
       "  '08c051b9-4722-44d9-bed0-3f8fcc3ea946': Document(page_content='BUSINESS REQUIREMENTS (2/2)\\n◼WestoreEMPLOYEE’s name ,Social Security number ,\\naddress ,salary ,sex,and birth date .Anemployee is\\nassigned toone department, butmay work onseveral\\nprojects ,which arenotnecessarily controlled bythe\\nsame department .Wekeep track ofthecurrent number\\nofhours perweek that anemployee works oneach\\nproject .Wealso keep track ofthedirect supervisor of\\neach employee .\\n◼Wewant tokeep track oftheDEPENDENTs ofeach\\nemployee, including first name ,sex,birth date,and\\nrelationship totheemployee .\\n10', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 9}),\n",
       "  '4df5bf82-18dc-4ef7-8e59-a53ec580e911': Document(page_content='CONCEPTUAL DESIGN\\n11\\n', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 10}),\n",
       "  'e9648fef-e53b-4245-8fe8-50d874d4c29a': Document(page_content='LOGICAL DESIGN\\n12\\n', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 11}),\n",
       "  '9d308a6f-346c-4304-a334-1f9c7e3c7f17': Document(page_content='PHYSICAL DESIGN\\n13\\n', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 12}),\n",
       "  'bdd19e14-0b59-475c-af59-07fc57779bdb': Document(page_content='DATABASE DESIGN PRACTICE\\n◼Quarantine Camp\\n◼Feedback Builder\\n◼Item Management\\n14', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 13}),\n",
       "  'bef3d7a8-04f0-486f-80cd-9c116693a7d6': Document(page_content='QUESTIONS AND ANSWERS\\n15Picture from: http://philadelphiasculpturegym.blogspot.com/2013/09/save -date-free-talk-and-q-on-affordable.html', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 14}),\n",
       "  'fa839b33-90f6-4a22-bb62-baa2d9b5ba8a': Document(page_content='REFERENCES\\n[1]R.Elmasri, S.B.Navathe :“Fundamentals ofDatabase\\nSystems” ,7thEdition, Pearson Addison -Wesley, 2016 .\\n[2]P.A.Bernstein, E.Newcomer :\"Principles of Transaction\\nProcessing \",2ndEdition, Elsevier Inc.,2009 .\\n[3]S.Lightstone, T.Teorey, T.Nadeau :\"Physical Database Design \",\\nElsevier Inc.,2007 .\\n[4]R.Kimball, M.Ross, \"The Data Warehouse ToolKit\", 3rd\\nEdition, Wiley Publishing ,Inc.,2013 .\\n[5]J.Hurwitz, A.Nugent, F.Halper, M.Kaufman :\"BigData for\\nDummies \",John Wiley &Sons Inc.,2013 .\\n[6]Jiawei Han, Micheline Kamber, Jian Pei,“Data Mining :\\nConcepts andTechniques”, Third Edition, Morgan Kaufmann\\nPublishers, 2012 .\\n[7]Jure Leskovec, Anand Rajaraman', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 15}),\n",
       "  'd8f4881c-6e21-479e-9813-a7f22cf30389': Document(page_content=' Leskovec, Anand Rajaraman, Jeffrey D.Ullman :Mining of\\nMassive Datasets, 2014 .\\n16', metadata={'source': 'data/web_data//Lecture 1.pdf', 'page': 15}),\n",
       "  '4941aaa3-ab5f-4c40-82dd-53ab5b684cf5': Document(page_content='1DATA ENGINEERING\\nCOURSE OVERVIEW\\nTrong Nhan Phan, PhD\\n', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 0}),\n",
       "  'dfdb6977-2528-4c26-9914-0cb05d005aff': Document(page_content='CONTACT\\n❑Lecturer: Phan Trọng Nhân, PhD\\n❑Faculty: Computer Science and Engineering\\n❑Department: Information Systems\\n❑Email: nhanpt@hcmut.edu.vn\\n❑Course site: LMS\\n2', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 1}),\n",
       "  '5c0ba59c-cfe0-43e4-a552-16b048ca6624': Document(page_content='COURSE INTRODUCTION\\n❑Subject: Data Engineering\\n❑Number: 055240\\n❑Credit: 3\\n❑Period: 70.5\\n❑Prerequisite :None\\n❑Class Periods :13weeks\\n❑Course Syllabus\\n3', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 2}),\n",
       "  'cf4fca3c-8624-4bb1-8436-863b105d8cb3': Document(page_content='COURSE AIMS\\n◼Database analysis and design methodologies\\nincluding relational data model and NoSQL\\nmodels .\\n◼Advances indata storage andretrieval methods ,\\nflexible query answering ,query processing and\\noptimization ,transaction processing .\\n◼Object -oriented and time series databases ,as\\nwellasbigdata andapplications .\\n◼Research topics relevant tomodern databases\\nand applications willalso beintroduced and\\ndiscussed .\\n4', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 3}),\n",
       "  'a257de81-f10d-4abe-b309-94d5b245df35': Document(page_content='REFERENCES\\n[1]R.Elmasri, S.B.Navathe :“Fundamentals ofDatabase\\nSystems” ,7thEdition, Pearson Addison -Wesley, 2016 .\\n[2]P.A.Bernstein, E.Newcomer :\"Principles of Transaction\\nProcessing \",2ndEdition, Elsevier Inc.,2009 .\\n[3]S.Lightstone, T.Teorey, T.Nadeau :\"Physical Database Design \",\\nElsevier Inc.,2007 .\\n[4]R.Kimball, M.Ross, \"The Data Warehouse ToolKit\", 3rd\\nEdition, Wiley Publishing ,Inc.,2013 .\\n[5]J.Hurwitz, A.Nugent, F.Halper, M.Kaufman :\"BigData for\\nDummies \",John Wiley &Sons Inc.,2013 .\\n[6]Jiawei Han, Micheline Kamber, Jian Pei,“Data Mining :\\nConcepts andTechniques”, Third Edition, Morgan Kaufmann\\nPublishers, 2012 .\\n[7]Jure Leskovec, Anand Rajaraman', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 4}),\n",
       "  '0d9365d2-8996-41ac-8a71-f07b6397d1e9': Document(page_content=' Leskovec, Anand Rajaraman, Jeffrey D.Ullman :Mining of\\nMassive Datasets, 2014 .\\n5', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 4}),\n",
       "  'c645e775-bf46-4d76-9483-9fccf415a4dc': Document(page_content='OUTLINE\\n❑Chapter 1:Introduction andreview\\n❑Chapter 2:Database tuning methodologies\\n❑Chapter 3:Data indexing andquery optimization\\n❑Chapter 4:Non-relation databases\\n❑Chapter 5:Data streaming\\n❑Chapter 6:Bigdata andapplications\\n❑Chapter 7:Data warehouse\\n6', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 5}),\n",
       "  'c1fd1fca-c148-4867-b347-c6f7ac26fe71': Document(page_content='LEARNING OUTCOMES\\n◼CLO.1:Understand principles indatabase analysis and\\ndesign methodologies (conceptual, logical and physical\\ndatabase design) .\\n◼CLO.2:Obtain skills indata storage andretrieval ,flexible\\nquery answering ,query processing and optimization ,\\ntransaction processing .\\n◼CLO.3:Understand concepts and fundamental\\ntechniques indata warehousing techniques .\\n◼CLO.4: Understand knowledge related to\\nobject -oriented andtime series databases ,aswellasbig\\ndata andapplications .\\n◼CLO.5:Be able toconduct research onmodern\\ndatabases andapplications .\\n7', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 6}),\n",
       "  'ac60ceb8-5712-4227-ba4b-7ed8be6bd301': Document(page_content='ASSESSMENT\\n8❑Course Work (CW)\\noWeight : 30%\\noWhen: by schedule.\\n❑Team Assignment (TA)\\noWeight: 30%\\noMembers: 2-3\\nTotal Score = (30% * CW) + (30% * TA) + (40% * FT) > 5.5No Nullable score!\\n2. Absent 2 weeks: -1 point from total score, -1 more for each further absence❑Final Test (FT)\\noWeight : 40%\\noDuration: 70 -90 mins\\noMultiple choice and \\nessay questions', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 7}),\n",
       "  'c9caf29f-ef91-46d5-8d68-9bdc2995c9ef': Document(page_content='TEAM REGISTRATION\\n9◼Please check it on LMS', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 8}),\n",
       "  '85e5a85b-894e-4089-8e62-ada77ccb27b2': Document(page_content='COURSE WORK (30%)\\n◼20 minute presentation + 10 minute Q&A\\n◼Every member must present\\n◼Contribution of each member must be clearly \\nclaimed\\n◼An assigned review team will give feedback \\nand questions\\n10', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 9}),\n",
       "  '701e58f2-bdc8-4a8e-8973-4e13f8d31e9d': Document(page_content='COURSE WORK ASSESSMENT\\n◼Problem statement (1 point)\\n◼Why it matters (1 point)\\n◼Theory and technology base (2 points)\\n◼How to solve them (3 points)\\n◼Demo (2 points)\\n◼Presentation slides (1 point)\\n11', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 10}),\n",
       "  'affaae53-5c79-4037-a78c-2a097bb8a6d6': Document(page_content='TEAM ASSIGNMENT (30%)\\n◼Focus on “Data Engineering” point of view\\n◼Available technologies may be employed to \\nsolve the problem but with knowhow\\n◼20minute presentation + 10minute Q&A\\n◼Every member must present\\n◼Contribution of each member must be clearly \\nclaimed\\n◼An assigned review team will give feedback \\nand questions\\n12', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 11}),\n",
       "  '5022f597-1c68-4d95-a7f5-8c36ce199e48': Document(page_content='TEAM WORK ASSESSMENT\\n◼Problem statement and why it matters (1 point)\\n◼Challenges in terms of data engineering (1.5 \\npoints)\\n◼Theory and technology base (1 point)\\n◼How to solve them (3 points)\\n◼Demo (1.5 points)\\n◼Report (2 points)\\n13', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 12}),\n",
       "  '12c007c3-b787-4961-8310-10d18e6e6ab2': Document(page_content='SUBMISSION\\n❑Submission toLMS:\\n❑OnSchedule\\n❑Team representative submits thework on\\nbehalf oftheteam\\n❑Donotforget topress “submit” button\\n14', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 13}),\n",
       "  '1f7fa61e-f520-4f28-ba31-8e23baaef4b1': Document(page_content='QUESTIONS AND ANSWERS\\n15Picture from: http://philadelphiasculpturegym.blogspot.com/2013/09/save -date-free-talk-and-q-on-affordable.html', metadata={'source': 'data/web_data//Chapter 0-Course introduction.pdf', 'page': 14}),\n",
       "  '516d1718-5c24-49d2-97d8-f6e45ecfa63a': Document(page_content='Chapter 5\\x0bDynamic Programming and Greedy Algorithms\\n\\nDynamic Programming\\n\\nGreedy Algorithms\\n\\n<number>\\n\\n\\n\\n1. Dynamic programming\\n\\n Dynamic programming solves problems by combining the solutions of subproblems to form the solution of the original problem. \\n\\nDynamic programming is applicable when the subproblems are not independent, that is, subproblems share subsubproblems. \\n\\nA dynamic programming algorithm solves every subsubproblem once and then save its answer in a table, thereby avoiding the work of recomputing the answer  every time the subsubproblems is encountered.\\n\\nDynamic programming is applied to optimization problem. \\n\\n<number>\\n\\n\\n\\nFour steps of dynamic programming\\n\\nThe development of a dynamic programming algorithm can be divided into a sequence of four steps.\\n\\nCharacterize the structure of an optimal solution.\\n\\n2. Recursively define the value of an optimal solution. \\n\\n3. Compute the value of an optimal solution in a bottom-up fashion.\\n\\n4. Construct an optimal solution from computed information.\\n\\n<number>\\n\\n\\n\\nExample 1: Matrix-chain multiplication\\n\\nGiven a sequence <A1, A2, …', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '1c35d939-052a-4d23-9639-9de154f918df': Document(page_content=' a sequence <A1, A2, …, An> of n matrices to be multiplied, and we wish to compute the product.\\n\\n              A1 A2 … An\\t\\t\\t\\t\\t(5.1)\\n\\nA product of matrices is fully parenthesized  if it is either a single matrix or the product of two fully-parenthesized matrix products, surrounded by parentheses.\\n\\nExample:  A1 A2 A3 A4 can be fully parenthesized in five distinct ways:\\n\\n(A1(A2(A3A4)))\\n\\n(A1((A2A3)A4)\\n\\n((A1A2)(A3A4))\\n\\n(A1(A2A3))A4)\\n\\n(((A1A2)A3)A4)\\n\\n<number>\\n\\n\\n\\nThe way we parenthesize a chain of matrices can have a dramatic impact on the cost of evaluating the product.\\n\\n Example: \\tA1 \\t10 \\uf0b4 100\\n\\n                   ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'f2eafdb3-6880-4044-8d09-48883313c92e': Document(page_content=\"               A2\\t100 \\uf0b4 5\\n\\n                        A3\\t5 \\uf0b4 50\\n\\n((A1A2)A3)) needs\\n\\n10.100.5 + 10.5.50 = 5000 + 2500 \\n\\n                               = 7500 scalar multiplications.\\n\\n(A1(A2A3)) needs\\n\\n100.5.50 + 10.100.50 = 25000 + 50000  = 75000 scalar multiplications.\\n\\nComputing the product according to the first parenthesization is 10 time faster.\\n\\n<number>\\n\\n\\n\\nProblem statement\\n\\nThe matrix-chain multiplication problem can be stated as follows:\\n\\n'‘Given a chain <A1, A2, …, An> of n matrices, where for  \\n\\n   i = 1, 2, …, n, matrix Ai has dimension pi-1 \\uf0b4\", metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '1b764569-b76b-4d30-80e6-e5ccc5bdd1bc': Document(page_content=' matrix Ai has dimension pi-1 \\uf0b4 pi, fully parenthesize the product A1A2 …An in such a way that minimizes the number of scalar multiplications”.\\n\\nThis is a difficult optimization problem.\\n\\n<number>\\n\\n\\n\\nThe structure of an optimal parenthesization\\n\\nStep 1: Characterize the structure of an optimal solution. \\n\\nLet adopt Ai..j denote the matrix that results from evaluating the product                Ai Ai+1…Aj.\\n\\nAn optimal parenthesization of the product A1.A2… An \\n\\nsplits the product between Ak and Ak+1 for some integer k, 1 \\uf0a3 k < n. That is, for some value of k, we first compute the matrices A1..k and Ak+1..n and then multiply them together to produce the final product A1.n.\\n\\nThe cost of this optimal parenthesization = the cost of computing Al..k + the cost of computing Ak+1..n, + the cost of multiplying them together.\\n\\n<number>\\n\\n\\n\\nRepresent a recursive solution\\n\\nFor the', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '04399bc8-1030-4885-b92c-98f50affc738': Document(page_content='\\n\\nRepresent a recursive solution\\n\\nFor the matrix-chain multiplication problem, our subproblems are the problems of determining the minimum cost of a parenthesization of Ai.Ai+1… Aj for    1 \\uf0a3 i \\uf0a3 j \\uf0a3 n.\\n\\nLet m[i, j] the minimum number of scalar multiplications needed to compute the matrix Ai..j. The cost of a cheapest way to compute A1..n would be m[1, n].\\n\\n Assume that the optimal parenthesization splits the product Ai Ai+l… Aj between Ak and Ak+l, where i \\uf0a3 k < j.  Then m[i, j] is equal to the minimum cost of computing the subproducts Ai..k and Ak+1..j, plus the cost of multiplying these two matrices together.\\n\\n                          m[i, j] = m[i, k] + m[k+1, j] + pi-1pkpj.\\n\\n<number>\\n\\n\\n\\nA recursive solution\\n\\nThus, our recursive definition for', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '744908aa-f952-4a4c-9013-63d3e34b122e': Document(page_content=\" recursive solution\\n\\nThus, our recursive definition for the minimum cost of parenthesizing the product Ai Ai+l… Aj becomes:\\n\\nm[i, j] = 0\\t\\t    if i = j,\\n\\n\\t= min {m[i, k] + m[k + 1, j] + pi-1pkpj.}\\t\\n\\n\\t\\t         \\t    if i < j (for all k from i to j-1).\\t\\t\\t        (5.2)\\n\\nTo help us to keep track of how to construct an optimal solution, let define:\\n\\ns[i, j]:    a value of k at which we can split the product           AiAi+1…Aj to obtain an optimal parenthesization.\\n\\n<number>\\n\\n\\n\\nThe important observation\\n\\nThe important observation  \\n\\n'‘The full parenthesization for the subchain A1A2....Ak inside the optimal parenthesization for the chain A1A2…An must also be an optimal parenthesization''.\\n\\nSo the\", metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '057f4d1c-0b30-4f29-9d4f-002dc9876006': Document(page_content=\" an optimal parenthesization''.\\n\\nSo the optimal solution for the matrix-chain multiplication problem contains within it optimal solutions to subprobems.  \\n\\nThe second step of the dynamic programming paradigm is to define the value of an optimal solution recursively in terms of the optimal solutions to subproblems.\\n\\n<number>\\n\\n\\n\\nStep 3: Computing the optimal costs\\x0b\\n\\nInstead of computing the solution to recurrence formula (5.2) by  a recursive algorithm, we perform the third step of the dynamic programming paradigm and compute the optimal cost by using a bottom-up approach. \\n\\nAssume that matrix Ai has dimensions pi-1\\uf0b4 pi for \\n\\ni = 1, 2 ,.., n.\\n\\nThe input is a sequence <p0, p1, …, pn>, where length[p]= n+1.\\n\\nThe procedure uses an auxiliary table m[1..n, 1..n] for storing the m[i, j] cost and an auxiliary table s[1..n, 1..n] that records which index of k achieved the optimal cost in computing m[i, j].\\n\\nProcedure MATRIX-CHAIN\", metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'f5c7d8e6-a758-49d5-a9c3-b01637d6ba24': Document(page_content='\\nProcedure MATRIX-CHAIN-ORDER returns two tables m and s.\\n\\n<number>\\n\\n\\n\\n Procedure that computes tables m and s\\n\\nprocedure MATRIX-CHAIN-ORDER(p, m, s);\\n\\nbegin\\n\\n    n:= length[p] - 1;\\n\\n    for i: = 1 to n do m[i, i] := 0;\\n\\n    for l:= 2 to n do   /* l: length of the chain */\\n\\n       for i:= 1 to n – l + 1 do\\n\\n       begin\\n\\n           j:= i + l – 1;\\n\\n           m[i, j]:= \\uf0a5;\\t\\t/* initialization   */\\n\\n           for k:= i to j-1 do\\n\\n           begin\\n\\n               q:= m[i, k] + m[k + 1,', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '46295d61-925c-4466-9678-19c71937620e': Document(page_content=', k] + m[k + 1, j] + pi-1pkpj;\\n\\n                if q < m[i, j] then\\n\\n                begin  m[i, j]: = q; s[i, j]: = k end\\n\\n           end\\n\\n      end\\n\\nend\\n\\nComplexity: O(n3)\\n\\n<number>\\n\\n\\n\\nAn example of matrix-chain multiplication\\n\\nSince we have defined m[i, j] only for i < j, only the portion of the table m above the main diagonal is used.\\n\\nGiven the matrices with the dimensions:\\n\\nA1\\t30 \\uf0b4 35\\n\\nA2        35 \\uf0b4 15\\n\\nA3        15 \\uf0b4 5\\n\\nA4         5 \\uf0b4 10\\n\\nA5        10 \\uf0b4 20\\n\\nA6    ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '7432f9df-3e6e-470a-8b39-596ad5c0c367': Document(page_content='� 20\\n\\nA6        20 \\uf0b4 25\\n\\n Figure 5.1 shows the tables m và s computed by procedure MATRIX-CHAIN-ORDER with n = 6.\\n\\n<number>\\n\\n\\n\\nAn example of matrix-chain multiplication (cont.)\\n\\n Table m\\n\\n\\t\\t\\t\\ti\\n\\n             1\\t 2\\t  3\\t 4\\t 5        6\\n\\n    6    15125   10500   51375   3500    5000       0\\n\\n    5    11875     7125     2500   1000       0\\n\\nj   4      9357     4375       750      0\\n\\n    3      7875     2625         0\\n\\n    2     15750        0\\n\\n    1   ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '27b17924-8c74-414e-bccc-35108f96a412': Document(page_content='  0\\n\\n    1         0\\n\\nTable s\\t\\t\\t\\t\\t\\t\\tFigure 5.1\\n\\n\\t\\t\\ti\\n\\n       \\t1\\t2\\t3\\t4\\t5\\t\\n\\n    6    \\t3\\t3\\t3\\t5\\t5\\t\\n\\n    5    \\t3\\t3\\t3\\t4\\n\\nj   4       3\\t3\\t3\\n\\n    3       1\\t2\\n\\n    2     \\t1\\t\\n\\n<number>\\n\\n\\n\\nAn example of matrix-chain multiplication (cont.)\\n\\nm[2,5] = min \\n\\n               = 7125\\n\\n \\uf0de k = 3  for A2..5\\n\\nStep 4 of dynamic programming paradigm is to construct an optimal solution from computed information.\\n\\n<number>\\n\\n\\n\\nStep 4: Constructing an optimal solution\\n\\nWe use table  s[1..n, 1..n] to determine the best way to multiply the matrices', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'b5ce3c7b-67ee-4b16-a897-2ae920c9efc8': Document(page_content=' to determine the best way to multiply the matrices. Each entry s[i, j] records the value  of k  such that the optimal parenthesization of AiAi+1… Aj  splits the product between Ak and Ak+1.\\n\\nGiven the matrices  A = <A1, A2…, An>, table s computed by MATRIX-CHAIN-ORDER and the indices i and j, the following recursive procedure MATRIX-CHAIN-MULTIPLY computes the matrix chain product  Ai..j,. The procedure returns the result in parameter AIJ.\\n\\nWith the initial call \\n\\n                  MATRIX-CHAIN-MULTIPLY(A, s, 1, n, A1N)\\n\\nThe procedure will return the matrix chain product with the parameter A1N.\\n\\n<number>\\n\\n\\n\\nCompute the result \\n\\nprocedure MATRIX-CHAIN-MULTIPLY(A, s, i, j, AIJ);\\n\\nbegin\\n\\n    if j > i then\\n\\n    begin \\n\\n   ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'ea0a0bd2-1dc5-46f9-964c-4118d9364765': Document(page_content='\\n\\n    begin \\n\\n        MATRIX-CHAIN-MULTIPLY(A, s, i, s[i, j], X);\\n\\n        MATRIX-CHAIN-MULTIPLY(A,s, s[i, j]+1, j, Y);\\n\\n        MATRIX-MULTIPLY(X, Y, AIJ);\\n\\n    end\\n\\n    else\\n\\n       assign Ai to AIJ;\\n\\nend;\\n\\n<number>\\n\\n\\n\\n Elements of dynamic programming\\n\\nThere are two key elements that an optimization problem must have for dynamic programming to be applicable:\\n\\n        (1) optimal substructure and\\n\\n        (2) overlapping subproblems.\\n\\nOptimal substructure\\n\\nThe optimal solution for the problem contains within it optimal solutions to subproblems. \\xa0\\n\\n<number>\\n\\n\\n\\nOverlapping subproblems \\n\\n When a recursive algorithm revisits the same problem over and over again, we say that the optimization problem has overlapping subproblems.', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'a280e5e5-3625-44c2-b623-54a3f06abae1': Document(page_content=' that the optimization problem has overlapping subproblems. \\n\\n\\xa0Dynamic programming algorithms take advantage of overlapping subproblems by solving each subproblem once and then storing the solution in a table where it can be looked up when needed, using constant time per lookup.\\n\\nRecursive algorithms often work in top-down fashion while dynamic programming algorithms work in a bottom-up fashion. The latter approach is more efficient.\\n\\n<number>\\n\\n\\n\\nExample 2: Longest common subsequence\\n\\n\\xa0A subsequence of a sequence is just the given sequence with some elements left out. \\n\\n    Example: Z = <B, C, D, B> is a subsequence of X = <A, B, C, B, D, A, B> with the corresponding index sequence <2, 3, 5, 7>.\\n\\n\\xa0Given two subsequences X and Y, we say that Z is a common subsequence  of X and Y if Z  is a subsequence of both X and Y.\\n\\n\\xa0In the longest-common-subsequence problem, we are given two subsequences X = <x1, x2, …, xm> anh Y = <y1, y2,…, yn', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '1a3fd4fb-49e0-439c-ba32-b71805e035cc': Document(page_content='y1, y2,…, yn> and wish to a maximum length common subsequence (LCS) of X and Y.\\n\\n<number>\\n\\n\\n\\nCharacterizing a longest common subsequence\\n\\nExample: X = <A, B, C, B, D, A, B> and  Y = <B, D, C, A, B, A>\\n\\n          <B, D, A, B> is LCS of X and Y.\\n\\n\\xa0Given a sequence X = <x1, x2, …, xm>, we define the i-th prefix of X, for  i = 0, 1, …, m, is Xi = <x1, x2, …, xi>.\\n\\n\\xa0Theorem 4.1 \\n\\nLet X = <x1, x2, …, xm> and Y = <y1, y2, …, yn> be sequences, and let  Z = <z1, z2, …, zk> be any LCS of X and Y.\\n\\n1.\\xa0 If xm = yn then zk = xm = yn and Zk-1', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'c6769612-18e5-4433-b1cc-a7fa843e611c': Document(page_content=' xm = yn and Zk-1 is LCS of Xm-1 and Yn-1.\\n\\n2.\\xa0 If xm \\uf0b9 yn, then zk \\uf0b9 xm implies that  Z is LCS of Xm-1 and Y.\\n\\n3.\\xa0 If xm \\uf0b9 yn, then zk \\uf0b9 yn implies that Z is LCS of X and Yn-1.\\xa0\\n\\n<number>\\n\\n\\n\\nA recursive solution to subproblems\\n\\nTo find an LCS of X and Y, we may need to find the LCS’s of X and  Yn-1 and of Xm-1 and Y.  But each of these subproblems has the subsubproblem of finding the LCS of Xm-1 and Yn-1. \\n\\n\\xa0Let define c[i, j] to be the length of LCS of the subsequences Xi and Yj. If either  i = 0 or j = 0,  one of the sequences has length 0, so the LCS has length 0. The optimal substructure of the LCS problem gives the recursive formula:\\n\\n\\xa0            ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '04ee3334-7f74-4741-ab20-fc7b9f9ff13a': Document(page_content='            0                                  if i =0 or j = 0\\n\\nc[i, j] =  c[i-1, j-1]+1                 if i, j > 0 and xi = yj\\n\\n              max(c[i, j-1],c[i-1,j])   if  i,j >0 and xi \\uf0b9 yj          (5.3) \\n\\n<number>\\n\\n\\n\\nComputing the length of an LCS\\n\\nBased on equation (5.3), we could write a recursive algorithm to compute the length of an LCS of two sequences. However, we can use dynamic programming to compute the solutions bottom-up.\\n\\nProcedure LCS-LENGTH takes two sequences X = <x1,x2, …, xm> và Y = <y1', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'b6001eca-1c2b-4ea2-a04c-45d167735b9a': Document(page_content=' xm> và Y = <y1, y2, …, yn> as inputs. \\n\\nIt stores the c[i, j] values in a table c[0..m, 0..n]. It also maintains the table b[1..m, 1..n] to simplify construction of an optimal solution. \\n\\n<number>\\n\\n\\n\\nprocedure LCS-LENGTH(X, Y)\\n\\nbegin \\n\\n    m: = length[X];  n: = length[Y];\\n\\n    for i: = 1 to m do c[i, 0]: = 0;   for j: = 1 to n do c[0, j]: = 0;\\n\\n    for i: = 1 to m do \\n\\n       for j: = 1 to n do \\n\\n          if xi = yj then \\n\\n          begin   c[i, j]: = c[i-1, j-1] + 1; b[i, j]: = “\\uf0e3”   end\\n\\n   ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '3ff8c64a-b499-453b-8346-994f8a3021b0': Document(page_content='�”   end\\n\\n          else if c[i – 1, j] > = c[i, j-1] then\\n\\n          begin  c[i, j]: = c[i – 1, j]; b[i, j]: = “\\uf0ad”   end\\n\\n          else\\n\\n          begin c[i, j]: = c[i, j-1]; b[i, j]: = “\\uf0ac”    end\\n\\nend;\\n\\nThe following figure 5.2 shows the table c for the example.\\n\\n<number>\\n\\n\\n\\n             yj         B        D       C        A        B       A\\n\\n0  0     0     0     0     0     0\\n  0', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '921bb523-8888-4241-bcbf-e724984d4f73': Document(page_content='  0     0\\n  0  0  \\uf0ad  0 \\uf0ad   0 \\uf0ad   1 \\uf0e3   1  \\uf0ac  1 \\uf0e3\\n  0  1 \\uf0e3   1  \\uf0ac  1  \\uf0ac  1 \\uf0ad   2 \\uf0e3   2  \\uf0ac\\n  0  1 \\uf0ad   1 \\uf0ad   2 \\uf0e3   2 \\uf0ac   2 \\uf0ad   2 \\uf0ad\\n  0  1 \\uf0e3   1 \\uf0ad   2 \\uf0ad   2 \\uf0ad   3 \\uf0e3   3  \\uf0ac\\n  0  1 \\uf0ad   2 \\uf0e3   2 \\uf0ad   2 \\uf0ad   3 \\uf0ad   3 \\uf0ad\\n  0  1 \\uf0ad   2 \\uf0ad   2 \\uf0ad   3  \\uf0e3  3 \\uf0ad   4 \\uf0e3\\n  0  1 \\uf0e3   2 \\uf0ad   2 \\uf0ad   3 �', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '4424b97d-4b57-4e4d-8813-347af77ec64e': Document(page_content=\"  2 \\uf0ad   3 \\uf0ad   4 \\uf0e3   4 \\uf0ad\\n\\nxi\\n\\nA\\n\\nB\\n\\nC\\n\\nFigure 5.2\\n\\nB\\n\\nD\\n\\nA\\n\\nB\\n\\n<number>\\n\\n\\n\\nTạo chuỗi con chung dài nhất\\n\\nThe table b can be used to construct an LCS of\\n\\nX = <x1,x2, …xm> and Y = <y1, y2, …, yn>\\n\\n\\xa0The following recursive procedure prints out an LCS of X and Y. The initial invocation is PRINT-LCS(b, X, m, n).\\n\\nprocedure PRINT-LCS(b, X, i, j)\\n\\n begin \\n\\n    if i <> 0 and j <> 0 then\\n\\n        if b[i, j] = '' \\uf0e3 '' then\\n\\n        begin PRINT-LCS(b, X, i- 1 , j - l ) ; \\n\\n         print x\", metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'ebacd9a0-a643-43d0-9328-b17f90564e38': Document(page_content=\"         print xi \\n\\n        end\\n\\n        else if b[i,j] = ''\\uf0ad'' then \\n\\n                PRINT-LCS (b, X, i-1, j)\\n\\n         else PRINT-LCS(b, X, i, j-1)\\n\\n end;\\n\\nTime complexity of procedure PRINT-LCS is O(m+n), since at least one of i or j is decremented in each stage of the recursion. \\n\\n<number>\\n\\n\\n\\nExample 3. Knapsack problem \\n\\n'‘A thief robbing a store find it filled with N types of items of varying size and value, but has only a small knapsack of capacity M to use to carry the goods. The knapsack problem is to find the combination of items which the thief should choose for his knapsack in order to maximize the total value of  all the items he takes.”\\n\\nThe problem can be solved using dynamic programming by using two tables cost and best as\", metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '715a0bf2-5e94-4b18-aa3f-5fab6e6ef6cb': Document(page_content=' dynamic programming by using two tables cost and best as follows:\\n\\ncost[i] stores the highest value that can be achieved with a knapsack of capacity i \\n\\n\\xa0                  cost[i] = cost[i – size[j]] + val[j]\\n\\nbest[i] stores the last item that was added to achieve that maximum.\\n\\n<number>\\n\\n\\n\\nExample of knapsack problem \\n\\nvalue    4             5           10\\t  11           13\\n\\nname    A           B           C                D            E\\n\\n             M = 17\\n\\nFigure 5.3  An example of knapsack problem\\n\\n<number>\\n\\n\\n\\nDynamic programming algorithm for the knapsack problem\\n\\n', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '035c0881-8eea-494e-927c-0b86d14d2a5d': Document(page_content=' programming algorithm for the knapsack problem\\n\\nM:   knapsack capacity\\n\\nfor i: = 0 to M do cost[i]: = 0;\\n\\nfor j: = 1 to N do   /* each of item type   */\\n\\nbegin \\n\\n    for i:= 1 to M do  /* i means capacity  */\\n\\n       if i – size[j] > = 0 then \\n\\n          if cost[i] < (cost[i – size[j]] + val[j]) then \\n\\n          begin\\n\\n               cost[i]: = cost[i – size[j]] + val[j];      best[i]: = j\\n\\n           end;\\n\\nend; \\n\\n<number>\\n\\n\\n\\nKnapsack problem solution\\n\\n\\xa0K              1    2    3   4    5   6   7   ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'df93d051-9d45-4537-9149-986684e645f6': Document(page_content=' 5   6   7    8    9   10    11  12   13   14   15  16   17\\n\\nj=1\\n\\ncost[k]      0    0   4    4    4    8   8   8  12   12   12   16   16   16   20   20   20\\n\\nbest[k]                 A   A   A    A   A  A   A    A     A     A    A     A    A     A    A\\n\\n\\xa0j=2\\n\\ncost[k]      0    0   4    5    5    8   9   10  12   13   14   16   17   18   20  21   22\\n\\nbest[k]', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '1320c27d-eda3-4a53-9d7e-59ae8b3ff3d3': Document(page_content=' 21   22\\n\\nbest[k]                 A   B   B    A   B   B    A    B    B    A     B    B    A    B    B\\n\\nj=3\\n\\ncost[k]      0    0   4    5    5    8   10   10  12   14   15   16   18   20   20  22   24\\n\\nbest[k]                 A   B   B   A    C    B    A    C    C    A    C    C     A   C    C\\n\\nj=4\\n\\ncost[k]      0    0   4    5    5 ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '96cd0fe1-7369-4957-97ed-2a7d6ea0b7bd': Document(page_content=' 4    5    5    8   10   11  12   14   15   16   18   20   21  22   24\\n\\nbest[k]                 A   B   B   A    C    D    A    C    C    A    C    C     D   C    C\\n\\nj=5\\n\\ncost[k]      0    0   4    5    5    8   10   11  13   14   15   17   18   20   21  23   24\\n\\nbest[k]                 A   B   B    A   C    D    E    C    C    E    C    C     D', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '3c14b8ae-c6e2-41e7-be7f-473f75018f53': Document(page_content=' C    C     D   E    C\\n\\nFigure 5.4 Tables cost and best of an example of knapsack problem\\n\\n<number>\\n\\n\\n\\nNotes:\\n\\nThe knapsack probem is easily solved if M is not large, but the running time can become unacceptable for large capacities.\\n\\nThe method does not work at all if M and the sizes or values are real numbers instead of integers. \\n\\nProperty  5.1 Giải thuật The dynamic programming algorithm for knapsack problem takes time proportional to  NM.\\n\\n<number>\\n\\n\\n\\n Example 4:  Warshall algorithm and Floyd algorithm\\n\\nTransitive closure \\n\\nFor directed graphs, we’re often interested in the set of vertices that can be reached from a given vertex by traversing edges from the graph in the indicated direction. \\n\\nOne operation we might want to perform is “to add an edge directly from vertex x to vertex y if there is some way to get from x to y”. \\n\\n          The graph that results from adding all edges of this nature to a directed', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'e215f194-8c8a-4539-9d81-5715579c86f4': Document(page_content=' from adding all edges of this nature to a directed graph is called the transitive closure of the graph.\\n\\nSince the transitive closure is likely to be dense, so an adjacency matrix representation is called for. \\n\\n<number>\\n\\n\\n\\nGiải thuật Warshall\\n\\nThere is a simple algorithm for computing the transitive closure of a graph represented by an adjacency matrix.\\n\\nfor y : = 1 to V do \\n\\n   for x : = 1 to V do \\n\\n      if a[x, y] then\\n\\n         for j: = 1 to V do\\n\\n            if a[y, j] then a[x, j]: = true;\\n\\nS. Warshall invented this method in 1962, using the simple observation: “If there is a way to get from node x to node y and a way to get from y to node j, then there is a way to get from node x to node j.”\\n\\n<number>\\n\\n\\n\\nAn example of computing transitive closure\\n\\n    A B', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '8110edb9-0286-42b6-9139-15569aa2b729': Document(page_content=' computing transitive closure\\n\\n    A B C D E F G H I  J K L M\\n\\nA  1  1  0  0  0  1 1  0  0  0 0  0  0\\n\\nB  0  1  0  0  0  0 0  0  0  0 0  0  0\\n\\nC  1  0  1  0  0  0 0  0  0  0 0  0  0\\n\\nD  0  0  0  1  0  1 0  0  0  0 0  0  0\\n\\nE   0  0  0 1  1  0 0  0  0  0  0  0  0\\n\\nF   0  0  0  0 1  1 0  0  0  0  0  0  0\\n\\nG  0  0  1  0 1  0 1  0  0  1  0  0  0\\n\\nH  0  0  0  0 0  0 1  1  1  0  0  0  0\\n\\nI   0  0  0  0  0  0', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'ef9d662f-3c60-4184-a4fe-8165f28f8b37': Document(page_content='  0  0  0  0  0 0  1  1  0  0  0  0\\n\\nJ   0  0  0  0  0  0 0  0  0  1  1  1  1\\n\\nK  0  0  0  0  0  0 0  0  0  0  1  0  0\\n\\nL   0  0  0  0  0  0 0  0 0   0  0  1  1\\n\\nM  0  0  0  0  0  0 0  0 0   0  0  1  1\\n\\nA\\n\\nH\\n\\nI\\n\\nB\\n\\nC\\n\\nG\\n\\nD\\n\\nE\\n\\nJ\\n\\nK\\n\\nF\\n\\nL\\n\\nM\\n\\nAdjacency matrix of the initial stage of Warshall algorithm \\n\\n<number>\\n\\n\\n\\nAn example of computing transitive closure, y=1\\n\\n    A B C D E F G H I  J K L M\\n\\nA  1  1  0  0  0  1 1  0  0  0 0  0  0', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '54d1f52f-2463-4645-9c22-5406bffc6be3': Document(page_content=' 0  0  0 0  0  0\\n\\nB  0  1  0  0  0  0 0  0  0  0 0  0  0\\n\\nC  1  1  1  0  0  1 1  0  0  0 0  0  0\\n\\nD  0  0  0  1  0  1 0  0  0  0 0  0  0\\n\\nE   0  0  0 1  1  0 0  0  0  0  0  0  0\\n\\nF   0  0  0  0 1  1 0  0  0  0  0  0  0\\n\\nG  0  0  1  0 1  0 1  0  0  1  0  0  0\\n\\nH  0  0  0  0 0  0 1  1  1  0  0  0  0\\n\\nI   0  0  0  0  0  0 0  1  1  0  0  0  0\\n\\nJ   0  0  0  0  0  0 0  0  0  1  1 ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '942f0898-2b4a-4d71-9a1e-f32cc06dc0db': Document(page_content=' 0  0  0  1  1  1  1\\n\\nK  0  0  0  0  0  0 0  0  0  0  1  0  0\\n\\nL   0  0  0  0  0  0 0  0 0   0  0  1  1\\n\\nM  0  0  0  0  0  0 0  0 0   0  0  1  1\\n\\n<number>\\n\\n\\n\\nAn example of computing transitive closure, y=2\\n\\n<number>\\n\\n\\n\\nAn example of computing transitive closure, y=3\\n\\n<number>\\n\\n\\n\\n     A B C D E F G H  I  J K L M\\n\\nA 1  1  1  1  1  1  1  0  0  1 1  1  1\\n\\nB  0 1  0  0  0  0  0  0  0  0 0  0  0\\n\\nC  1 1  1  1  1  1  1  0  0  1 1  1  1\\n\\nD  0 0  0  1  1 ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'cda272a3-bae3-4530-8101-ed07b192db70': Document(page_content='  0 0  0  1  1  1  0  0  0  0 0  0  0\\n\\nE  0  0  0 1  1  1  0  0  0  0  0  0  0\\n\\nF  0  0  0 1  1  1  0  0  0  0  0  0  0\\n\\nG  1  1  1 1 1  1  1  0  0  1  1  1  1\\n\\nH  1  1  1 1  1 1  1  1  1  1  1  1  1\\n\\nI   1  1  1  1  1 1  1  1  1  1  1  1  1\\n\\nJ   1  1  1  1  1 1  1  0  0  1  1  1 1\\n\\nK  0  0  0  0  0 0  0  0  0  0 1  0  0\\n\\nL   1  1  1 1  1 1  1  0  0  1  1  1 1\\n\\nM  1  1  1 1 ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'b91cb537-59c6-4afa-9b11-07ec58caba65': Document(page_content='\\nM  1  1  1 1  1 1  1  0  0  1  1  1 1\\n\\nAdjacency matrix of the final stage of Warshall algorithm  \\n\\nProperty 5.3.1  Warshall algorithm finds the transitive closure in O(V3).\\n\\n<number>\\n\\n\\n\\n Explaining Warshall algorithm\\n\\nWarshall algorithm repeats V iterations on the adjacency matrix a, constructing a series of V boolean matrices:\\n\\n               a(0),.., a(y-1),a(y),…,a(V)                                           (5.4)\\n\\nThe central point of the algorithm is that we can compute all elements of each matrix a(y) from its immediate predecessor a(y-1) in series (5.4)\\n\\nAfter the y-th iteration, a[x, j] is equal to 1 if and only if there exists a directed path', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '9f37cb37-036f-48fa-97dc-deeb0e053cfc': Document(page_content=' 1 if and only if there exists a directed path of a positive length from the vertex x to vertex j with each intermediate vertex, if any, numbered not higher than y. \\n\\nAfter the y-th iteration, we compute the elements of matrix a by the following formula:\\n\\n           ay[x,j] = ay-1[x,j] or (ay-1[x, y] and ay-1[y, j])   (5.5)\\n\\n     The superscript y indicates the value of an element in matrix a after the y-th iteration.\\n\\nWarshall algorithm applies dynamic programming paradigm since it uses the recurrence formula (5.5) but it does not bring out a recursive algorithm. Instead, it brings out an iterative algorithm with the support of a matrix for storing intermediate results.\\n\\n<number>\\n\\n\\n\\nExample:\\n\\n    \\t\\t      a  b  c  d\\n\\n\\t\\ta  0  1  0  0\\n\\nA(0)\\t=\\tb  0  0  0  1\\n\\n\\t\\tc  0  0  0 ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '33ec6ca3-c9c7-40f9-a3a4-eb2eb8ea8c81': Document(page_content='\\t\\tc  0  0  0  0\\n\\n\\t\\td  1  0  1  0\\n\\n\\t\\t    a  b  c  d\\n\\n\\t\\ta  0  1  0  0\\n\\nA(1)\\t=\\tb  0  0  0  1\\n\\n\\t\\tc  0  0  0  0\\n\\n\\t\\td  1  1  1  0\\n\\n\\t\\t    a  b  c  d\\n\\n\\t\\ta  0  1  0  1\\n\\nA(2)\\t=\\tb  0  0  0  1\\n\\n\\t\\tc  0  0  0  0\\n\\n\\t\\td  1  1  1  1\\n\\n                   a  b  c  d\\n\\n\\t\\ta  0  1  0  1\\n\\nA(3)\\t=\\tb  0  0  0  1\\n\\n\\t\\tc  0  0  0  0\\n\\n\\t\\td  1  1  1  1\\n\\n\\t\\t    a  b  c  d\\n\\n\\t\\ta  1', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'd4e4eaf7-38f4-4e56-9175-6b61c7db730d': Document(page_content='  c  d\\n\\n\\t\\ta  1  1  1  1\\n\\nA(4)\\t=\\tb  1  1  1  1\\n\\n\\t\\tc  0  0  0  0\\n\\n\\t\\td  1  1  1  1\\n\\na\\n\\nb\\n\\nc\\n\\nd\\n\\n<number>\\n\\n\\n\\nFloyd algorithm for the All-Pairs Shortest Paths Problem \\n\\nFor weighted graphs (directed or not) one might want to build a matrix allowing one to find the shortest path from   x to y for all pairs of vertices. This is the all-pairs shortest path problem.\\n\\nA\\n\\n4\\n\\n2\\n\\n3\\n\\nH\\n\\nI\\n\\n1\\n\\nB\\n\\n1\\n\\nC\\n\\nG\\n\\nFigure 5.7\\n\\n1\\n\\n1\\n\\n2\\n\\n2\\n\\n1\\n\\n1\\n\\nD\\n\\nE\\n\\nJ\\n\\nK\\n\\n1\\n\\n5\\n\\n2\\n\\nF\\n\\n3\\n\\n2\\n\\nL\\n\\nM\\n\\n1\\n\\n<number>\\n\\n\\n\\nFloyd algorithm\\n\\nIt is also possible to use', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '0e006f74-6a20-49e3-9cb0-102533efb508': Document(page_content='oyd algorithm\\n\\nIt is also possible to use a method just like Warshall’s method, which is attributed to R. W. Floyd:\\n\\nfor y : = 1 to V do \\n\\n   for x : = 1 to V do \\n\\n      if a [x, y] > 0 then\\n\\n         for j: = 1 to V do\\n\\n            if a [y, j] > 0 then \\n\\n               if (a[x, j] = 0) or (a[x, y] + a[y, j] < a [x, j]) \\n\\n               then\\n\\n                  a[x, j] = a[x, y] + a[y, j]; \\n\\n<number>\\n\\n\\n\\nAn example of Floyd algorithm (for Figure 5.7)\\n\\n\\x0b     A B C D  E F G H I  J K L M', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'b3fb36ac-945d-40f6-b25a-b7ff6dfac852': Document(page_content=' E F G H I  J K L M\\n\\nA  0  1  0  0  0  2 4  0  0  0 0  0  0\\n\\nB  0  0  0  0  0  0 0  0  0  0 0  0  0\\n\\nC  1  0  0  0  0  0 0  0  0  0 0  0  0\\n\\nD  0  0  0  0  0  1 0  0  0  0 0  0  0\\n\\nE   0  0  0 2  0  0 0  0  0  0  0  0  0\\n\\nF   0  0  0  0 2  0 0  0  0  0  0  0  0\\n\\nG  0  0  1  0 1  0 0  0  0  1  0  0  0\\n\\nH  0  0  0  0 0  0 3  0  1  0  0  0  0\\n\\nI   0  0  0  0  0  0 0  1  0  0  0  0 ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'e3fd76b4-7e93-4475-9df9-9dc86f6692a4': Document(page_content=' 1  0  0  0  0  0\\n\\nJ   0  0  0  0  0  0 0  0  0  0  1  3  2\\n\\nK  0  0  0  0  0  0 0  0  0  0  0  0  0\\n\\nL   0  0  0  0  0  5 5 0   0  0  0  0  1\\n\\nM  0  0  0  0  0  0 0  0 0   0  0  1  0\\n\\nAdjacency matrix in the initial stage of Floyd’s algorithm\\n\\nNotes: All the elements in the diagonal are 0.\\n\\n<number>\\n\\n\\n\\n      A B C D E F G H  I  J K L M\\n\\nA  6  1  5  6  4  2  4  0  0  5 6  8  7\\n\\nB   0 0  0  0  0  0  0  0  0  0 0  0  0\\n\\nC   1 2  6  7  5  3  5  0', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '67cb9615-96fb-4882-b803-bde477e36ef5': Document(page_content='  7  5  3  5  0  0  6 7  9  8\\n\\nD  0  0  0  5  3  1  0  0  0  0 0  0  0\\n\\nE  0  0  0 2  5  3  0  0  0  0  0  0  0\\n\\nF  0  0  0 4  2  5  0  0  0  0  0  0  0\\n\\nG  2  3  1 3 1  4  6  0  0  1  2  4  3\\n\\nH  5  6  4 6  4 7  3  2  1  4  5  7  6\\n\\nI   6  7  5  7  5  8  4  1  2  5  6  8  7\\n\\nJ 10 11 9 11 9 12  8  0  0  9  1 3  2\\n\\nK  0  0  0  0  0  0  0  0  0  0  0  0  0\\n\\nL   7  8  6 8  6  9  5 ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'bd793180-75d9-49b8-8b7a-a0c8b001ec91': Document(page_content='  6 8  6  9  5   0  0  6  7  2  1\\n\\nM  8  9  7 9  7 10  6  0  0  7  8  1 2\\n\\nAdjacency matrix in the final stage of Floyd algorithm\\n\\nProperty 5.3.2 Floyd algorithm solves the all-pairs shortest path problem in O(V3).\\n\\n<number>\\n\\n\\n\\nExplaining Floyd Algorithm\\n\\nFloyd algorithm repeats V iterations on the adjacency matrix a, constructing a series of V matrices:\\n\\n           a(0), …,a(y-1),a(y),…,a(V)                                       (5.6)\\n\\nThe central point of the algorithm is that we can compute all elements of each matrix a(y) from its immediate predecessor \\n\\na(y-1) in series (5.6)\\n\\nAfter the y-th iteration, a[x, j]', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '0f2e7445-e732-4324-86c6-3271d419ac42': Document(page_content='-th iteration, a[x, j] stores the shortest length of the direxted path from the vertex x to vertex j with each intermediate vertex, if any, numbered not higher than y. \\n\\nAfter the y-th iteration, we compute the elements of matrix a by the following formula:\\n\\n\\tay[x,j] = min( ay-1[x,j], ay-1[x, y] + ay-1[y, j])   (5.7)\\n\\nThe superscript y indicates the value of an element in matrix a after the y-th iteration.\\n\\n<number>\\n\\n\\n\\nThe formula (5.7) is illustrated by the following figure.\\n\\ny\\n\\nay-1[x,y]\\n\\nay-1[y,j ]\\n\\nj\\n\\nx\\n\\nay-1[x,j ]\\n\\nFloyd algorithm applies dynamic programming paradigm since it uses the recurrence formula (5.7) but it does not bring out a recursive algorithm. Instead, it brings out an iterative algorithm with the support of a matrix for storing intermediate results.\\n\\n<number>\\n\\n\\n\\nExample:\\n\\n    \\t', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '4d60decf-2224-4c95-8e87-8aa00f12d97f': Document(page_content='\\n\\nExample:\\n\\n    \\t\\t      a  b  c  d\\n\\n\\t\\ta  0  0  3  0              0 0 0 0\\n\\nR(0)\\t=\\tb  2  0  0  0     P(0)= 0 0 0 0\\n\\n\\t\\tc  0  7  0  1              0 0 0 0\\n\\n\\t\\td  6  0  0  0              0 0 0 0\\n\\n\\t\\t    a  b  c  d\\n\\n\\t\\ta  0  0  3  0              0 0 0 0\\n\\nR(1)\\t=\\tb  2  0  5  0     P(1)= 0 0 a 0\\n\\n\\t\\tc  0  7  0  1              0 0 0 0\\n\\n\\t\\td  6  0  9  0   ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'c3c15c80-25f5-4bcd-b874-b42333d3f42d': Document(page_content=' 6  0  9  0              0 0 a 0\\n\\n\\t\\t    a  b  c  d\\n\\n\\t\\ta  0  0  3  0              0 0 0 0\\n\\nR(2)\\t=\\tb  2  0  5  0     P(2)= 0 0 a 0\\n\\n\\t\\tc  9  7 12  1             b 0 b 0\\n\\n\\t\\td  6  0  9  0              0 0 a 0\\n\\n                   a   b  c  d\\n\\n\\t\\ta  12 10 3  4            c c 0 c\\n\\nR(3)\\t=\\tb  2  12  5  6   P(3)= 0 c a c\\n\\n\\t\\tc  9   7 12  1            b', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'e9f0645b-5ed5-4300-90ec-054cc9513cf3': Document(page_content='          b 0 b 0\\n\\n\\t\\td  6  16 9  10           0 c a c\\n\\n\\t\\t    a   b  c  d\\n\\n\\t\\ta  10 10  3  4            d c 0 c\\n\\nR(4)\\t=\\tb  2   12  5  6   P(4)= 0 c a c\\n\\n\\t\\tc  7   7  10  1            d 0 d 0\\n\\n\\t\\td  6  16  9  10           0 c a c\\n\\n2\\n\\na\\n\\nb\\n\\n7\\n\\n3\\n\\n 6\\n\\nc\\n\\nd\\n\\n1\\n\\n<number>\\n\\n\\n\\nImproving the Floyd algorithm\\n\\nIn many situations we may want to print out the cheapest path from one vertex to another. \\n\\nOne way to accomplish this is to use another matrix P, where P[i,j] holds the vertex k that led Floyd algorithm to find the smallest value of', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '0ed676a9-2aa2-4711-ab40-2dbf3132f24e': Document(page_content=' that led Floyd algorithm to find the smallest value of a[i,j]. \\n\\nThe modified version of Floyd algorithm is as follows:\\n\\nfor i := 1 to V do\\n\\n   for j:= 1 to V do\\n\\n       P[i,j] := 0;\\n\\nfor i := 1 to V do\\n\\n      a[i,i]:= 0;\\n\\n<number>\\n\\n\\n\\nfor y : = 1 to V do \\n\\n   for x : = 1 to V do \\n\\n      if a [x, y] > 0 then\\n\\n         for j: = 1 to V do\\n\\n            if a [y, j] > 0 then \\n\\n               if (a[x, j] = 0) or (a[x, y] + a[y, j] < a [x, j]) then\\n\\n                  begin\\n\\n             ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '68a691d7-55c1-4185-9da8-c2ab464aac65': Document(page_content='                 a[x, j] = a[x, y] + a[y, j]; \\n\\n\\t       P[x,j] := y;\\n\\n\\t    end\\n\\nTo print out the intermediate vertices on the shortest path from vertex x to vertex j, we invoke the procedure path(x,j) where path is a recursive procedure given in the next table.\\n\\nprocedure path(x, j: int)\\n\\nvar k : int;\\n\\nBegin \\n\\n    k := P[x,j];\\n\\n    if k = 0 then return;\\n\\n    path(x,k); writeln(k); path(k,j);\\n\\nend\\n\\n<number>\\n\\n\\n\\n2. Greedy algorithm\\n\\nAlgorithms for optimization problems typically go through a sequence of steps, with a set of choices at each step. A greedy algorithm always makes the choice that looks best at the moment.  \\n\\n\\xa0That is, it makes a locally optimal choice in the hope that this choice will leads to a globally optimal solution. \\n\\n\\xa0Some', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '89a1ccb2-74ad-441e-bdee-e294b8522c6f': Document(page_content=' to a globally optimal solution. \\n\\n\\xa0Some examples of greedy algorithm:\\n\\n  - An activity-selection problem\\n\\n  - The fractional knapsack problem\\n\\n  - Huffman code problem\\n\\n -\\xa0 Prim algorithm for minimum-spanning trees\\n\\n<number>\\n\\n\\n\\nActivity-Selection Problem\\n\\nSuppose we have a set S = {1, 2, …, n} of n activities that  wish to use a resource, such as a lecture hall, which can be used by only one activity at a  time. \\n\\nEach activity i has a starting time si and a finish time fi, mà   si \\uf0a3 fi. If selected, activity i takes place during the half-open time interval [si, fi). Activities i and j are compatible if the interval [si, fi) and [sj, fj)  do not overlap (i.e., i and j are compatible if si >= fj or sj >= fi).\\n\\n\\xa0The activity-selection problem is to select a maximum-size set of mutually compatible activities. \\n\\n<number>\\n\\n\\n\\n Greedy algorithm for activity-selection problem\\n\\nIn the greedy algorithm for activity-selection problem, we assume', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'c90f241f-fde2-41fc-814a-ccfe327424e2': Document(page_content=' greedy algorithm for activity-selection problem, we assume that the input activities are in order by increasing finish times:                   f1 \\uf0a3 f2 \\uf0a3 … \\uf0a3 fn.\\n\\nprocedure GREED-ACTIVITY-SELECTOR(S, f) ;       /* s is the array keeping the set of activities and f is the array keeping the finishing times */\\n\\nbegin\\n\\n    n := length[s];   A := {1};  j: = 1;\\n\\n    for i: = 2 to n do\\n\\n       if si >= fj then /* i is compatible with all  activities in A */\\n\\n       begin  A: = A \\uf0c8 {i}; j: = i  end\\n\\nend\\n\\n<number>\\n\\n\\n\\nProcedure Greedy-activity-selector\\n\\nThe activity picked next by GREEDY-ACTIVITY-SELECTER is always the one with the earliest finish time that can be legally scheduled. The activity picked is thus  “greedy choice” in', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '89bb20f2-ad43-496a-bace-a41b52db06fe': Document(page_content=' thus  “greedy choice” in the sense that it leaves as much opportunities as possible for the remaining activities to be scheduled. \\n\\nGreedy algorithms do not always produce optimal solutions. However, GREEDY-ACTIVITY-SELECTOR always finds an optimal solution to an instance of the activity-selection problem. \\n\\n<number>\\n\\n\\n\\n   i     si       fi\\n\\n   1     1       4\\n\\n   2     3      5\\n\\n   3     0      6\\n\\n   4     5      7\\n\\n   5     3      8\\n\\n   6     5      9\\n\\n   7     6     10\\n\\n   8     8     11\\n\\n   9     8     12\\n\\n  10    2     13\\n\\n  11   12    14\\n\\n         ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '83f88c3a-1ce0-45a2-9862-5782ab03f326': Document(page_content='\\n\\n                                  0     1      2      3      4      5     6     7      8     9    10    11    12    13    14\\n\\nFigure 5.5 An example of activity-selection problem\\n\\n<number>\\n\\n\\n\\nElements of greedy algorithm\\n\\nThere are two ingredients that are exhibited by most problems that lend themselves to a greedy strategy: (1) the greedy choice property and (2) optimal substructure.\\n\\nGreedy choice property \\n\\nThe choice made by a greedy algorithm may depend on choices so far, but it cannot depend on any future choices or on the solutions of the subproblems. Thus, unlike dynamic programming, a greedy algorithm usually progresses in a top-down fashion, making one greedy choice after another, iteratively reducing each given problem instance to a smaller. \\n\\nOptimal Substructure\\n\\nThe optimal solution for', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '45fc0dc9-5692-4e60-bacd-5704ff4d996a': Document(page_content=\"Optimal Substructure\\n\\nThe optimal solution for the problem contains within it optimal solutions to subprobems.  \\n\\n<number>\\n\\n\\n\\nGreedy versus dynamic programming\\n\\nGiven an optimization problem, it is not easy to decide whether dynamic programming or greedy algorithm should be used to solve it. Let investigate two variants of a classical optimization problem.\\n\\nThe 0-1 knapsack problem is posed as follows.\\n\\n'‘A thief robbing a store find it filled with N types of items of varying size and value (the i-th items is worth vi dollars and weights wi pounds), but has only a small knapsack of capacity M to use to carry the goods. The knapsack problem is to find the combination of items which the thief should choose for his knapsack in order to maximize the total value of  all the items he takes.”\\n\\nThis is called the 0-1 knapsack problem because each item must either be taken or left behind; the thief can not take a fractional amount of an item or take an item more than once.\\n\\n<number>\\n\\n\\n\\nFractional knapsack problem\\n\\nIn the fractional knapsack problem, the setup\", metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '0422a45b-8282-4277-bd03-876f4d350048': Document(page_content=' the fractional knapsack problem, the setup is the same, but the thief can take fractions of items, rather than having to make a binary (0-1) choice for each item.\\n\\n\\xa0Both knapsack problems exhibit the optimal substructure property. \\n\\n\\xa0\\uf0b7 For the 0-1 problem, consider the most valuable load with the weights at most M pounds. If we remove item j from this load, the remaining load must be the most valuable load weighing at most M - wj that the thief can take from the  n-1 original items excluding j. \\n\\n\\xa0\\uf0b7 For the fractional problem, consider that if we remove a weight wj -w of one item j from the optimal load, the remaining load must be the most valuable load weighting at most M – (wj –w) that the thief can take from the n-1original items, excluding item j.\\n\\n<number>\\n\\n\\n\\nFractional knapsack problem (cont.)\\n\\nWe use greedy algorithm for the fractional knapsack and dynamic programming for the 1-0 knapsack. \\xa0\\n\\nTo solve the fractional problem, we first compute the value per pound (vi/wi )', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'a6755d84-f218-4ad0-a51d-4af34e64a572': Document(page_content=' compute the value per pound (vi/wi ) for each item. \\n\\nThe thief begins by taking as much as possible of the item with the greatest value per pound (vi/wi). If the supply of that item is exhausted and he still can carry more, he takes as much as possible of the item with the next greatest value per pound, and so forth until he cannot carry any more.\\n\\n<number>\\n\\n\\n\\n Figure 5.6\\n\\n<number>\\n\\n\\n\\nprocedure GREEDY_KNAPSACK(V, W, M, X, n);\\n\\n/* V, W are the arrays contain the values and weights of n objects ordered so that Vi/Wi \\uf0b3 Vi+1/Wi+1. M is the knapsack capacity and X is solution vector */\\n\\nvar rc: real; i: integer;\\n\\nbegin\\n\\n    for i:= 1 to n do  X[i]:= 0;\\n\\n    rc := M ;  // rc = remaining knapsack\\n\\n                                    ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'c8688b95-1756-4ade-a5b9-d401c3bd5431': Document(page_content='                      capacity  //\\n\\n    for i := 1 to n do\\n\\n    begin\\n\\n        if W[i] > rc  then  exit;\\n\\n        X[i] := 1;  rc := rc – W[i]\\n\\n    end;\\n\\n    if i \\uf0a3 n then X[i] := rc/W[i]\\n\\nend\\n\\nBy sorting the items by value per pound, the greedy algorithm, the greedy algorithm runs in O(nlogn). \\n\\n<number>\\n\\n\\n\\nHuffman codes \\n\\nThis topic is related to file compression. Huffman codes are a widely used and very effective techniques for compressing data, savings of 20% to 90% are typical.\\n\\nHuffman algorithm uses a table of the frequencies of occurrences of each character as a binary strings. \\xa0\\n\\nSuppose we have a 100000 character data file that we wish to store compactly. \\n\\n<number>\\n\\n\\n\\n               ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'f2d688ad-97b4-4cf0-8f00-76a2c78e8491': Document(page_content='                                  a           b             c           d            e           f\\n\\n                      Frequency      45         13           12          16           9          5\\n\\nFixed length codeword       000      001        010        011         100       101\\n\\nVariable length codeword     0       101        100  ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'ed33299b-7005-4454-99c0-d18158a26423': Document(page_content='        100         111       1101     1100\\n\\nWe consider the problem of designing a binary character code wherein each character is represented by a unique binary string.\\n\\n\\xa0If we use a fixed length code (3 bit) to represent 6 characters:\\n\\n a = 000, b = 001, . . . , f = 101\\n\\nThis method requires 300000 bits to code the entire file.\\n\\n<number>\\n\\n\\n\\nVariable-length code\\n\\nA variable-length code can do better than a fixed length code, by giving frequent characters short code-words and infrequent characters long code-words.\\n\\n                        a = 0, b = 101, . . . f = 1100\\n\\nThis code requires:\\n\\n(45. 1 + 13 .3 + 12.3 + 16.3 + 9.4 + 5.4).1000 = 224000 bits\\n\\nto represent the file, a savings of approximately \\uf0bb 25 %.\\n\\nIn fact, this is an optimal character code for this file, as', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '7a583131-3f3d-4d42-9605-7236ca848b8d': Document(page_content=' is an optimal character code for this file, as we shall see.\\n\\n<number>\\n\\n\\n\\nPrefix-free code\\n\\nWe consider here only codes in which no codeword is also a prefix of some other codeword. Such codes are called prefix-free-code or prefix-code.\\n\\nIt is possible to show that the optimal data compression achievable by a character code can always be achieved with a prefix code. \\n\\nPrefix codes are desirable because they simplify encoding and decoding.\\n\\n\\xa0- Encoding is simple: we just concatenate the code-words representing each character of the file. \\n\\n- Decoding is simple with a prefix code. Since no codeword is a prefix of any other, the code word that begins an encoded file is unambiguous. \\n\\n<number>\\n\\n\\n\\nPrefix-free code and binary tree\\n\\nAn optimal code for a file is always represented by a full binary tree in which every non-leaf node has two children. \\n\\nWe interpret the binary codeword  for a charater as the path from the root to that character, where 0 means “go to the left child” and 1 means “go to the', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'ac8f6cb8-2bb7-4a6c-81ff-ef4524094acb': Document(page_content='” and 1 means “go to the right child”.  \\n\\nIf C is the alphabet from which the characters are drawn, then the tree for an optimal prefix code has exactly |C| leaves, one for each letter of the alphabet, and exactly |C|-1 internal node.\\n\\n<number>\\n\\n\\n\\n100\\n\\n100\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n86\\n\\n14\\n\\n55\\n\\na:45\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n30\\n\\n58\\n\\n28\\n\\n14\\n\\n25\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\na:45\\n\\nb:13\\n\\nc:12\\n\\nd:16\\n\\ne:9\\n\\nf:5\\n\\nc:12\\n\\nb:13\\n\\n14\\n\\nd:16\\n\\n0\\n\\n1\\n\\nf:5\\n\\ne:9\\n\\n(a)\\n\\n(b)\\n\\nFigure 5.7 Two ways of coding\\n\\n<number>\\n\\n\\n\\nPrefix', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'cb9dde10-e28b-48f8-8bf1-97caf23f26d1': Document(page_content='\\n\\n<number>\\n\\n\\n\\nPrefix-free code and binary tree (cont.)\\n\\n Given a tree T corresponding to a prefix code, it is a simple matter to compute the number of bits required to encode a file. \\n\\nFor each character c in the alphabet C, let f(c) denote the frequency of c in the file and dT(c) is the length of the codeword for character c. The number of bits required to encode a file is\\xa0\\n\\nwhich we define as the cost of the tree T.\\n\\n<number>\\n\\n\\n\\nConstructing a Huffman code\\n\\nHuffman invented a greedy agorithm that constructs an optimal prefix code called a Huffman code. \\n\\nThe algorithm builds the tree T corresponding to the optimal code in a bottom-up manner. It begins with a set of |C| leaves and performs a sequence of |C|-1 “merging” operations to create the final tree.\\n\\nA priority queue Q, keyed on f, is used to identify the two least frequency objects to merge together.  \\n\\nThe result of the merger of the two objects is the new object whose frequency is the sum of the frequencies', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '6e4960b5-c37c-46c7-8ed8-3a2312c011e0': Document(page_content=' new object whose frequency is the sum of the frequencies of the two objects that were merged. \\n\\n<number>\\n\\n\\n\\n(a)\\n\\nf:5\\n\\ne:9\\n\\nc:12\\n\\nb:13\\n\\nd:16\\n\\na:45\\n\\n(b)\\n\\nc:12\\n\\nb:13\\n\\n14\\n\\nd:16\\n\\na:45\\n\\n0\\n\\n1\\n\\nf:5\\n\\ne:9\\n\\n(c)\\n\\n14\\n\\nd:16\\n\\n25\\n\\na:45\\n\\n(d)\\n\\n25\\n\\n30\\n\\na:45\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\nf:5\\n\\ne:9\\n\\nc:12\\n\\nb:13\\n\\nc:12\\n\\nb:13\\n\\n14\\n\\nd:16\\n\\n0\\n\\n1\\n\\nf:5\\n\\ne:9\\n\\n(e)\\n\\na:45\\n\\n55\\n\\n(f)\\n\\n100\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n30\\n', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'f8dc265d-4de3-4201-a8f2-5ea39b57e74c': Document(page_content='\\n\\n0\\n\\n1\\n\\n30\\n\\n25\\n\\n55\\n\\na:45\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\nc:12\\n\\nb:13\\n\\n14\\n\\nd:16\\n\\n30\\n\\n25\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\nf:5\\n\\ne:9\\n\\nc:12\\n\\nb:13\\n\\n14\\n\\nd:16\\n\\n0\\n\\n1\\n\\n<number>\\n\\nFigue 5.8  The steps of Huffman algorithm\\n\\nf:5\\n\\ne:9\\n\\n\\n\\nHuffman algorithm\\n\\nprocedure HUFFMAN(C) ;\\n\\nbegin \\n\\n    n := |C| ; Q := C ;\\n\\n    for i := 1 to n -1 do\\n\\n    begin\\n\\n         z: = ALLOCATE-NODE( );\\n\\n        left[z]: = EXTRACT-MIN(Q);\\n\\n        right[z]: = EXTRACT-', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'b0e6e088-9a0a-477f-9956-3d744c0dd3af': Document(page_content='  right[z]: = EXTRACT-MIN(Q);\\n\\n        f[z] := f[left[z]] + f[right[z]];\\n\\n        INSERT(Q, z);\\n\\n    end\\n\\nend\\n\\nAssume Q  is implemented by a min-heap. \\n\\nGiven a set C of n characters, the building of Q can be done with time O(n). \\n\\nThe for  loop is executed exactly  n-1 times, and since each heap operation requires O(lgn), this loop contributes O(nlgn) to the running time. \\n\\nThus, the total running time of  HUFFMAN algorithm on a set of n characters is O(nlgn).\\n\\n<number>\\n\\n\\n\\nExample 4: Graph coloring\\n\\nGiven an undirected graph, a coloring of that graph is an assignment of a color to each vertex of the graph so that no two vertices connected by an edge have the same color. We wish to find a coloring with the minimum number of colors.\\n\\nThis is an optimization problem.\\n\\nOne reasonable strategy for graph coloring is using greedy', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'a9056498-3870-4db4-9b5c-5c04fdaf695d': Document(page_content='\\nOne reasonable strategy for graph coloring is using greedy algorithm. \\n\\nThe idea: Initially, we try to color as many vertices as possible with the first color, then as many as possible of the uncolored vertices with the second color and so on.\\n\\n Note: Greedy algorithm can not yield the optimal solution for this problem.\\n\\n<number>\\n\\n\\n\\nTo color vertices with a new color, we perform the following steps:\\n\\nSelect some uncolored vertex and color it with a new color.\\n\\nScan the list of uncolored vertices. For each uncolored vertex, determine whether it has an edge to any vertex already colored with the new color. If there is no such edge, color the present vertex with the new color. \\n\\nExample: In the following figure, we color vertex 1 with red color and then we color vertices 3 and 4 with the same red color.\\n\\n3\\n\\n1\\n\\n5\\n\\n2\\n\\n4\\n\\n<number>\\n\\n\\n\\nProcedure SAME_COLOR\\n\\nProcedure SAME_COLOR determines a set of vertices (called newclr), all of which can be colored with a new color. This procedure is called repe', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'e96d3f62-f2ab-41b9-8baa-153b1b16b797': Document(page_content=' with a new color. This procedure is called repeately until all vertices are colored.\\n\\n procedure SAME_COLOR(G, newclr);\\n\\n   /* SAME_COLOR assigns to newclr a set \\n\\n        of vertices of G that may be given the   \\n\\n        same color  */\\n\\n   begin\\n\\n    newclr := \\uf0c6;\\n\\n       for each uncolored vertex v of G do\\n\\n          if v is not adjacent to any vertex in newclr \\n\\n          then\\n\\n\\t  mark v colored and add v to newclr.\\n\\n    end;\\n\\n<number>\\n\\n\\n\\nprocedure G_COLORING(G);\\n\\n   procedure SAME_COLOR(G, newclr);\\n\\n   /* SAME_COLOR assigns to newclr a set of\\n\\n        vertices of G that may be given the same color ;\\n\\n        a: adjacency matrix for graph G */\\n\\n  ', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  'ccaacb74-bfb6-4f33-aa60-34a275b6d5fc': Document(page_content='acency matrix for graph G */\\n\\n   begin\\n\\n    newclr := \\uf0c6;\\n\\n       for each uncolored vertex v of G do\\n\\n       begin\\n\\n         found := false;\\n\\n         for each vertex w \\uf0ce newclr do\\n\\n            if a[v,w] = 1    /*there is an edge between v and w in G */ \\n\\n            then\\n\\n\\t         found := true;\\n\\n         if  not found  then\\n\\n            mark v colored and add v to newclr\\n\\n       end\\n\\n    end;\\n\\n<number>\\n\\n\\n\\nfor each vertex in G do mark uncolored;\\n\\nwhile there is any vertex marked uncolored do\\n\\nbegin\\n\\n  SAME_COLOR(G, newclr);\\n\\n  print newclr \\n\\nend.\\n\\nDegree', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '6d62a1ee-8f56-4624-8062-796fbd8ad9a4': Document(page_content=' \\n\\nend.\\n\\nDegree of a vertex: the number of edges connected to this vertex.\\n\\nTheorem: If \\uf063(G) is the minimum number of colors to color graph G and \\uf044G is the largest degree in that graph then \\uf063(G) ≤ \\uf044G +1\\n\\nComplexity of greedy algorithm for graph coloring\\n\\nAssume that the graph is represented by an adjacency matrix.\\n\\nIn procedure SAME_COLOR each cell in the adjacency matrix is examined when we color a new color for the uncolored vertices.\\n\\nComplexity of procedure SAME_COLOR: O(n2) where n is the number of vertices in G.\\n\\nIf m is the number of colors used to color the graph then procedure SAME_COLOR is called m times at all. Therefore, the complexity of the whole algorithm is   m*O(n2). Since  m is often a small number, we can say:        \\uf0de The algorithm has a quadratic complexity.\\n\\n<number>\\n\\n\\n\\nApplication:  Exam timetabling\\x0b\\n\\nEach exam is represented by', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '4b5eca46-e130-4374-83d0-cf5e9e05a04f': Document(page_content=' timetabling\\x0b\\n\\nEach exam is represented by a vertex in the graph.\\n\\nExam timetabling is assigning time periods to exams. Time periods are the colors used to color the vertices of the graph\\n\\nAn edge connects two vertices if there exist at least one student who takes both the exams, therefore we are not allowed to assign those two exams which are represented by the two vertices to the same time peroid.\\n\\nAnother application: Frequency assignment problem in wireless broadcasting or mobile telephone\\n\\n<number>\\n\\n\\n\\nA Heuristic for Graph coloring\\x0b\\n\\n“The vertex with the largest degree will be examined to color first”. \\n\\nDegree of a vertex: the number of edges connected to this vertex. \\n\\nReason: The vertices with more connected edges will be more difficult to be colored if we wait until all their adjacent vertices had been colored.\\n\\nAlgorithm\\n\\n1.Arrange the vertices by decreasing order of degrees.\\n\\n2.Color a vertex with maximal degree with color 1.\\n\\n3. Choose an uncolored vertex with a maximum degree. If there is another vertex with the same maximum degree, choose either of them.\\n\\nColor the chosen', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '0384074d-9b29-420e-a0f4-da6f0dbd021f': Document(page_content=' choose either of them.\\n\\nColor the chosen vertex with the least possible (lowest numbered) color.\\n\\nIf all vertices are colored, stop. Otherwise, return to 3.\\n\\n<number>\\n\\n\\n\\nReferences\\n\\n[1] Cormen, T. H., Leiserson, C. E, and Rivest, R. L., Introduction to Algorithms, The MIT Press, 2009.\\n\\n[2] Sedgewick, R., Algorithms in C++, Addison-Wesley, 1998.\\n\\n[3] Aho, A. V., Hoftcroft, J. E., Ullman, J.D., Data Structures and Algorithms, Addison-Wesley, 1987.\\n\\n[4] Levitin, A., Introduction to the Design and Analysis of Algorithms, 3rd Edition, Pearson, 2012.\\n\\n<number>', metadata={'source': 'data/web_data//CHAP5_En.pptx'}),\n",
       "  '8415ab71-0a9d-44b8-965a-315685196969': Document(page_content='VIETNAM NATIONAL UNIVERSITY - HO CHI MINH CITY\\nHO CHI MINH CITY UNIVERSITY OF TECHNOLOGY\\nFACULTY OF COMPUTER SCIENCE AND ENGINEERING\\nMATHS FOUNDATION for COMPUTER SCIENCE (055263)\\nAssignment\\nCommunity Structure Identification\\n(Version 0.2 for SEM231)\\nInstructors : Nguyen An Khuong, CSE-HCMUT\\nTran Tuan Anh, CSE-HCMUT\\nNguyen Tien Thinh, CSE-HCMUT\\nTeaching Assistants : Le Thanh Son, KTH Royal Institute of Technology\\nTran Dinh Vinh Thuy, ´Ecole Centrale de Lyon\\nHo Chi Minh, November 2023', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 0}),\n",
       "  '6a85a6f9-d5cd-423b-8c66-c9bed36c7092': Document(page_content='Contents\\n1 Introduction 3\\n2 Community structure identification problem 3\\n3 Some use-cases 4\\n3.1 Social networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n3.2 Customer networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n3.3 Collaboration networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n3.4 Transportation networks: Multi-link graphical representation . . . . . . . . . . . . . . 5\\n4 Traditional approaches to the community structure identification problem 6\\n4.1 Hierarchical clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n4.2 Girvan-Newman algorithm . . . . . . .', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 1}),\n",
       "  'b01bad44-bc69-446d-a124-bfe0cd026391': Document(page_content='Newman algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n4.3 Louvain algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n4.4 Matrix factorization-based methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n4.5 Tensor factorization-based methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n5 Suggested datasets 7\\n6 Guidelines 7\\n6.1 Objectives (Questions) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n6.2 Requirements and Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 1}),\n",
       "  'c4cc1588-4349-42fa-951a-ddaff65e74ae': Document(page_content=' . . . . . . . . . . . . . . 7\\nReferences 8', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 1}),\n",
       "  '54aa620b-6672-4ab5-85d4-b4f6ccc1eb88': Document(page_content='1 Introduction\\nThe origin of graph theory can be dated back to when Leonard Euler first gave his proof on the\\nK¨ onigsberg seven bridges problem. By viewing the problem abstractly, using the notations of letters\\nand lines, not only did Euler find the solution, he was able to generalize this problem. The idea of\\nEuler on solving this problem put the stepping stones for the branch of graph theory. Since then, this\\nbranch has been an important field of mathematics, with much work made to study graphs and their\\nmathematical properties. During the 20thand 21stcentury, there has been an explosion within the\\nways of using graphs to model real-life problems. Graphs are now becoming more and more useful to\\nrepresent a wide variety of systems from different areas. For instance, a graph can be used to describe\\nthe relations of one person to others on a social network by treating the person as a vertex and each of\\ntheir connections as an edge of a graph. Subsequently, a rising demand is to analyze the interactions\\namong elements of a graph.\\nBesides using the normal representation of graphs, with edges and vertices, to deal with practical\\nproblems, we can take use of', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 2}),\n",
       "  'e0e9d669-2239-4701-b425-713a313a660e': Document(page_content=' practical\\nproblems, we can take use of the description of graphs through matrices or tensors. Indeed, the duality\\nbetween graph and matrix multiplication motivates the use of graph approaches in practice. Assuming\\nwe are performing a breadth-first search on a graph depicted in Figure 1. In the first step, we would\\nlike to find all the adjacent nodes of Alice, which are Bob and Carl on the graph. We recall that the\\nrelationship about adjacency between the nodes can be represented by an adjacency matrix A= (aij)\\nin which aij= 1 means that we have a connection from node ito node jandaij= 0 otherwise. The\\nmatrix A⊤(the transpose of A) is shown in Figure 1 where the dots denote the values 1. Then, by\\nusing one-hot encoding vector vwhich is active at node Alice, we can observe that A⊤vcan be used\\nto represent the set of adjacent nodes {Bob,Carl}.\\nFigure 1: Duality between breadth-first search on a graph (left) and matrix multiplication (right) [1]\\nGraphical analysis has played a crucial part in understanding behaviors in various fields, such', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 2}),\n",
       "  '445d8aef-c9a1-429f-a6d3-b54a0f91aba3': Document(page_content=' crucial part in understanding behaviors in various fields, such as\\nbiology, sociology, and computer science. Several problems are defined for each case of investigating\\nsuch graph features.\\n2 Community structure identification problem\\nThe one problem when using complex graphical networks to model real-life behaviors is that it tends to\\nhave a high level of order and organization within the elements of the graph. These graphs can display\\nseveral clusters of density-connected vertices called communities. The vertices in one community\\nprobably share some common properties and/or behave similarly within the graph. One community\\nis hence usually sparsely connected to other communities. Community structure identification in a\\ngraph aims to detect communities (see [2] for precise definition1), in other words, clusters, or possibly\\ntheir hierarchy structure, given only the information of the graphical representation. It is one of the\\nmost important problems in graphical analysis (see [3] and in [4, Sections 10.2, 10.3, 10.5] for more\\ndetail), and it goes with many names in the literature: community detection, network clustering,\\ngraph partitioning, etc.\\n1See also at https://en.wikipedia.org/wiki/Community_structure and the references', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 2}),\n",
       "  'db85cf2e-0119-410f-b5e2-bf4ccef04d1c': Document(page_content='/wiki/Community_structure and the references therein.\\n3', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 2}),\n",
       "  'f966860d-1713-4a1b-a942-f2d821b39666': Document(page_content='For example, in the social network scenario, consider a guy who works for an American start-up,\\ngoes to the tennis club every Friday and attends Chinese classes at a center. It is understandable to\\nsuppose that everyone in the start-up knows each other well and the same thing can be said for people\\nfrom the tennis club and the Chinese classes. It is unlikely, however, to have almost every person in\\nthe start-up goes to the same center to learn some Chinese or goes to the same club to play tennis at\\nthe same time as the considering guy. This one particular instance illustrates that in real life, people\\ntend to form community groups and behave accordingly in each group with little to no connection\\nto other groups. Identifying such groups in the graphical representation enables the use of further\\napplications.\\n3 Some use-cases\\n3.1 Social networks\\nWhen speaking of social networks, we think of Facebook, Instagram, or Twitter, to name a few. In\\nthese social networks, there are always relations between two entities residing in the network. On\\nFacebook or Instagram, one account can follow or befriend another. Naturally, we want to represent\\nthe network of Facebook users as a graph, where each', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 3}),\n",
       "  '9d77378e-8fc9-4d56-81af-5bab53222913': Document(page_content=' network of Facebook users as a graph, where each vertex represents one user. There will be\\nan edge between two vertices if the two users represented by the vertices have a relation. This\\nway of visualization can be enhanced if we want to represent more than just simple relationships\\nby adding weights to the edges. For example, the “followed” edges will weigh 1 while 2 will be for\\nthe “befriended”. The communities in these networks will represent the groups that share things in\\ncommon, such as being members of the same family or sharing a similar hobby. Identifying these\\ncommunities is meaningful since we will have ideal targets to perform some specific operations. For\\nexample, we may have higher revenue by promoting hotel discounts to people from a traveling group\\nthan to those who are in a gaming group.\\nFor further illustration, Figure 2 represents a simple example of a social network. There are three\\ncommunities, each with a different number of members, represented by the large lines encircling the\\nvertices. Also, we can observe that two of the communities are separated and two of them overlapped.\\nIn practice, we can have also a community that entirely belongs to another community.\\nFigure', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 3}),\n",
       "  'b365a9c3-e7e4-482c-936f-c74486573b23': Document(page_content=' community that entirely belongs to another community.\\nFigure 2: Social network using graphical representation\\nBesides social networks as discussed, there are numbers of other use cases that can be represented\\nin the same way such as the followings.\\n3.2 Customer networks\\nSuppose that we are running an online bookstore. Customers may be interested in more than one\\ntype of book. They can find their interests in the latest fantasy thriller by Stephen King while sharing\\nsome of their pleasures for mathematical textbooks. Finding which types of books each customer\\n4', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 3}),\n",
       "  '43836a14-da90-4bd7-8673-ef1772c43936': Document(page_content='invests in can help us improve our service through investigations in the detected communities. In this\\ncase, each book is represented by a vertex in a graph and an edge between two books is created if\\nthese two are bought by the same customer.\\n3.3 Collaboration networks\\nConsider an interdisciplinary research center located in Ho chi Minh City. We can construct a graph\\nwhose vertices represent scientists in residence during any time window, say from 2013 to 2023.\\nAn edge will be created between two vertices if the corresponding researchers appear in at least\\none joint work during the given time. The communities will represent the people working on the\\nsame particular topic or with similar methodologies. In this use-case, we can see that there can be\\ncommunities within a community and overlap communities. For instance, a research article proposing\\na new algorithm can be coauthored by mathematicians for the theoretical parts and computer scientists\\nfor the experimental sections. Besides, we can have various authors working in the same field, say\\ncomputer vision, but they can form into more specific tasks such as image denoising, object detecting,\\netc. Finding such communities within the constructed graph can help tracing related works less\\nburden, which is sufficient', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 4}),\n",
       "  'a92571ff-cbfb-4290-98a1-62aaeac2b838': Document(page_content=' related works less\\nburden, which is sufficient in the first step of research.\\n3.4 Transportation networks: Multi-link graphical representation\\nIn cities around the world, there are normally means of transportation to go from one city to another,\\nsuch as car, plane, or ship. These are considered to be relationships in a graph where each city is\\na node. Hence, we place one edge for each line between the two cities. In this case, the graph is\\nmore challenging to be analyzed compared to the one introduced in Subsection 3.1, which is called\\nmulti-link graph [5, Section 7.1]. We are interested in finding the communities in this graph so we can\\ndetermine the similarities between cities, which help us in traveling or shipment services. Figure 3\\nillustrates a multi-link transportation graph with two detected communities. There is more than one\\nlink between City 1 and City 2, hence, our graph is a multi-link graph. To represent this graph, we\\ncan use the adjacent tensor.\\nFigure 3: Multi-link graph representation\\nAs in Section 1, we also have the duality between the multi-link graph and the tensor. Thus, besides\\nsome', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 4}),\n",
       "  '5e21f70f-247e-4992-8a01-03fbb933d30c': Document(page_content=' and the tensor. Thus, besides\\nsome classical methods in [6, 7, 8], we can perform tensor decomposition described in [5, Chapter 7].\\n5', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 4}),\n",
       "  '1f72e860-38a1-4c2a-b1f3-cc6b2b01cc45': Document(page_content='Tensor decomposition methods such as principal component analysis2orlow-rank approximation3\\ncould also give insights about the latent structure.\\n4 Traditional approaches to the community structure identi-\\nfication problem\\nWe introduce some approaches that can be applied in the community structure identification problem.\\nThese are to get you the ideas of the approaches. You are encouraged to choose your preference\\nmethod.\\n4.1 Hierarchical clustering\\nSince community structure identification sometimes goes with the name of graph clustering, we can\\napply some traditional graph clustering approaches such as hierarchical clustering (see Chapter 7 in\\n[4]). In this approach, the communities are created by the “closeness” between the vertices in the\\ngraph. The definition of “closeness” may vary, depending on the particular algorithm.\\n4.2 Girvan-Newman algorithm\\nThe Girvan-Newman algorithm [9, 10] detects communities by removing the edges iteratively according\\nto their highest “betweenness” score. This algorithm is based on the intuition we discussed in Section\\n2 that the communities are sparsely connected by a few edges located in the shortest path between\\nvertices. Thus, removing these edges', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 5}),\n",
       "  '379efce7-d39a-4879-a3c4-176ba99bd61b': Document(page_content=' between\\nvertices. Thus, removing these edges will reveal the communities inside the graph. Some other\\nenhancements of this algorithm can be founded in [11] and [12].\\n4.3 Louvain algorithm\\nThe idea of the Louvain algorithm [13] is based on the maximization of “modularity”, which is a\\nvalue measuring the density of edges inside a community. At each iteration, the algorithm places\\nthe vertices to other communities until to attain maximum modularity and it stops when there is no\\nfurther improvement in modularity. There are also many improvements to this algorithm that can be\\nfound on the Internet.\\n4.4 Matrix factorization-based methods\\nIf we used adjacent matrix to represent the graph, we can apply the non-negative matrix factorization\\nmethods (MNF) on the adjacent matrix to find the communities within the graph. The general idea\\nof this method is to factorize the adjacent matrix Aby two smaller matrices WandHsuch that\\nA≈WH andWandHboth have no negative elements. By examining the structures of Wand\\nH, we can find identify the communities within the graph. Some particular research works on this\\napproach can', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 5}),\n",
       "  '285bbfef-9f6c-42d9-a5f2-bfe9cfb4421e': Document(page_content=' Some particular research works on this\\napproach can be found in [14], [15], or [16].\\n4.5 Tensor factorization-based methods\\nWhen working with a multi-link graph, we can not use an adjacent matrix. Instead, we must use a\\ntensor to represent our graph. The traditional matrix factorization-based approaches obviously will\\nnot work in this case. Hence, we will consider ways to factorize the tensor representing the graph. The\\nCP decomposition [17, Section 3] can be considered as a generalization of the SVD for tensor. Indeed,\\nwith the given rank R, we can decompose a tensor Xas the sum of Rrank-one tensors. Precisely, each\\nfactor represents a “community” within the data and the number of factors Rin the approximation\\nshould loosely reflect the number of communities in the data [5, Subsection 7.2.4]. There also exists\\nother means of tensor decomposition, such as Tucker decomposition [17, Section 4]. Examinating\\nthe structures generated by these decompositions can help finding and understanding the underlying\\ncommunities of a graph.\\n2https://en.wikipedia', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 5}),\n",
       "  'c254bba8-ef88-4619-9d81-d7f6cce0a688': Document(page_content=' a graph.\\n2https://en.wikipedia.org/wiki/Principal_component_analysis\\n3https://en.wikipedia.org/wiki/Low-rank_approximation\\n6', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 5}),\n",
       "  '7fa3ab8d-e259-44b1-bd10-585fb8d1137d': Document(page_content='5 Suggested datasets\\nWe now give some datasets that can be used for the experimental parts of this assignment, note that\\nwe do not state the explicit use case for these datasets and some of these datasets require additional\\npreprocessing before being able to put into your version of implementations.\\n•Dolphins online social network: A social network of bottlenose dolphins. The dataset contains\\na list of all of links, where a link represents frequent associations between dolphins.\\n•Political books: A network of books about US politics published around the time of the 2004\\npresidential election and sold by the online bookseller amazon.com. Edges between books rep-\\nresent frequent co-purchasing of books by the same buyers.\\n•Jazz musicians: A network between Jazz musicians. Each node is a Jazz musician and an edge\\ndenotes that two musicians have played together in a band.\\n•NIPS Conference Papers Vols 0-12: A dataset contain full information of papers published from\\nvolume 0 to 12 of conference on neural information processing systems (NIPS). This dataset\\nrequires preprocessing to create a meaningful graphical representation, for instance, the collab-\\noration networks as in Section 3.\\nSeveral other datasets can be found', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 6}),\n",
       "  'b6c62860-0f7c-4ab3-8ae0-8a02b5dc65e9': Document(page_content=' Section 3.\\nSeveral other datasets can be found here and here, each with corresponding description and download\\nlink. Other datasets can also be found in the content of the papers in the References.\\nIf you find the given suggestions hard to interpret or unsatisfied, you can find from other sources\\nthat are not listed here or created your synthesis data. However, if you opt for this direction, please\\nindicate specifically where you found the dataset on the internet or how your dataset created , its\\ndescription (number of vertices and edges, some other prevelence statistics,...).\\n6 Guidelines\\n6.1 Objectives (Questions)\\n1. Choose a use-case that requires or can apply the idea of community structure identification to\\nsolve (be careful in setting the problem and limiting the scope so that your team can manage\\nto solve it within 4 weeks). You can refer to some ideas in Section 3, and in fact, you can\\nfind dozens of use cases related to the community structure identification problem through the\\nreferences in this assignment. You are encouraged to find or construct one use case of your own\\nif you find the given suggestions unsatisfied.\\n2. With the chosen use case, find or create a suitable dataset that best', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 6}),\n",
       "  '6e16b4d0-968e-48e4-8e88-95defae46aff': Document(page_content=' case, find or create a suitable dataset that best represents your problem,\\nsee Section 5 for more details. There will be a penalty if your choice dataset is not specified.\\n3. Study and implement your chosen approach for this problem and experiment with your dataset\\n(see [18] for some of the most popular approaches, and see [19, Section 7.2] for sample codes in\\nR or Python).\\n4. Explain in detail the behaviors and the results of your approach, especially the motivation, the\\napproaches, the metrics, and the dataset you use.\\n6.2 Requirements and Instructions\\n1. Form your group of up to six people and at least four on BKel as soon as possible (preferably\\nthe same with your HW team).\\n2. Write a technical report (advisedly in English if possible) by LaTeX4justifying the group’s\\nuse-case by modeling the problem using any graphical representation and specifying what com-\\nmunities mean in your case. This report should also explain your data choices and approach\\nin-depth and give meaningful observations and interpretations of your results. The report should\\n4See templates and sample available at https://www.overleaf.com/', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 6}),\n",
       "  'df8f228e-27d9-40fd-a269-1fbe2f0bd245': Document(page_content=' at https://www.overleaf.com/gallery/tagged/report\\n7', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 6}),\n",
       "  '36017ef9-3d8d-4f11-be7c-b4bac0abfdd8': Document(page_content='also cite all of your references, whether it is an article, a research paper, a textbook, etc. The\\nstructure of the report can be the following\\n(a)Chapter I. Introduction : Explain your motivation, and introduce your use-case with a\\nproblem statement (what you are trying to do with this use-case) on a specific dataset.\\n(b)Chapter II. Preliminaries : Recall or present all the definitions and properties with\\nconcrete examples of all foundations for later uses. [ Doing this part carefully will be helpful\\nwith your final examination. ]\\n(c)Chapter III. Approaches : Present your choices of algorithm or method that can be used\\nto solve the problem with a detailed explanation.\\n(d)Chapter IV. Experiments : Perform numerical experiments using the implemented ap-\\nproach or approaches on your dataset and give your in-depth analysis.\\n(e)Chapter V. Conclusion : Summarize what you have done, your limitations, and the\\ndirections for future works.\\n(f)References .\\n3. Write the codes (required) and prepare demonstrations (if any) your group made during the\\ntime doing this assignment.\\n4. Prepare slides by Be', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 7}),\n",
       "  'e57a73f2-966e-415d-82dc-97c174ed4a28': Document(page_content=' this assignment.\\n4. Prepare slides by Beamer5explaining your group work thoroughly. The number of slides should\\nnot be lengthy (at most 30 slides, preferably 20-25 slides or less) with adequate contents of your\\nwork. All groups must give presentations with a reporting time of no more than 45 minutes for\\neach group, record it with your team’s faces6, and submit the link together with the assignment\\n(selected teams will have a chance to give presentations during the two last lectures (scheduled as\\nmake-up lectures and will take place in the last week of December 2023). You will be provided\\nwith the entire content of all other groups before the presentation day. Based on that, each\\ngroup must prepare at least two questions and two comments/suggestions for all other groups\\nto give the presentation on another day.\\n5. All of the meeting minutes of your group must be combined into one .txt file and consist of the\\n% of each member’s effort on the whole (total effort is 100%) on the first line.\\n6. Please compress all materials relating to your work as mentioned above in one .zip file and\\nonly the team leader submits it to the e-', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 7}),\n",
       "  '4de6a549-e44c-4dab-a9fb-75e98dbddbb1': Document(page_content=' the team leader submits it to the e-learning site of this course.7You will have 4 weeks\\nto do this project, starting from November 12, 2023 (hard deadline, there will be no\\nextension.) Note that plagiarism is strictly prohibited and will be handled accordingly.\\n7. Some basic technical backgrounds related to this assignment will be asked in the final exam.\\nTherefore, team members must work together so that all of you understand all aspects of the\\nproject. The team leader should organize the team to meet this requirement.\\nThis assignment is two-fold:\\n1. You are expected to find at least an applicable use-case that can be modeled by a single graph\\nfor the community structure identification problem, such as the ones introduced in Section 3.1,\\nand implement at least one algorithm from the suggestions in Section 4 to solve this problem\\nwith your use case.\\n2. At a more advanced level, you can work a use-case where a multi-graph must be used to model\\nand implement some appropriate algorithms to find the answers to your problem. You can also\\ntackle a case where the graph’s adjacent matrix (or tensor) representation is sparse. If you opt\\nfor this direction', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 7}),\n",
       "  'fb869c96-9c3f-47d8-8dc8-77e0970990a1': Document(page_content=' is sparse. If you opt\\nfor this direction, you can work with a group of less than 4 (strong) members.\\nIf you are not used to working with graphs or/and having trouble comprehending concepts in\\nEnglish. In that case, a suggestion is to support with [3], rather well-written in Vietnamese and\\nprovided with several explicit computational examples, to grasp the main idea of this assignment\\nbefore continuing working.\\n5See templates and samples available at https://www.overleaf.com/gallery/tagged/presentation\\n6See a sample at https://www.youtube.com/watch?v=L263tOAkWC0\\n7The assignment link on BKeL\\n8', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 7}),\n",
       "  '207aa871-43f2-4ccd-8cd3-28a6db5813dc': Document(page_content='References\\n[1] J. Kepner, D. A. Bader, T. Davis, R. Pearce, and M. M. Wolf, “Graphblas and graphchallenge\\nadvance network frontiers,” SIAM News , vol. 55, no. 08, October 2022, URL:\\nhttps://sinews.siam.org/Details-Page/graphblas-and-graphchallenge-advance-netwo\\nrk-frontiers .\\n[2] S. Fortunato and C. Castellano, Community Structure in Graphs , pp. 490–512. New York, NY:\\nSpringer New York, 2012, URL: https://doi.org/10.1007/978-1-4614-1800-9_33 .\\n[3] N. T. Duc, “Community detection on social graphs, B. Eng Thesis, HCMUT, VNU-HCM (in\\nvietnamese),” 2018, URL: https://tinyurl.com/TanDucThesis .\\n[4] J. Leskovec, A. Rajaraman, and J. D. Ullman', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 8}),\n",
       "  '004cbda1-756e-4d79-9865-2ddc15f2c08f': Document(page_content='aman, and J. D. Ullman, Mining of massive data sets . Cambridge University\\nPress, 2020, URL: https://www.mmds.org/ .\\n[5] J. Kepner and J. Gilbert, Graph algorithms in the language of linear algebra . SIAM, 2011, URL:\\nhttps://doi.org/10.1137/1.9780898719918 .\\n[6] J. Kim and J.-G. Lee, “Community detection in multi-layer graphs: A survey,” ACM SIGMOD\\nRecord , vol. 44, no. 3, pp. 37–48, 2015.\\n[7] L. Getoor and C. P. Diehl, “Link mining: a survey,” Acm Sigkdd Explorations Newsletter , vol. 7,\\nno. 2, pp. 3–12, 2005.\\n[8] L. Getoor, N. Friedman, D. Koller, and B. Taskar, “Learning probabilistic models of link struc-\\nture,” Journal of Machine Learning Research , vol. 3, no. Dec, pp. 679–707, 2002.', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 8}),\n",
       "  '30afe775-4126-442c-90c1-fbe5b20e5191': Document(page_content=', pp. 679–707, 2002.\\n[9] M. Girvan and M. E. Newman, “Community structure in social and biological networks,”\\nProceedings of the national academy of sciences , vol. 99, no. 12, pp. 7821–7826, 2002, URL:\\nhttps://www.pnas.org/doi/10.1073/pnas.122653799 .\\n[10] M. E. Newman and M. Girvan, “Finding and evaluating community structure in networks,”\\nPhysical review E , vol. 69, no. 2, p. 026113, 2004, URL: https://arxiv.org/pdf/cond-mat/0\\n308217.pdf .\\n[11] M. E. Newman, “Fast algorithm for detecting community structure in networks,” Physical review\\nE, vol. 69, no. 6, p. 066133, 2004, URL: https://arxiv.org/pdf/cond-mat/0309508.pdf .\\n[12] F. Radicchi, C. Castellano, F. Cecconi, V. Lore', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 8}),\n",
       "  'f6aa42a1-a547-4d00-b58f-b13510369ca5': Document(page_content=', F. Cecconi, V. Loreto, and D. Parisi, “Defining and identifying com-\\nmunities in networks,” Proceedings of the national academy of sciences , vol. 101, no. 9, pp. 2658–\\n2663, 2004, URL: https://www.pnas.org/doi/10.1073/pnas.0400054101 .\\n[13] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, “Fast unfolding of communities\\nin large networks,” Journal of statistical mechanics: theory and experiment , vol. 2008, no. 10,\\np. P10008, 2008, URL: https://arxiv.org/pdf/0803.0476.pdf .\\n[14] F. Wang, T. Li, X. Wang, S. Zhu, and C. Ding, “Community discovery using nonnegative matrix\\nfactorization,” Data Mining and Knowledge Discovery , vol. 22, no. 3, pp. 493–521, 2011, URL:\\nhttps://www.researchgate.net/', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 8}),\n",
       "  '718f6fc3-fb04-4f88-900d-093332258843': Document(page_content='\\nhttps://www.researchgate.net/publication/220451825_Community_discovery_using_non\\nnegative_matrix_factorization .\\n[15] Z.-Y. Zhang, Y. Wang, and Y.-Y. Ahn, “Overlapping community detection in complex networks\\nusing symmetric binary matrix factorization,” Physical Review E , vol. 87, no. 6, p. 062803, 2013,\\nURL: https://arxiv.org/pdf/1303.5855.pdf .\\n[16] N. P. Nguyen and M. T. Thai, “Finding overlapped communities in online social networks with\\nnonnegative matrix factorization,” in MILCOM 2012-2012 IEEE Military Communications Con-\\nference , pp. 1–6, IEEE, 2012, URL: https://ieeexplore.ieee.org/abstract/document/6415\\n744.\\n9', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 8}),\n",
       "  '0f1a9bf8-c21d-4af7-a8f1-3229071895a7': Document(page_content='[17] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,” SIAM review , vol. 51,\\nno. 3, pp. 455–500, 2009.\\n[18] V. L. Dao, C. Bothorel, and P. Lenca, “Community structure: A comparative evaluation of\\ncommunity detection methods,” Network Science , vol. 8, no. 1, pp. 1–41, 2020, URL: https:\\n//arxiv.org/pdf/1812.06598 .\\n[19] K. McNulty, Handbook of Graphs and Networks in People Analytics: With Examples in R and\\nPython . CRC Press, 2022, URL: https://ona-book.org/ .\\n10', metadata={'source': 'data/web_data//HCMUT_MATHS4CS__055263__SEM231_Assignment___Community_Structure_Identification.pdf', 'page': 9}),\n",
       "  '9dec37bf-a7da-41cb-a5e1-795b93ed40d9': Document(page_content='CH1. INTRODUCTIONSOFTWARE TESTINGCO3015 / CO5252', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 0}),\n",
       "  'b3255b37-8241-420a-a6f3-24c515127341': Document(page_content='´The role and the importance of software testing´Testing levels´Testcases´Basic principles of software testing´Testing process and plan´Test automationContent\\nAug 2022Software Testing - Introduction2', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 1}),\n",
       "  '5a2d3fdf-1ee3-4bd2-80ec-6ff302866958': Document(page_content='Why do we test software?´Software is with us!´Is it safe?´Fault vs. Failure vs. Error´Fault : It is a condition that causesthe software to fail to perform its required function.´Error : Refers to differencebetween Actual Output and Expected output.´Failure : It is the inabilityof a system or component to perform required function according to its specification.\\nAug 2022Software Testing - Introduction3https://softwaretestingtimes.com/2010/04/fault-error-failure.html', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 2}),\n",
       "  '1fd48465-5ed5-4add-adad-f017b826b25c': Document(page_content='What is software testing?´Software Testing is a method to check whether the actual software product matches expected requirements and to ensure that software product is defect free. It involves execution of software/system components using manual or automated tools to evaluate one or more properties of interest. The purpose of software testing is to identify errors, gaps or missing requirements in contrast to actual´[1] Testing is the process of executing a program with the intent of finding errors.\\nAug 2022Software Testing - Introduction4https://www.guru99.com/software-testing-introduction-importance.html', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 3}),\n",
       "  '1a9b01df-7430-4975-8cf9-070831bcd1e7': Document(page_content='´The role of testing in software development begins with improved reliability, quality and performance of the software. It assists a developer to check out whether the software is performing the right way and to assure that software is not performing what it is not supposed to do.\\nAug 2022Software Testing - Introduction5https://www.testbytes.net/blog/role-of-software-testing-in-software-development/The role and the importance of software testing', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 4}),\n",
       "  '50cc0fb9-aff2-47f3-963d-0acc75949602': Document(page_content='Testcases´A test case is a specification of the inputs, execution conditions, testing procedure, and expected results that define a single testto be executed to achieve a particular software testing objective, such as to exercise a particular program path or to verify compliance with a specific requirement\\nAug 2022Software Testing - Introduction6https://en.wikipedia.org/wiki/Test_case', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 5}),\n",
       "  '8ce3843f-5197-4d67-b04b-d3e4d5e3cf91': Document(page_content='Testcases vs. test scenario´A Test case is a set of actions executed to verify a particular feature or functionality of your software application. A Test Case contains test steps, test data, precondition, postcondition developed for specific test scenario to verify any requirement. The test case includes specific variables or conditions, using which a testing engineer can compare expected and actual results to determine whether a software product is functioning as per the requirements of the customer.´A Test Scenario is defined as any functionality that can be tested. It is a collective set of test cases which helps the testing team to determine the positive and negative characteristics of the project.Aug 2022Software Testing - Introduction7https://www.guru99.com/test-case-vs-test-scenario.html', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 6}),\n",
       "  '62e3acd5-0ce2-49f8-a8b6-a926be819b9c': Document(page_content='Basic principles of software testing [2]´Principle 1: Testing shows the presence of defects, not their absence.´Principle 2: Exhaustive testing is impossible.´Principle 3: Testing activities should start as early as possible.´Principle 4: Defect clustering.´Principle 5: The pesticide paradox.´Principle 6: Testing is context dependent.´Principle 7: No failures means the system is useful is a fallacy.\\nAug 2022Software Testing - Introduction8', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 7}),\n",
       "  '0748a879-77b2-4a4f-9471-54c8801e5803': Document(page_content='Testing process –The general V-model [2]\\nAug 2022Software Testing - Introduction9\\n', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 8}),\n",
       "  'ed909967-8f1e-4705-8afd-1ff59f322762': Document(page_content='Aug 2022Software Testing - Introduction10\\nA MODEL OF THE SOFTWARE TESTING PROCESS\\nSep 2019CHAPTER 8. SOFTWARE TESTING8Design testcasesPrepare testdataRun programwith test dataCompare resultsto test casesT estcasesT estdataT estresultsT estreports', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 9}),\n",
       "  'fc083ef0-e7fd-494b-af63-e2953eb8ae54': Document(page_content='Testing levels\\nAug 2022Software Testing - Introduction11\\nhttps://www.seguetech.com/the-four-levels-of-software-testing/', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 10}),\n",
       "  '6809d8f3-eafd-431f-92d1-f3d8b743422c': Document(page_content='Test plan´ATest Planis a detailed document that describes the test strategy, objectives, schedule, estimation, deliverables, and resources required to perform testing for a software product. Test Plan helps us determine the effort needed to validate the quality of the application under test. The test plan serves as a blueprint to conduct software testing activities as a defined process, which is minutely monitored and controlled by the test manager.´As per ISTQB definition: “Test Plan is A document describing the scope, approach, resources, and schedule of intended test activities.”\\nAug 2022Software Testing - Introduction12https://www.guru99.com/what-everybody-ought-to-know-about-test-planing.html', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 11}),\n",
       "  '1f115553-316f-4569-903e-5a75f178ca9b': Document(page_content='Test plan´ATest Planis a detailed document that describes the test strategy, objectives, schedule, estimation, deliverables, and resources required to perform testing for a software product. Test Plan helps us determine the effort needed to validate the quality of the application under test. The test plan serves as a blueprint to conduct software testing activities as a defined process, which is minutely monitored and controlled by the test manager.´As per ISTQB definition: “Test Plan is A document describing the scope, approach, resources, and schedule of intended test activities.”\\nAug 2022Software Testing - Introduction13https://www.guru99.com/what-everybody-ought-to-know-about-test-planing.html\\n', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 12}),\n",
       "  '5bba7371-fc38-48db-9c87-512f9f0d5976': Document(page_content='Test automation´The use of software to controlthe execution of tests, the comparison of actual outcomes to predicted outcomes, the setting up of test preconditions, and other test control and test reporting functions\\nAug 2022Software Testing - Introduction14', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 13}),\n",
       "  'db90aef9-eccb-4aea-b2a1-29b11b5f1be7': Document(page_content='Summary´Software testing is important´What is software testing vs. testing purposes/goals´Satisfy the requirements vs. finding defects´Testing levels´Unit/Component -> Integration -> System -> Acceptance´Testcases´Basic principles of software testing´7 principles´Testing process and plan´The V-model´The plan´Test automationAug 2022Software Testing - Introduction15', metadata={'source': 'data/web_data//1_Introduction to Software Testing.pdf', 'page': 14}),\n",
       "  'eb7271b1-c02f-45ce-82b4-4f5a1be9f141': Document(page_content='Introduction  to Algorithms  \\nFourth  Edition  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 2}),\n",
       "  'ad7027f8-0401-4c3e-8789-d1f25d6cfe07': Document(page_content='Thomas H. Cormen \\nCharles E. Leiserson \\nRonald L. Rivest \\nClifford Stein \\nIntroduction  to Algorithms  \\nFourth  Edition  \\nThe MIT Press \\nCambridge, Massachusetts London, England ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 4}),\n",
       "  '2da2af1f-281d-4fc8-8774-6eae57350011': Document(page_content='c \\ue001  2022 Massachusetts Institute of Technology \\nAll rights reserved. No part of this book may be reproduced in any form or by any elect ronic or mechanical means \\n(including photocopying, recording, or information storage and retrieval) without permission in writ ing from the \\npublisher. \\nThe MIT Press would like to thank the anonymous peer reviewers who provided comment s on drafts of this book. \\nThe generous work of academic experts is essential for establishing the authority and qua lity of our publications. \\nWe acknowledge with gratitude the contributions of these otherwise uncredited reade rs. \\nThis book was set in Times Roman and MathTime Professional II by the authors. \\nNames: Cormen, Thomas H., author. j Leiserson, Charles Eric, author. j \\nRivest, Ronald L., author. j Stein, Clifford, author. \\nTitle: Introduction to algorithms / Thomas H. Cormen, Charles E. Leiserson, \\nRonald L. Rivest, Clifford Stein. \\nDescription: Fourth edition. j Cambridge, Massachusetts : The MIT Press, \\n[2022] j Includes bibliographical references and index. \\nIdentiûers', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 5}),\n",
       "  '9f7967c4-0660-48d6-9da4-c4a7da312c94': Document(page_content=' references and index. \\nIdentiûers:  LCCN  2021037260  j ISBN  9780262046305  \\nSubjects: LCSH: Computer programming. j Computer algorithms. \\nClassiûcation:  LCC  QA76.6  .C662  2022  j DDC  005.13--dc23  \\nLC  record  available  at http://lccn.loc.gov/2021037260  \\n10 9 8 7 6 5 4 3 2 1  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 5}),\n",
       "  '40f4e148-c994-4ef7-8c16-afd3f377db6f': Document(page_content='Contents \\nPreface  xiii  \\nI Foundations  \\nIntroduction  3 \\n1 The  Role  of Algorithms  in Computing  5 \\n1.1  Algorithms  5 \\n1.2  Algorithms  as a technology  12 \\n2 Getting  Started  17  \\n2.1  Insertion  sort  17 \\n2.2 Analyzing algorithms 25 \\n2.3  Designing  algorithms  34 \\n3 Characterizing  Running  Times  49  \\n3.1  O-notation,  �-notation,  and  ‚-notation  50 \\n3.2  Asymptotic  notation:  formal  deûnitions  53 \\n3.3  Standard  notations  and  common  functions  63 \\n4 Divide-and-Conquer  76  \\n4.1  Multiplying  square  matrices  80 \\n4.2  Strassen’s  algorithm  for  matrix  multiplication  85 \\n4.3  The  substitution  method  for  solving  recurrences  90 \\n4.4', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 6}),\n",
       "  '16a94eb4-1a32-4f0d-a385-4ae4f2559c74': Document(page_content=' recurrences  90 \\n4.4  The  recursion-tree  method  for  solving  recurrences  95 \\n4.5  The  master  method  for  solving  recurrences  101 \\n? 4.6  Proof  of the  continuous  master  theorem  107 \\n? 4.7  Akra-Bazzi  recurrences  115 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 6}),\n",
       "  '41cf55d8-f71c-461c-aef7-ee77d68f6113': Document(page_content='vi Contents \\n5 Probabilistic  Analysis  and  Randomized  Algorithms  126  \\n5.1  The  hiring  problem  126 \\n5.2  Indicator  random  variables  130 \\n5.3  Randomized  algorithms  134 \\n? 5.4  Probabilistic  analysis  and  further  uses  of indicator  random variables \\n140 \\nII Sorting  and  Order  Statistics  \\nIntroduction  157  \\n6 Heapsort  161  \\n6.1  Heaps  161 \\n6.2  Maintaining  the  heap  property  164 \\n6.3  Building  a heap  167 \\n6.4  The  heapsort  algorithm  170 \\n6.5  Priority  queues  172 \\n7 Quicksort  182  \\n7.1  Description  of quicksort  183 \\n7.2  Performance  of quicksort  187 \\n7.3  A randomized  version  of quicksort  191 \\n7.4  Analysis  of quicksort  193 \\n8 Sorting  in Linear', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 7}),\n",
       "  'fff91f67-81df-4211-8f73-ce063d7a663a': Document(page_content='  193 \\n8 Sorting  in Linear  Time  205  \\n8.1  Lower  bounds  for  sorting  205 \\n8.2  Counting  sort  208 \\n8.3  Radix  sort  211 \\n8.4  Bucket  sort  215 \\n9 Medians  and  Order  Statistics  227  \\n9.1  Minimum  and  maximum  228 \\n9.2 Selection in expected linear time 230 \\n9.3  Selection  in worst-case  linear  time  236 \\nIII  Data  Structures  \\nIntroduction  249  \\n10  Elementary  Data  Structures  252  \\n10.1  Simple  array-based  data  structures:  arrays,  matrices , stacks, queues \\n252 \\n10.2  Linked  lists  258 \\n10.3  Representing  rooted  trees  265 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 7}),\n",
       "  '8ac318a8-265a-4521-9477-631f5b250787': Document(page_content='Contents vii \\n11  Hash  Tables  272  \\n11.1  Direct-address  tables  273 \\n11.2  Hash  tables  275 \\n11.3  Hash  functions  282 \\n11.4  Open  addressing  293 \\n11.5  Practical  considerations  301 \\n12  Binary  Search  Trees  312  \\n12.1  What  is a binary  search  tree?  312 \\n12.2  Querying  a binary  search  tree  316 \\n12.3  Insertion  and  deletion  321 \\n13  Red-Black  Trees  331  \\n13.1  Properties  of red-black  trees  331 \\n13.2  Rotations  335 \\n13.3  Insertion  338 \\n13.4  Deletion  346 \\nIV  Advanced  Design  and  Analysis  Techniques  \\nIntroduction  361  \\n14  Dynamic  Programming  362  \\n14.1  Rod  cutting  363 \\n14.2  Matrix-chain  multiplication  373 \\n14.3  Elements  of', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 8}),\n",
       "  'f2e72dad-2579-413b-be23-a950039842f7': Document(page_content=' 373 \\n14.3  Elements  of dynamic  programming  382 \\n14.4  Longest  common  subsequence  393 \\n14.5  Optimal  binary  search  trees  400 \\n15  Greedy  Algorithms  417  \\n15.1  An  activity-selection  problem  418 \\n15.2  Elements  of the  greedy  strategy  426 \\n15.3  Huffman  codes  431 \\n15.4  Ofüine  caching  440 \\n16  Amortized  Analysis  448  \\n16.1  Aggregate  analysis  449 \\n16.2  The  accounting  method  453 \\n16.3  The  potential  method  456 \\n16.4  Dynamic  tables  460 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 8}),\n",
       "  '1e5b32b4-aff4-4ea8-a3b6-3f467b697194': Document(page_content='viii Contents \\nV Advanced  Data  Structures  \\nIntroduction  477  \\n17  Augmenting  Data  Structures  480  \\n17.1  Dynamic  order  statistics  480 \\n17.2  How  to augment  a data  structure  486 \\n17.3  Interval  trees  489 \\n18  B-Trees  497  \\n18.1  Deûnition  of B-trees  501 \\n18.2  Basic  operations  on  B-trees  504 \\n18.3  Deleting  a key  from  a B-tree  513 \\n19  Data  Structures  for  Disjoint  Sets  520  \\n19.1  Disjoint-set  operations  520 \\n19.2  Linked-list  representation  of disjoint  sets  523 \\n19.3  Disjoint-set  forests  527 \\n? 19.4  Analysis  of union  by  rank  with  path  compression  531 \\nVI  Graph  Algorithms  \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 9}),\n",
       "  'efdf76dd-6b5d-4820-8aaa-00070498927f': Document(page_content='VI  Graph  Algorithms  \\nIntroduction  547  \\n20  Elementary  Graph  Algorithms  549  \\n20.1  Representations  of graphs  549 \\n20.2  Breadth-ûrst  search  554 \\n20.3  Depth-ûrst  search  563 \\n20.4  Topological  sort  573 \\n20.5  Strongly  connected  components  576 \\n21  Minimum  Spanning  Trees  585  \\n21.1  Growing  a minimum  spanning  tree  586 \\n21.2  The  algorithms  of Kruskal  and  Prim  591 \\n22  Single-Source  Shortest  Paths  604  \\n22.1  The  Bellman-Ford  algorithm  612 \\n22.2  Single-source  shortest  paths  in directed  acyclic  graphs 616 \\n22.3  Dijkstra’s  algorithm  620 \\n22.4  Difference  constraints  and  shortest  paths  626 \\n22.5 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 9}),\n",
       "  '16eff326-409d-4805-a96f-6162c2b1c6de': Document(page_content=' paths  626 \\n22.5  Proofs  of shortest-paths  properties  633 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 9}),\n",
       "  '39bdfeec-f07c-4470-a0bb-50a4c4b42c00': Document(page_content='Contents ix \\n23  All-Pairs  Shortest  Paths  646  \\n23.1  Shortest  paths  and  matrix  multiplication  648 \\n23.2  The  Floyd-Warshall  algorithm  655 \\n23.3  Johnson’s  algorithm  for  sparse  graphs  662 \\n24  Maximum  Flow  670  \\n24.1  Flow  networks  671 \\n24.2  The  Ford-Fulkerson  method  676 \\n24.3  Maximum  bipartite  matching  693 \\n25  Matchings  in Bipartite  Graphs  704  \\n25.1  Maximum  bipartite  matching  (revisited)  705 \\n25.2  The  stable-marriage  problem  716 \\n25.3  The  Hungarian  algorithm  for  the  assignment  problem  723 \\nVII  Selected  Topics  \\nIntroduction  745  \\n26  Parallel  Algorithms  748  \\n26.1  The  basics  of fork-join ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 10}),\n",
       "  '419bf19e-f286-4381-a8f4-47fdc2d57005': Document(page_content='  The  basics  of fork-join  parallelism  750 \\n26.2  Parallel  matrix  multiplication  770 \\n26.3  Parallel  merge  sort  775 \\n27  Online  Algorithms  791  \\n27.1  Waiting  for  an elevator  792 \\n27.2  Maintaining  a search  list  795 \\n27.3  Online  caching  802 \\n28  Matrix  Operations  819  \\n28.1  Solving  systems  of linear  equations  819 \\n28.2  Inverting  matrices  833 \\n28.3  Symmetric  positive-deûnite  matrices  and  least-squar es approximation \\n838 \\n29  Linear  Programming  850  \\n29.1  Linear  programming  formulations  and  algorithms  853 \\n29.2 Formulating problems as linear programs 860 \\n29.3  Duality  866 \\n30  Polynomials  and  the  FFT  877  \\n30.1  Representing  polynomials ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 10}),\n",
       "  '9e7f9225-1660-4f2a-beb7-8ad9ccda88ae': Document(page_content='1  Representing  polynomials  879 \\n30.2  The  DFT  and  FFT  885 \\n30.3  FFT  circuits  894 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 10}),\n",
       "  'ca6d28b6-5bea-4be1-acae-6b592d3f40a3': Document(page_content='x Contents \\n31  Number-Theoretic  Algorithms  903  \\n31.1  Elementary  number-theoretic  notions  904 \\n31.2  Greatest  common  divisor  911 \\n31.3  Modular  arithmetic  916 \\n31.4  Solving  modular  linear  equations  924 \\n31.5  The  Chinese  remainder  theorem  928 \\n31.6  Powers  of an element  932 \\n31.7  The  RSA  public-key  cryptosystem  936 \\n? 31.8  Primality  testing  942 \\n32  String  Matching  957  \\n32.1  The  naive  string-matching  algorithm  960 \\n32.2  The  Rabin-Karp  algorithm  962 \\n32.3  String  matching  with  ûnite  automata  967 \\n? 32.4  The  Knuth-Morris-Pratt  algorithm  975 \\n32.5  Sufûx  arrays  985 \\n33', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 11}),\n",
       "  '4171982e-1ab8-4f0a-a1a0-e035712a7f96': Document(page_content='ûx  arrays  985 \\n33  Machine-Learning  Algorithms  1003  \\n33.1  Clustering  1005 \\n33.2  Multiplicative-weights  algorithms  1015 \\n33.3  Gradient  descent  1022 \\n34  NP-Completeness  1042  \\n34.1  Polynomial  time  1048 \\n34.2  Polynomial-time  veriûcation  1056 \\n34.3  NP-completeness  and  reducibility  1061 \\n34.4  NP-completeness  proofs  1072 \\n34.5  NP-complete  problems  1080 \\n35  Approximation  Algorithms  1104  \\n35.1  The  vertex-cover  problem  1106 \\n35.2  The  traveling-salesperson  problem  1109 \\n35.3  The  set-covering  problem  1115 \\n35.4  Randomization  and  linear  programming  1119 \\n35.5  The  subset-sum  problem  112', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 11}),\n",
       "  'd2eeb143-5cb4-4ab5-b666-818a41c1f4bb': Document(page_content='  The  subset-sum  problem  1124 \\nVIII  Appendix:  Mathematical  Background  \\nIntroduction  1139  \\nA Summations  1140  \\nA.1  Summation  formulas  and  properties  1140 \\nA.2 Bounding summations 1145 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 11}),\n",
       "  '485f6c4c-a272-4b05-a4b1-11552d632db7': Document(page_content='Contents xi \\nB Sets,  Etc.  1153  \\nB.1  Sets  1153 \\nB.2 Relations 1158 \\nB.3  Functions  1161 \\nB.4  Graphs  1164 \\nB.5  Trees  1169 \\nC Counting  and  Probability  1178  \\nC.1  Counting  1178 \\nC.2 Probability 1184 \\nC.3  Discrete  random  variables  1191 \\nC.4  The  geometric  and  binomial  distributions  1196 \\n? C.5  The  tails  of the  binomial  distribution  1203 \\nD Matrices  1214  \\nD.1  Matrices  and  matrix  operations  1214 \\nD.2 Basic matrix properties 1219 \\nBibliography  1227  \\nIndex  1251  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 12}),\n",
       "  '18e7a2db-a8ee-472f-9bcf-7c6c990c5aa9': Document(page_content='Preface \\nNot so long ago, anyone who had heard the word <alg orithm= was almost certainly \\na computer scientist or mathematician. With compute rs having become prevalent in \\nour modern lives, however, the term is no longer es oteric. If you look around your \\nhome,  you’ll  ûnd  algorithms  running  in the  most  mundane  places: your microwave \\noven, your washing machine, and, of course, your co mputer. You ask algorithms \\nto make recommendations to you: what music you migh t like or what route to \\ntake  when  driving.  Our  society,  for  better  or for  worse,  asks  algorithms to suggest \\nsentences for convicted criminals. You even rely on  algorithms to keep you alive, \\nor at least not to kill you: the control systems in  your car or in medical equipment. 1 \\nThe word <algorithm= appears somewhere in the news seemingly every day. \\nTherefore, it behooves you to understand algorithms  not just as a student or \\npractitioner of computer science, but as a citizen of the world. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 14}),\n",
       "  '6ff861af-6a52-41f2-9041-c879bf145c11': Document(page_content=', but as a citizen of the world.  Once  you  understand  \\nalgorithms, you can educate others about what algor ithms are, how they operate, \\nand what their limitations are. \\nThis book provides a comprehensive introduction to the modern study  of com-  \\nputer algorithms. It presents many algorithms and c overs them in considerable \\ndepth, yet makes their design accessible to all lev els of readers. All the analyses \\nare laid out, some simple, some more involved. We h ave tried to keep explanations \\nclear  without  sacriûcing  depth  of coverage  or mathematical  rigor. \\nEach chapter presents an algorithm, a design techni que, an application area, or a \\nrelated topic. Algorithms are described in English and in a pseudocode designed to \\nbe readable by anyone who has done a little program ming. The book  contains  231  \\nûgures4many  with  multiple  parts4illustrating  how  the  algorithms work. Since \\nwe emphasize efﬁciency  as a design criterion, we include careful analyses of the \\nrunning times', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 14}),\n",
       "  '8768c33a-5778-4a41-99b9-f7c058260222': Document(page_content=' we include careful analyses of the \\nrunning times of the algorithms. \\n1 To  understand  many  of the  ways  in which  algorithms  inüuence  our daily lives, see the book by \\nFry  [162].  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 14}),\n",
       "  'c49f1571-053f-4622-9da4-0ad50f303945': Document(page_content='xiv  Preface  \\nThe text is intended primarily for use in undergrad uate or graduate courses in \\nalgorithms or data structures. Because it discusses  engineering issues in algorithm \\ndesign, as well as mathematical aspects, it is equa lly well suited  for  self-study  by  \\ntechnical professionals. \\nIn this, the fourth edition, we have once again upd ated the entire book. The \\nchanges cover a broad spectrum, including new chapt ers and sections,  color  illus-  \\ntrations,  and  what  we  hope  you’ll  ûnd  to be a more  engaging  writing style. \\nTo  the  teacher  \\nWe have designed this book to be both versatile and  complete. You  should  ûnd  it \\nuseful for a variety of courses, from an undergradu ate course in data structures up \\nthrough a graduate course in algorithms. Because we  have provided considerably \\nmore  material  than  can  ût in a typical  one-term  course,  you  can select the material \\nthat best supports the course you wish', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 15}),\n",
       "  'c8680c62-ab18-4fe3-8d54-17ac2689956e': Document(page_content=' material \\nthat best supports the course you wish to teach. \\nYou  should  ûnd  it easy  to organize  your  course  around  just  the  chapters you \\nneed.  We  have  made  chapters  relatively  self-contained,  so that you need not \\nworry about an unexpected and unnecessary dependenc e of one chapter  on  an-  \\nother. Whereas in an undergraduate course, you migh t use only some sections \\nfrom a chapter, in a graduate course, you might cov er the entire chapter. \\nWe  have  included  931  exercises  and  162  problems.  Each  section  ends  with  exer-  \\ncises, and each chapter ends with problems. The exe rcises are generally  short  ques-  \\ntions that test basic mastery of the material. Some  are simple self-check  thought  \\nexercises, but many are substantial and suitable as  assigned homework.  The  prob-  \\nlems include more elaborate case studies which ofte n introduce new material. They \\noften consist of several parts that lead the studen t', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 15}),\n",
       "  '2a247b84-89c8-47c1-92b0-b5028b6634cc': Document(page_content=' consist of several parts that lead the studen t through the  steps  required  to ar-  \\nrive at a solution. \\nAs with the third edition of this book, we have mad e publicly available solutions \\nto some, but by no means all, of the problems and e xercises. You can  ûnd  these  so-  \\nlutions on our website, http://mitpress.mit.edu/alg orithms/. You will want to check \\nthis site to see whether it contains the solution t o an exercise or problem that you \\nplan to assign. Since the set of solutions that we post might grow over time, we \\nrecommend that you check the site each time you tea ch the course. \\nWe have starred ( ?) the sections and exercises that are more suitable  for graduate \\nstudents than for undergraduates. A starred section  is not necessarily  more  difû-  \\ncult than an unstarred one, but it may require an u nderstanding of more advanced \\nmathematics. Likewise, starred exercises may requir e an advanced background or \\nmore than average creativity. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 15}),\n",
       "  '82f5330b-a348-48e3-a1ab-805918facf49': Document(page_content='Preface  xv \\nTo  the  student  \\nWe hope that this textbook provides you with an enj oyable introduction  to the  ûeld  \\nof algorithms. We have attempted to make every algo rithm accessible  and  inter-  \\nesting.  To  help  you  when  you  encounter  unfamiliar  or difûcul t algorithms, we \\ndescribe  each  one  in a step-by-step  manner.  We  also  provide  careful explanations \\nof the mathematics needed to understand the analysi s of the algorithms  and  sup-  \\nporting  ûgures  to help  you  visualize  what  is going  on.  \\nSince this book is large, your class will probably cover only a portion of its \\nmaterial.  Although  we  hope  that  you  will  ûnd  this  book  helpfu l to you as a course \\ntextbook now, we have also tried to make it compreh ensive enough to warrant space \\non your future professional bookshelf. \\nWhat  are  the  prerequisites ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 16}),\n",
       "  'c571004f-3b32-4353-9998-3793eca38a2c': Document(page_content='\\nWhat  are  the  prerequisites  for  reading  this  book?  \\n\\ue001 You need some programming experience. In particular , you should understand \\nrecursive procedures and simple data structures, su ch as arrays and linked lists \\n(although  Section  10.2  covers  linked  lists  and  a variant  that  you  may  ûnd  new).  \\n\\ue001 You should have some facility with mathematical pro ofs, and especially proofs \\nby mathematical induction. A few portions of the bo ok rely on some knowledge \\nof elementary calculus. Although this book uses mat hematics throughout, Part I \\nand Appendices A–D teach you all the mathematical t echniques you will need. \\nOur  website,  http://mitpress.mit.edu/algorithms/,  links to solutions for some of \\nthe problems and exercises. Feel free to check your  solutions against ours. We ask, \\nhowever, that you not send your solutions to us. \\nTo  the  professional  \\nThe wide range of topics in this book makes it an e xcellent handbook  on  algo- ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 16}),\n",
       "  '770ac736-04b5-46d9-bec0-abd68cefe7e5': Document(page_content='cellent handbook  on  algo-  \\nrithms.  Because  each  chapter  is relatively  self-contained , you can focus on the \\ntopics most relevant to you. \\nSince most of the algorithms we discuss have great practical utility, we address \\nimplementation concerns and other engineering issue s. We often provide practical \\nalternatives to the few algorithms that are primari ly of theoretical interest. \\nIf you  wish  to implement  any  of the  algorithms,  you  should  ûnd  the  transla-  \\ntion of our pseudocode into your favorite programmi ng language to be a fairly \\nstraightforward task. We have designed the pseudoco de to present each algorithm \\nclearly and succinctly. Consequently, we do not add ress error handling and other \\nsoftware-engineering  issues  that  require  speciûc  assumptions  about  your  program-  \\nming environment. We attempt to present each algori thm simply and  directly  with-  \\nout allowing the idiosyncrasies of a particular pro gramming language to obscure its \\nessence. If you are used', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 16}),\n",
       "  'dfed344e-339f-41c9-95cf-1b64ba6c6929': Document(page_content=' its \\nessence. If you are used to 0-origin  arrays,  you  might  ûnd  our  frequent  practice  of ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 16}),\n",
       "  '89c7f482-909d-457b-9c43-5bd285ed6f06': Document(page_content='xvi  Preface  \\nindexing arrays from 1 a minor stumbling block. You can always either subt ract 1 \\nfrom our indices or just overallocate the array and  leave position 0 unused. \\nWe understand that if you are using this book outsi de of a course, then you \\nmight be unable to check your solutions to problems  and exercises against solutions \\nprovided  by  an instructor.  Our  website,  http://mitpress.m it.edu/algorithms/, links \\nto solutions for some of the problems and exercises  so that you can check your \\nwork. Please do not send your solutions to us. \\nTo  our  colleagues  \\nWe have supplied an extensive bibliography and poin ters to the current literature. \\nEach chapter ends with a set of chapter notes that give historical  details  and  ref-  \\nerences. The chapter notes do not provide a complet e reference to the  whole  ûeld  \\nof algorithms, however. Though it may be hard to be lieve for a book of this size, \\nspace constraints prevented us from including many interesting algorithms. \\nDespite myriad requests from', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 17}),\n",
       "  '2042ad09-663f-4f0e-935a-610dcdb4b557': Document(page_content=' many interesting algorithms. \\nDespite myriad requests from students for solutions  to problems and exercises, \\nwe have adopted the policy of not citing references  for them, removing  the  temp-  \\ntation for students to look up a solution rather th an to discover it themselves. \\nChanges  for  the  fourth  edition  \\nAs we said about the changes for the second and thi rd editions, depending on how \\nyou look at it, the book changed either not much or  quite a bit. A quick look at the \\ntable  of contents  shows  that  most  of the  third-edition  chapt ers and sections appear \\nin the fourth edition. We removed three chapters an d several sections, but we have \\nadded three new chapters and several new sections a part from these new chapters. \\nWe  kept  the  hybrid  organization  from  the  ûrst  three  editions . Rather than \\norganizing chapters only by problem domains or only  according to techniques, \\nthis book incorporates elements of both. It contain s technique-based  chapters  on  \\ndivide-and-conquer,  dynamic ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 17}),\n",
       "  'f3b31dfd-a278-45d1-bc06-c3b402800020': Document(page_content='ide-and-conquer,  dynamic  programming,  greedy  algorithms,  amortized  analy-  \\nsis,  augmenting  data  structures,  NP-completeness,  and  approximation algorithms. \\nBut it also has entire parts on sorting, on data st ructures for dynamic sets, and on \\nalgorithms  for  graph  problems.  We  ûnd  that  although  you  need  to know  how  to ap-  \\nply techniques for designing and analyzing algorith ms, problems seldom announce \\nto you which techniques are most amenable to solvin g them. \\nSome of the changes in the fourth edition apply gen erally across the book, and \\nsome  are  speciûc  to particular  chapters  or sections.  Here  is a summary of the most \\nsigniûcant  general  changes:  \\n\\ue001 We  added  140  new  exercises  and  22  new  problems.  We  also  improv ed many of \\nthe old exercises and problems, often as the result  of reader feedback. (Thanks \\nto all readers who made', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 17}),\n",
       "  '9a6e07d9-8447-49d5-b7be-0091312eed5e': Document(page_content='. (Thanks \\nto all readers who made suggestions.) ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 17}),\n",
       "  '849c0323-24d1-45d2-8466-d6965f94a90a': Document(page_content='Preface  xvii \\n\\ue001 We have color! With designers from the MIT Press, w e selected a limited \\npalette, devised to convey information and to be pl easing to the eye. (We are \\ndelighted  to display  red-black  trees  in4get  this4red  and  black!) To enhance \\nreadability,  deûned  terms,  pseudocode  comments,  and  page  numbers  in the  in-  \\ndex are in color. \\n\\ue001 Pseudocode procedures appear on a tan background to  make them easier to spot, \\nand  they  do  not  necessarily  appear  on  the  page  of their  ûrst  reference. When \\nthey  don’t,  the  text  directs  you  to the  relevant  page.  In the  same vein, nonlocal \\nreferences to numbered equations, theorems, lemmas,  and corollaries include \\nthe page number. \\n\\ue001 We removed topics that were rarely taught. We dropp ed in their entirety the \\nchapters on', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 18}),\n",
       "  'd79e592e-006b-4fac-ac54-dfdf12ef6540': Document(page_content=' ed in their entirety the \\nchapters on Fibonacci heaps, van Emde Boas trees, a nd computational  geom-  \\netry. In addition, the following material was excis ed: the maximum-subarray  \\nproblem, implementing pointers and objects, perfect  hashing, randomly built \\nbinary  search  trees,  matroids,  push-relabel  algorithms  for  maximum  üow,  the  \\niterative fast Fourier transform method, the detail s of the simplex algorithm for \\nlinear  programming,  and  integer  factorization.  You  can  ûnd  all the removed \\nmaterial on our website, http://mitpress.mit.edu/al gorithms/. \\n\\ue001 We reviewed the entire book and rewrote sentences, paragraphs, and sections \\nto make the writing clearer, more personal, and gen der neutral. For example, \\nthe  <traveling-salesman  problem=  in the  previous  editions  is now called the \\n<traveling-salesperson  problem.=  We  believe  that  it is critically important for \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 18}),\n",
       "  '72eb65e8-4a60-4264-b3db-1c4d5ca981cb': Document(page_content='  that  it is critically important for \\nengineering  and  science,  including  our  own  ûeld  of computer  science, to be \\nwelcoming to everyone. (The one place that stumped us is in Chapter  13,  which  \\nrequires  a term  for  a parent’s  sibling.  Because  the  English  language has no such \\ngender-neutral  term,  we  regretfully  stuck  with  <uncle.=)  \\n\\ue001 The chapter notes, bibliography, and index were upd ated, reüecting  the  dra-  \\nmatic  growth  of the  ûeld  of algorithms  since  the  third  editio n. \\n\\ue001 We corrected errors, posting most corrections on ou r website of third-edition  \\nerrata. Those that were reported while we were in f ull swing preparing this \\nedition were not posted, but were corrected in this  edition. (Thanks again to all \\nreaders who helped us identify issues.) \\nThe  speciûc  changes  for  the  fourth  edition ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 18}),\n",
       "  '5eb239e3-343f-451c-a608-014850a9affb': Document(page_content=' changes  for  the  fourth  edition  include  the  following: \\n\\ue001 We  renamed  Chapter  3 and  added  a section  giving  an overview  of asymptotic \\nnotation  before  delving  into  the  formal  deûnitions.  \\n\\ue001 Chapter  4 underwent  substantial  changes  to improve  its  mathematical  founda-  \\ntion and make it more robust and intuitive. The not ion of an algorithmic  re-  \\ncurrence  was  introduced,  and  the  topic  of ignoring  üoors  and  ceilings  in recur-  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 18}),\n",
       "  '1e8f6ebd-138c-4fae-91f9-5be14ba45323': Document(page_content='xviii  Preface  \\nrences was addressed more rigorously. The second ca se of the master theorem \\nincorporates polylogarithmic factors, and a rigorou s proof of a <continuous= \\nversion of the master theorem is now provided. We a lso present the powerful \\nand  general  Akra-Bazzi  method  (without  proof).  \\n\\ue001 The  deterministic  order-statistic  algorithm  in Chapter  9 is slightly different, and \\nthe analyses of both the randomized and determinist ic order-statistic  algorithms  \\nhave been revamped. \\n\\ue001 In addition  to stacks  and  queues,  Section  10.1  discusses  ways to store arrays \\nand matrices. \\n\\ue001 Chapter  11  on  hash  tables  includes  a modern  treatment  of hash  functions. It \\nalso  emphasizes  linear  probing  as an efûcient  method  for  resolving collisions \\nwhen the underlying hardware implements caching to favor local searches. \\n\\ue001 To  replace  the  sections  on  matroids  in Chapter  15', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 19}),\n",
       "  '5e5a808f-4d1e-4603-b93a-c312621d8248': Document(page_content=' on  matroids  in Chapter  15,  we  convert ed a problem in \\nthe  third  edition  about  ofüine  caching  into  a full  section.  \\n\\ue001 Section  16.4  now  contains  a more  intuitive  explanation  of the  potential  func-  \\ntions to analyze table doubling and halving. \\n\\ue001 Chapter  17  on  augmenting  data  structures  was  relocated  from  Part III to Part V, \\nreüecting  our  view  that  this  technique  goes  beyond  basic  material. \\n\\ue001 Chapter  25  is a new  chapter  about  matchings  in bipartite  graphs. It presents \\nalgorithms  to ûnd  a matching  of maximum  cardinality,  to solve  the  stable-  \\nmarriage  problem,  and  to ûnd  a maximum-weight  matching  (known  as the  <as-  \\nsignment problem=). \\n\\ue001 Chapter  26,  on  task-parallel  computing,  has  been  updated', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 19}),\n",
       "  'c947513f-5bc3-45eb-a349-02452ee04e78': Document(page_content='allel  computing,  has  been  updated  with  modern  termi-  \\nnology, including the name of the chapter. \\n\\ue001 Chapter  27,  which  covers  online  algorithms,  is another  new  chapter. In an \\nonline algorithm, the input arrives over time, rath er than being available in its \\nentirety at the start of the algorithm. The chapter  describes several examples \\nof online algorithms, including determining how lon g to wait for an elevator \\nbefore taking the stairs, maintaining a linked list  via the move-to-front  heuristic,  \\nand evaluating replacement policies for caches. \\n\\ue001 In Chapter 29, we removed the detailed presentation  of the simplex algorithm, \\nas it was math heavy without really conveying many algorithmic ideas. The \\nchapter now focuses on the key aspect of how to mod el problems as linear \\nprograms, along with the essential duality property  of linear programming. \\n\\ue001 Section  32.5  adds  to the  chapter  on  string  matching  the  simpl e, yet powerful, \\nstructure  of sufû', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 19}),\n",
       "  '5c01cb22-fe08-454d-9805-ee22d9a5fbf2': Document(page_content=', \\nstructure  of sufûx  arrays.  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 19}),\n",
       "  'a2de3c17-b565-4149-9340-21ee12451543': Document(page_content='Preface  xix \\n\\ue001 Chapter  33,  on  machine  learning,  is the  third  new  chapter.  It introduces  sev-  \\neral basic methods used in machine learning: cluste ring to group similar items \\ntogether,  weighted-majority  algorithms,  and  gradient  descent  to ûnd  the  mini-  \\nmizer of a function. \\n\\ue001 Section  34.5.6  summarizes  strategies  for  polynomial-time  reductions to show \\nthat  problems  are  NP-hard.  \\n\\ue001 The  proof  of the  approximation  algorithm  for  the  set-covering  problem  in Sec-  \\ntion  35.3  has  been  revised.  \\nWebsite  \\nYou can use our website, http://mitpress.mit.edu/al gorithms/,  to obtain  supplemen-  \\ntary information and to communicate with us. The we bsite links to a list of known \\nerrors, material from the third edition that is not  included in the fourth edition, \\nsolutions to', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 20}),\n",
       "  'a8d73f6e-8ace-4620-a0b2-4135eaa7a8a0': Document(page_content=' in the fourth edition, \\nsolutions to selected exercises and problems, Pytho n implementations of many of \\nthe algorithms in this book, a list explaining the corny professor jokes (of course), \\nas well as other content, which we may add to. The website also tells you how to \\nreport errors or make suggestions. \\nHow  we  produced  this  book  \\nLike the previous three editions, the fourth editio n was produced in L A T E X 2 \" . We \\nused the Times font with mathematics typeset using the MathTime Professional II \\nfonts. As in all previous editions, we compiled the  index using  Windex,  a C pro-  \\ngram that we wrote, and produced the bibliography u sing B IBT E X.  The  PDF  ûles  \\nfor  this  book  were  created  on  a MacBook  Pro  running  macOS  10.14.  \\nOur  plea  to Apple  in the  preface  of the  third  edition  to update  MacDraw Pro for \\nmacOS  10  went  for  naught,  and  so we', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 20}),\n",
       "  '8d99b90a-b307-44d8-891d-5f2bbec3ee3c': Document(page_content=' for  naught,  and  so we  continued  to draw  illustrations  on  pre-Intel  \\nMacs running MacDraw Pro under the Classic environm ent of older versions of \\nmacOS  10.  Many  of the  mathematical  expressions  appearing  in illustrations were \\nlaid in with the psfrag package for L A T E X 2 \" . \\nAcknowledgments  for  the  fourth  edition  \\nWe have been working with the MIT Press since we st arted writing  the  ûrst  edi-  \\ntion  in 1987,  collaborating  with  several  directors,  editor s, and production staff. \\nThroughout our association with the MIT Press, thei r support has  always  been  out-  \\nstanding. Special thanks to our editors Marie Lee, who put up with us for far too \\nlong,  and  Elizabeth  Swayze,  who  pushed  us over  the  ûnish  line. Thanks also to \\nDirector Amy Brand and to Alex Hoopes. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 20}),\n",
       "  '60996a7b-0eeb-41a8-acc5-6846055ad19f': Document(page_content='xx Preface  \\nAs in the third edition, we were geographically dis tributed while producing \\nthe fourth edition, working in the Dartmouth Colleg e Department of Computer \\nScience;  the  MIT  Computer  Science  and  Artiûcial  Intelligen ce Laboratory and \\nthe MIT Department of Electrical Engineering and Co mputer Science; and the \\nColumbia University Department of Industrial Engine ering and  Operations  Re-  \\nsearch, Department of Computer Science, and Data Sc ience Institute. During the \\nCOVID-19  pandemic,  we  worked  largely  from  home.  We  thank  our  respective \\nuniversities and colleagues for providing such supp ortive and  stimulating  environ-  \\nments. As we complete this book, those of us who ar e not retired are eager to return \\nto our respective universities now that the pandemi c seems to be abating. \\nJulie  Sussman,  P.P.A.,  came  to our  rescue  once  again  with  her  technical  copy-  \\nediting  under  tremendous  time  pressure.  If', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 21}),\n",
       "  '03aad2f2-fb04-4313-b9c2-76718bf7ecdb': Document(page_content=' under  tremendous  time  pressure.  If not  for  Julie,  this book would be riddled \\nwith  errors  (or,  let’s  say,  many  more  errors  than  it has)  and  would  be far  less  read-  \\nable.  Julie,  we  will  be forever  indebted  to you.  Errors  that  remain  are  the  responsi-  \\nbility  of the  authors  (and  probably  were  inserted  after  Julie read the material). \\nDozens of errors in previous editions were correcte d in the process of creating \\nthis  edition.  We  thank  our  readers4too  many  to list  them  all4 who have reported \\nerrors and suggested improvements over the years. \\nWe received considerable help in preparing some of the new material in this \\nedition.  Neville  Campbell  (unafûliated),  Bill  Kuszmaul  of MIT, and Chee Yap of \\nNYU provided valuable advice regarding the treatmen t of recurrences  in Chapter  4. \\nYan ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 21}),\n",
       "  'af257621-4a77-483f-a9c9-7493a52acbe4': Document(page_content='  in Chapter  4. \\nYan  Gu  of the  University  of California,  Riverside,  provided  feedback on parallel \\nalgorithms  in Chapter  26.  Rob  Shapire  of Microsoft  Research  altered our approach \\nto the material on machine learning with his detail ed comments on  Chapter  33.  Qi  \\nQi  of MIT  helped  with  the  analysis  of the  Monty  Hall  problem  (Problem  C-1).  \\nMolly Seaman and Mary Reilly of the MIT Press helpe d us select the color \\npalette  in the  illustrations,  and  Wojciech  Jarosz  of Dartmo uth College suggested \\ndesign  improvements  to our  newly  colored  ûgures.  Yichen  (Annie)  Ke  and  Linda  \\nXiao, who have since graduated from Dartmouth, aide d in colorizing  the  illus-  \\ntrations, and Linda also produced many of the Pytho n implementations that are \\navailable  on  the  book’s  website.  \\nFinally, ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 21}),\n",
       "  '99c81d35-5db1-456a-9750-e9e47fbbf5c7': Document(page_content='s  website.  \\nFinally,  we  thank  our  wives4Wendy  Leiserson,  Gail  Rivest,  Rebecca Ivry, and \\nthe  late  Nicole  Cormen4and  our  families.  The  patience  and  encouragement of \\nthose who love us made this project possible. We af fectionately dedicate this book \\nto them. \\nTHOMAS  H. CORMEN  Lebanon, New Hampshire \\nCHARLES E. LEISERSON  Cambridge,  Massachusetts  \\nRONALD  L. R IVEST Cambridge,  Massachusetts  \\nCLIFFORD  STEIN New York, New York \\nJune,  2021  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 21}),\n",
       "  'b678792c-def0-4316-b377-bb2288aeefc3': Document(page_content='Part  I Foundations  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 23}),\n",
       "  'adc0f2a3-cd75-48b7-997a-32fae3293fe8': Document(page_content='Introduction  \\nWhen you design and analyze algorithms, you need to  be able to describe how they \\noperate and how to design them. You also need some mathematical tools to show \\nthat  your  algorithms  do  the  right  thing  and  do  it efûciently.  This part will get you \\nstarted. Later parts of this book will build upon t his base. \\nChapter  1 provides  an overview  of algorithms  and  their  place  in modern  com-  \\nputing  systems.  This  chapter  deûnes  what  an algorithm  is and  lists some examples. \\nIt also makes a case for considering algorithms as a technology,  alongside  tech-  \\nnologies such as fast hardware, graphical user inte rfaces, object-oriented  systems,  \\nand networks. \\nIn Chapter  2, we  see  our  ûrst  algorithms,  which  solve  the  problem of sorting \\na sequence of n numbers. They are written in a pseudocode which, al though not \\ndirectly translatable to any conventional programmi ng language,  conveys', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 24}),\n",
       "  '998764e6-3061-47c8-a99b-57236d829dbe': Document(page_content=' any conventional programmi ng language,  conveys  the  struc-  \\nture of the algorithm clearly enough that you shoul d be able to implement it in the \\nlanguage of your choice. The sorting algorithms we examine are insertion sort, \\nwhich uses an incremental approach, and merge sort,  which uses a recursive  tech-  \\nnique  known  as <divide-and-conquer.=  Although  the  time  each requires increases \\nwith the value of n, the rate of increase differs between the two algo rithms. We \\ndetermine these running times in Chapter 2, and we develop a useful <asymptotic= \\nnotation to express them. \\nChapter  3 precisely  deûnes  asymptotic  notation.  We’ll  use  asymptotic notation \\nto bound  the  growth  of functions4most  often,  functions  that  describe the running \\ntime  of algorithms4from  above  and  below.  The  chapter  starts  by  informally  deûn-  \\ning the most commonly used asymptotic notations', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 24}),\n",
       "  '2b6865a4-dae5-4e7a-9726-ffb46e26e9d5': Document(page_content=' the most commonly used asymptotic notations and  giving an example of how to \\napply  them.  It then  formally  deûnes  ûve  asymptotic  notations  and  presents  conven-  \\ntions  for  how  to put  them  together.  The  rest  of Chapter  3 is primarily a presentation \\nof mathematical notation, more to ensure that your use of notation matches that in \\nthis book than to teach you new mathematical concep ts. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 24}),\n",
       "  '47fbedd2-863e-4499-8a2d-4915c61232c2': Document(page_content='4 Part  I Foundations  \\nChapter  4 delves  further  into  the  divide-and-conquer  metho d introduced in \\nChapter  2. It provides  two  additional  examples  of divide-and-conquer  algorithms  \\nfor  multiplying  square  matrices,  including  Strassen’s  surprising  method.  Chapter  4 \\ncontains methods for solving recurrences, which are  useful for  describing  the  run-  \\nning times of recursive algorithms. In the substitu tion method, you guess an answer \\nand prove it correct. Recursion trees provide one w ay to generate  a guess.  Chap-  \\nter  4 also  presents  the  powerful  technique  of the  <master  method,= which you can \\noften  use  to solve  recurrences  that  arise  from  divide-and-conquer  algorithms.  Al-  \\nthough the chapter provides a proof of a foundation al theorem on which the master \\ntheorem depends, you should feel free to employ the  master method  without  delv-  \\ning', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 25}),\n",
       "  '1f9bd6df-88ed-43d9-83af-28db6d3e3d2a': Document(page_content='  without  delv-  \\ning  into  the  proof.  Chapter  4 concludes  with  some  advanced  topics. \\nChapter  5 introduces  probabilistic  analysis  and  randomize d algorithms. You \\ntypically use probabilistic analysis to determine t he running time of an algorithm \\nin cases in which, due to the presence of an inhere nt probability distribution, the \\nrunning time may differ on different inputs of the same size. In some cases, you \\nmight assume that the inputs conform to a known pro bability distribution, so that \\nyou are averaging the running time over all possibl e inputs. In other cases, the \\nprobability distribution comes not from the inputs but from random choices made \\nduring the course of the algorithm. An algorithm wh ose behavior is determined \\nnot  only  by  its  input  but  by  the  values  produced  by  a random-nu mber generator is a \\nrandomized algorithm. You can use randomized algori thms to enforce a probability \\ndistribution  on  the  inputs4thereby  ensuring  that  no  partic ular input', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 25}),\n",
       "  'df64cb7a-788e-4e74-a6a0-e53054b0169e': Document(page_content=' ensuring  that  no  partic ular input always causes \\npoor  performance4or  even  to bound  the  error  rate  of algorith ms that are allowed \\nto produce incorrect results on a limited basis. \\nAppendices A–D contain other mathematical material that you will  ûnd  helpful  \\nas you read this book. You might have seen much of the material in the appendix \\nchapters  before  having  read  this  book  (although  the  speciûc  deûnitions  and  nota-  \\ntional conventions we use may differ in some cases from what you have seen in \\nthe past), and so you should think of the appendice s as reference  material.  On  the  \\nother hand, you probably have not already seen most  of the material in Part I. All \\nthe chapters in Part I and the appendices are writt en with a tutorial  üavor.  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 25}),\n",
       "  '9addf919-c32b-4b6c-ad2a-5566616a9fc7': Document(page_content='1 The  Role  of Algorithms  in Computing  \\nWhat  are  algorithms?  Why  is the  study  of algorithms  worthwhile?  What  is the  role  \\nof algorithms relative to other technologies used i n computers?  This  chapter  will  \\nanswer these questions. \\n1.1  Algorithms  \\nInformally, an algorithm  is any  well-deûned  computational  procedure  that  takes  \\nsome value, or set of values, as input  and produces some value, or set of values, as \\noutput  in a ûnite  amount  of time.  An  algorithm  is thus  a sequence  of computational \\nsteps that transform the input into the output. \\nYou  can  also  view  an algorithm  as a tool  for  solving  a well-speciûed  computa-  \\ntional  problem. The  statement  of the  problem  speciûes  in general  terms  the  desired \\ninput/output relationship for problem instances, ty pically of arbitrarily large size. \\nThe  algorithm  describes ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 26}),\n",
       "  'aa04e42c-5bee-4fda-855d-b8ff8cc2d564': Document(page_content=' size. \\nThe  algorithm  describes  a speciûc  computational  procedure  for  achieving  that  in-  \\nput/output relationship for all problem instances. \\nAs an example, suppose that you need to sort a sequ ence of numb ers  into  mono-  \\ntonically increasing order. This problem arises fre quently in practice and provides \\nfertile ground for introducing many standard design  techniques and analysis tools. \\nHere  is how  we  formally  deûne  the  sorting  problem : \\nInput:  A sequence of n numbers ha 1 ;a  2 ;:::;a  n i. \\nOutput:  A permutation (reordering) ha 0 \\n1 ;a  0 \\n2 ;:::;a  0 \\nn i of the input sequence such \\nthat a 0 \\n1 හ a 0 \\n2 හ \\ue001 \\ue001 \\ue001 හ  a 0 \\nn . \\nThus, given the input sequence h31;41;59;26;41;58 i, a correct sorting algorithm \\nreturn', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 26}),\n",
       "  'b4206a55-fd96-465e-9be0-bc5ff0c00ddf': Document(page_content='58 i, a correct sorting algorithm \\nreturns as output the sequence h26;31;41;41;58;59 i. Such an input sequence is ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 26}),\n",
       "  '97b725a5-427d-4844-8644-148c38418e29': Document(page_content='6 Chapter  1 The  Role  of Algorithms  in Computing  \\ncalled an instance  of the sorting problem. In general, an instance  of a problem  1 \\nconsists of the input (satisfying whatever constrai nts are imposed in the problem \\nstatement) needed to compute a solution to the prob lem. \\nBecause many programs use it as an intermediate ste p, sorting is a fundamental \\noperation in computer science. As a result, you hav e a large number  of good  sort-  \\ning algorithms at your disposal. Which algorithm is  best for a given application \\ndepends  on4among  other  factors4the  number  of items  to be sorted, the extent \\nto which the items are already somewhat sorted, pos sible restrictions on the item \\nvalues, the architecture of the computer, and the k ind of storage devices to be used: \\nmain  memory,  disks,  or even4archaically4tapes.  \\nAn algorithm for a computational problem is correct  if, for  every  problem  in-  \\nstance provided as input, it halts4ûn', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 27}),\n",
       "  '2b2b77fe-1903-40a0-a405-9305498e5183': Document(page_content=' provided as input, it halts4ûnishes  its  computing  in ûnite  time4and  out-  \\nputs the correct solution to the problem instance. A correct algorithm solves  the \\ngiven computational problem. An incorrect algorithm  might not halt at all on some \\ninput instances, or it might halt with an incorrect  answer. Contrary to what you \\nmight expect, incorrect algorithms can sometimes be  useful, if you can control \\ntheir  error  rate.  We’ll  see  an example  of an algorithm  with  a controllable error rate \\nin Chapter  31  when  we  study  algorithms  for  ûnding  large  prime  numbers.  Ordi-  \\nnarily,  however,  we’ll  concern  ourselves  only  with  correct  algorithms. \\nAn  algorithm  can  be speciûed  in English,  as a computer  progra m, or even as \\na hardware  design.  The  only  requirement  is that  the  speciûca tion must provide a \\nprecise description of the computational procedure to', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 27}),\n",
       "  'e830b5da-4057-453f-aa3a-5a4a37fae090': Document(page_content=' \\nprecise description of the computational procedure to be followed. \\nWhat  kinds  of problems  are  solved  by  algorithms?  \\nSorting is by no means the only computational probl em for which algorithms have \\nbeen developed. (You probably suspected as much whe n you saw the size of this \\nbook.) Practical applications of algorithms are ubi quitous and  include  the  follow-  \\ning examples: \\n\\ue001 The  Human  Genome  Project  has  made  great  progress  toward  the  goals  of iden-  \\ntifying  all  the  roughly  30,000  genes  in human  DNA,  determini ng the sequences \\nof the roughly 3 billion  chemical  base  pairs  that  make  up  human  DNA,  stor-  \\ning this information in databases, and developing t ools for data analysis. Each \\nof these steps requires sophisticated algorithms. A lthough the solutions to the \\nvarious problems involved are beyond the scope of t his book, many methods to \\nsolve these biological problems use ideas presented  here, enabling scientists to \\nac', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 27}),\n",
       "  'ed766e07-f25f-4362-810b-4e021587b13d': Document(page_content=' presented  here, enabling scientists to \\naccomplish  tasks  while  using  resources  efûciently.  Dynami c programming, as \\n1 Sometimes, when the problem context is known, probl em instances are themselves simply called \\n<problems.= ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 27}),\n",
       "  'c3fb6b75-393a-4fe8-9be8-bd2161c589b8': Document(page_content='1.1 Algorithms 7 \\nin Chapter  14,  is an important  technique  for  solving  several  of these biological \\nproblems, particularly ones that involve determinin g similarity between DNA \\nsequences. The savings realized are in time, both h uman and machine, and in \\nmoney, as more information can be extracted by labo ratory techniques. \\n\\ue001 The internet enables people all around the world to  quickly access and retrieve \\nlarge amounts of information. With the aid of cleve r algorithms, sites on the \\ninternet are able to manage and manipulate this lar ge volume of data.  Exam-  \\nples of problems that make essential use of algorit hms include ûnding  good  \\nroutes on which the data travels (techniques for so lving such problems appear \\nin Chapter  22),  and  using  a search  engine  to quickly  ûnd  pages  on  which  par-  \\nticular information resides (related techniques are  in Chapters  11  and  32).  \\n\\ue001 Electronic commerce enables goods and services to b e negotiated  and  ex-', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 28}),\n",
       "  '721ceaec-c2aa-436d-be3b-823179526a5d': Document(page_content=' services to b e negotiated  and  ex-  \\nchanged electronically, and it depends on the priva cy of personal  informa-  \\ntion such as credit card numbers, passwords, and ba nk statements. The core \\ntechnologies  used  in electronic  commerce  include  public-k ey cryptography and \\ndigital  signatures  (covered  in Chapter  31),  which  are  based  on  numerical  algo-  \\nrithms and number theory. \\n\\ue001 Manufacturing and other commercial enterprises ofte n need to allocate scarce \\nresources  in the  most  beneûcial  way.  An  oil  company  might  wish to know \\nwhere to place its wells in order to maximize its e xpected proût.  A political  \\ncandidate might want to determine where to spend mo ney buying campaign  ad-  \\nvertising in order to maximize the chances of winni ng an election. An airline \\nmight  wish  to assign  crews  to üights  in the  least  expensive  way  possible,  mak-  \\ning  sure  that  each  ü', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 28}),\n",
       "  'fdb491c4-0ace-44d4-aa31-7e7ea2066a07': Document(page_content='ing  sure  that  each  üight  is covered  and  that  government  regul ations regarding \\ncrew scheduling are met. An internet service provid er might wish to determine \\nwhere to place additional resources in order to ser ve its customers  more  effec-  \\ntively. All of these are examples of problems that can be solved by modeling \\nthem as linear programs, which Chapter 29 explores.  \\nAlthough some of the details of these examples are beyond the scope of this \\nbook, we do give underlying techniques that apply t o these problems and problem \\nareas.  We  also  show  how  to solve  many  speciûc  problems,  inclu ding the following: \\n\\ue001 You have a road map on which the distance between e ach pair of a djacent  in-  \\ntersections is marked, and you wish to determine th e shortest route from one \\nintersection to another. The number of possible rou tes can be huge, even if you \\ndisallow routes that cross over themselves. How can  you choose which of all \\npossible  routes  is the  shortest? ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 28}),\n",
       "  'f07d8bee-b73a-407e-827c-e270ee99dfd2': Document(page_content='ossible  routes  is the  shortest?  You  can  start  by  modeling  the road map (which \\nis itself a model of the actual roads) as a graph ( which we will meet in Part VI \\nand  Appendix  B).  In this  graph,  you  wish  to ûnd  the  shortest  path from one \\nvertex to another. Chapter 22 shows how to solve th is problem efûciently.  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 28}),\n",
       "  '1f362308-572c-40ae-aee0-8a5aaeeb0594': Document(page_content='8 Chapter  1 The  Role  of Algorithms  in Computing  \\n\\ue001 Given  a mechanical  design  in terms  of a library  of parts,  wher e each part may \\ninclude instances of other parts, list the parts in  order so that each part appears \\nbefore any part that uses it. If the design compris es n parts, then there are nŠ \\npossible orders, where nŠ denotes the factorial function. Because the factori al \\nfunction grows faster than even an exponential func tion, you cannot feasibly \\ngenerate each possible order and then verify that, within that order, each part \\nappears before the parts using it (unless you have only a few p arts).  This  prob-  \\nlem is an instance of topological sorting, and Chap ter 20 shows how to solve \\nthis  problem  efûciently.  \\n\\ue001 A doctor needs to determine whether an image repres ents a cancerous tumor or \\na benign one. The doctor has available images of ma ny other tumors, some of \\nwhich are known to be cancerous and some of which a re known', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 29}),\n",
       "  '02f89e36-9da7-446e-b881-cf72f3aaa252': Document(page_content=' be cancerous and some of which a re known to be benign. \\nA cancerous tumor is likely to be more similar to o ther cancerous tumors than \\nto benign tumors, and a benign tumor is more likely  to be similar to other  be-  \\nnign  tumors.  By  using  a clustering  algorithm,  as in Chapter  33,  the  doctor  can  \\nidentify which outcome is more likely. \\n\\ue001 You  need  to compress  a large  ûle  containing  text  so that  it occupies less space. \\nMany ways to do so are known, including <LZW compre ssion,= which looks for \\nrepeating  character  sequences.  Chapter  15  studies  a different  approach,  <Huff-  \\nman coding,= which encodes characters by bit sequen ces of various lengths, \\nwith characters occurring more frequently encoded b y shorter bit sequences. \\nThese lists are far from exhaustive (as you again h ave probably surmised from \\nthis  book’s  heft),  but  they  exhibit  two  characteristics ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 29}),\n",
       "  'bae2c31f-91ed-4964-82aa-f53c4c29d0d5': Document(page_content=' but  they  exhibit  two  characteristics  common to many interesting \\nalgorithmic problems: \\n1. They  have  many  candidate  solutions,  the  overwhelming  majority of which do \\nnot solve the problem at hand. Finding one that doe s, or one that is <best,=  with-  \\nout explicitly examining each possible solution, ca n present quite a challenge. \\n2. They  have  practical  applications.  Of  the  problems  in the  above  list,  ûnding  the  \\nshortest path provides the easiest examples. A tran sportation  ûrm,  such  as a \\ntrucking  or railroad  company,  has  a ûnancial  interest  in ûnding shortest paths \\nthrough a road or rail network because taking short er paths results in lower \\nlabor  and  fuel  costs.  Or  a routing  node  on  the  internet  might  need  to ûnd  the  \\nshortest path through the network in order to route  a message quickly.  Or  a \\nperson  wishing  to', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 29}),\n",
       "  'cbe197b7-87b2-4cc0-b6d8-375e71021428': Document(page_content=' Or  a \\nperson  wishing  to drive  from  New  York  to Boston  might  want  to ûnd driving \\ndirections using a navigation app. \\nNot  every  problem  solved  by  algorithms  has  an easily  identiûed  set  of candi-  \\ndate solutions. For example, given a set of numeric al values representing samples \\nof a signal taken at regular time intervals, the di screte Fourier transform converts ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 29}),\n",
       "  'a89f4cd4-3080-416e-a124-16cb28ed963f': Document(page_content='1.1 Algorithms 9 \\nthe time domain to the frequency domain. That is, i t approximates the signal as a \\nweighted sum of sinusoids, producing the strength o f various frequencies which, \\nwhen summed, approximate the sampled signal. In add ition to lying at the heart of \\nsignal processing, discrete Fourier transforms have  applications  in data  compres-  \\nsion and multiplying large polynomials and integers . Chapter 30  gives  an efûcient  \\nalgorithm, the fast Fourier transform (commonly cal led the FFT), for this problem. \\nThe chapter also sketches out the design of a hardw are FFT circuit. \\nData  structures  \\nThis book also presents several data structures. A data  structure  is a way to store \\nand  organize  data  in order  to facilitate  access  and  modiûcations.  Using  the  appro-  \\npriate data structure or structures is an important  part of algorithm  design.  No  sin-  \\ngle data structure works well for all purposes, and  so you should know the strengths', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 30}),\n",
       "  '588e5339-97c7-4d52-afc8-9ca34e79cf8a': Document(page_content=' purposes, and  so you should know the strengths \\nand limitations of several of them. \\nTechnique  \\nAlthough you can use this book as a <cookbook= for algorithms, you  might  some-  \\nday  encounter  a problem  for  which  you  cannot  readily  ûnd  a published algorithm \\n(many of the exercises and problems in this book, f or example). This book will \\nteach you techniques of algorithm design and analys is so that you  can  develop  al-  \\ngorithms on your own, show that they give the corre ct answer, and  analyze  their  ef-  \\nûciency.  Different  chapters  address  different  aspects  of algorithmic  problem  solv-  \\ning.  Some  chapters  address  speciûc  problems,  such  as ûnding  medians and order \\nstatistics in Chapter 9, computing minimum spanning  trees in Chapter  21,  and  de-  \\ntermining  a maximum  üow  in a network  in Chapter  24.  Other  chap ters introduce', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 30}),\n",
       "  'f9560201-bb06-4e68-bfce-22802b74b02c': Document(page_content='  24.  Other  chap ters introduce \\ntechniques,  such  as divide-and-conquer  in Chapters  2 and  4, dynamic programming \\nin Chapter  14,  and  amortized  analysis  in Chapter  16.  \\nHard  problems  \\nMost  of this  book  is about  efûcient  algorithms.  Our  usual  measure  of efûciency  \\nis speed: how long does an algorithm take to produc e its result? There  are  some  \\nproblems, however, for which we know of no algorith m that runs in a reasonable \\namount  of time.  Chapter  34  studies  an interesting  subset  of these problems, which \\nare  known  as NP-complete.  \\nWhy  are  NP-complete  problems  interesting?  First,  although  no  efûcient  algo-  \\nrithm  for  an NP-complete  problem  has  ever  been  found,  nobody  has ever proven \\nthat  an efûcient  algorithm  for  one  cannot  exist', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 30}),\n",
       "  '73e06388-0a01-4645-bd75-72e7e17318c8': Document(page_content='  algorithm  for  one  cannot  exist.  In other  words, no one knows \\nwhether  efûcient  algorithms  exist  for  NP-complete  problem s. Second, the set of ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 30}),\n",
       "  '8cf8c56c-b695-49a4-823a-5724a745dc41': Document(page_content='10  Chapter  1 The  Role  of Algorithms  in Computing  \\nNP-complete  problems  has  the  remarkable  property  that  if an efûcient  algorithm  \\nexists  for  any  one  of them,  then  efûcient  algorithms  exist  for  all  of them.  This  re-  \\nlationship  among  the  NP-complete  problems  makes  the  lack  of efûcient  solutions  \\nall  the  more  tantalizing.  Third,  several  NP-complete  probl ems are similar, but not \\nidentical,  to problems  for  which  we  do  know  of efûcient  algor ithms. Computer \\nscientists are intrigued by how a small change to t he problem statement can cause \\na big  change  to the  efûciency  of the  best  known  algorithm.  \\nYou  should  know  about  NP-complete  problems  because  some  of them  arise  sur-  \\nprisingly often in real applications. If you', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 31}),\n",
       "  'a9d70ec9-30f8-47d0-a7dd-92047f7fbb97': Document(page_content=' \\nprisingly often in real applications. If you are ca lled upon to produce  an efûcient  \\nalgorithm  for  an NP-complete  problem,  you  are  likely  to spend a lot of time in a \\nfruitless search. If, instead, you can show that th e problem is NP-complete,  you  \\ncan  spend  your  time  developing  an efûcient  approximation  algorithm, that is, an \\nalgorithm that gives a good, but not necessarily th e best possible, solution. \\nAs a concrete example, consider a delivery company with a central depot. Each \\nday, it loads up delivery trucks at the depot and s ends them around to deliver goods \\nto several addresses. At the end of the day, each t ruck must end up back at the depot \\nso that it is ready to be loaded for the next day. To reduce costs, the company wants \\nto select an order of delivery stops that yields th e lowest overall distance traveled by \\neach  truck.  This  problem  is the  well-known  <traveling-sale sperson problem,= and it \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 31}),\n",
       "  'a314a302-5014-4863-a6e7-73ab3cd39ff3': Document(page_content='sale sperson problem,= and it \\nis NP-complete.  2 It has  no  known  efûcient  algorithm.  Under  certain  assumptio ns, \\nhowever,  we  know  of efûcient  algorithms  that  compute  overal l distances close to \\nthe  smallest  possible.  Chapter  35  discusses  such  <approxim ation algorithms.= \\nAlternative  computing  models  \\nFor many years, we could count on processor clock s peeds increasing at a steady \\nrate. Physical limitations present a fundamental ro adblock to ever-increasing  clock  \\nspeeds, however: because power density increases su perlinearly with clock speed, \\nchips run the risk of melting once their clock spee ds become h igh  enough.  In or-  \\nder to perform more computations per second, theref ore, chips are being designed \\nto contain not just one but several processing <cor es.= We can liken  these  multi-  \\ncore computers to several sequential computers on a  single chip. In other words, \\nthey are a type', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 31}),\n",
       "  '38ea4195-65bc-41d4-aa71-f8abd45e120a': Document(page_content=' In other words, \\nthey are a type of <parallel computer.= In order to  elicit the best performance \\nfrom multicore computers, we need to design algorit hms with parallelism in mind. \\nChapter  26  presents  a model  for  =task-parallel=  algorithms , which take advantage \\nof multiple processing cores. This model has advant ages from both theoretical and \\n2 To  be precise,  only  decision  problems4those  with  a <yes/no=  answer4can  be NP-complete.  The  \\ndecision version of the traveling salesperson probl em asks whether there exists an order of stops \\nwhose distance totals at most a given amount. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 31}),\n",
       "  'bbadc457-946f-4585-86ca-2b54b58bc875': Document(page_content='1.1 Algorithms 11 \\npractical  standpoints,  and  many  modern  parallel-programm ing platforms embrace \\nsomething similar to this model of parallelism. \\nMost of the examples in this book assume that all o f the input data are available \\nwhen an algorithm begins running. Much of the work in algorithm design makes \\nthe  same  assumption.  For  many  important  real-world  example s, however, the input \\nactually arrives over time, and the algorithm must decide how to proceed without \\nknowing what data will arrive in the future. In a d ata center, jobs are constantly \\narriving and departing, and a scheduling algorithm must decide when and where to \\nrun a job, without knowing what jobs will be arrivi ng in the future.  Trafûc  must  \\nbe routed in the internet based on the current stat e, without knowing about where \\ntrafûc  will  arrive  in the  future.  Hospital  emergency  rooms  make triage decisions \\nabout  which  patients  to treat  ûrst  without  knowing  when  other patients will be ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 32}),\n",
       "  '184a6ffb-1508-410e-8a22-93eb314e1526': Document(page_content='  knowing  when  other patients will be \\narriving in the future and what treatments they wil l need. Algorithms that receive \\ntheir input over time, rather than having all the i nput present at the start, are online  \\nalgorithms, which  Chapter  27  examines.  \\nExercises  \\n1.1-1  \\nDescribe  your  own  real-world  example  that  requires  sorting . Describe one that \\nrequires  ûnding  the  shortest  distance  between  two  points.  \\n1.1-2  \\nOther  than  speed,  what  other  measures  of efûciency  might  you  need to consider in \\na real-world  setting?  \\n1.1-3  \\nSelect a data structure that you have seen, and dis cuss its strengths and limitations. \\n1.1-4  \\nHow  are  the  shortest-path  and  traveling-salesperson  problems  given  above  similar?  \\nHow  are  they  different?  \\n1', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 32}),\n",
       "  '7dfe04a3-e7f4-4307-a01a-eac57e417662': Document(page_content=' are  they  different?  \\n1.1-5  \\nSuggest  a real-world  problem  in which  only  the  best  solution  will do. Then come \\nup with one in which <approximately= the best solut ion is good enough. \\n1.1-6  \\nDescribe  a real-world  problem  in which  sometimes  the  entire  input is available \\nbefore you need to solve the problem, but other tim es the input is not entirely \\navailable in advance and arrives over time. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 32}),\n",
       "  '1a9b9fd9-640e-4666-addf-a9f9816a2380': Document(page_content='12  Chapter  1 The  Role  of Algorithms  in Computing  \\n1.2  Algorithms  as a technology  \\nIf computers  were  inûnitely  fast  and  computer  memory  were  free, would you have \\nany  reason  to study  algorithms?  The  answer  is yes,  if for  no  other reason than that \\nyou would still like to be certain that your soluti on method terminates and does so \\nwith the correct answer. \\nIf computers  were  inûnitely  fast,  any  correct  method  for  solving a problem \\nwould do. You would probably want your implementati on to be within the bounds \\nof good software engineering practice (for example,  your implementation should \\nbe well designed and documented), but you would mos t often use whichever \\nmethod was the easiest to implement. \\nOf  course,  computers  may  be fast,  but  they  are  not  inûnitely  fast. Computing \\ntime is therefore a bounded resource, which makes i t precious. Although the saying \\ngoes, <Time is money', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 33}),\n",
       "  '43f882e1-61ea-4161-a5c4-3bf30ba8ab88': Document(page_content=' saying \\ngoes, <Time is money,= time is even more valuable t han money: you can get back \\nmoney after you spend it, but once time is spent, y ou can never get it back. Memory \\nmay  be inexpensive,  but  it is neither  inûnite  nor  free.  You  should choose algorithms \\nthat  use  the  resources  of time  and  space  efûciently.  \\nEfûciency  \\nDifferent algorithms devised to solve the same prob lem often differ dramatically in \\ntheir  efûciency.  These  differences  can  be much  more  signiûc ant than differences \\ndue to hardware and software. \\nAs an example, Chapter 2 introduces two algorithms for sorting.  The  ûrst,  \\nknown as insertion  sort, takes time roughly equal to c 1 n 2 to sort n items, where c 1 \\nis a constant that does not depend on n. That is, it takes time roughly proportional \\nto n 2 . The second, merge  sort, takes time roughly equal to c 2 n lg n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 33}),\n",
       "  '7a1620b2-432b-4a4c-a591-403ecab79f77': Document(page_content=' time roughly equal to c 2 n lg n, where lg n \\nstands for log 2 n and c 2 is another constant that also does not depend on n. Inser-  \\ntion sort typically has a smaller constant factor t han merge sort, so that c 1 < c  2 . \\nWe’ll  see  that  the  constant  factors  can  have  far  less  of an impact on the running \\ntime than the dependence on the input size n. Let’s  write  insertion  sort’s  running  \\ntime as c 1 n \\ue001 n and  merge  sort’s  running  time  as c 2 n \\ue001 lg n. Then we see that where \\ninsertion sort has a factor of n in its running time, merge sort has a factor of lg n, \\nwhich is much smaller. For example, when n is 1000 , lg n is approximately 10, and \\nwhen n is 1,000,000,  lg n is approximately only 20. Although  insertion  sort  usu-  \\nally runs faster than merge sort for small input si z', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 33}),\n",
       "  '02b9fb88-2f5c-49b2-b238-ad640f0264a4': Document(page_content=' runs faster than merge sort for small input si zes, once the input size n becomes \\nlarge  enough,  merge  sort’s  advantage  of lg n versus n more than compensates for \\nthe difference in constant factors. No matter how m uch smaller c 1 is than c 2 , there \\nis always a crossover point beyond which merge sort  is faster. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 33}),\n",
       "  '835593bf-d225-4498-9eea-6e46c1731367': Document(page_content='1.2  Algorithms  as a technology  13 \\nFor a concrete example, let us pit a faster compute r (computer A)  running  inser-  \\ntion sort against a slower computer (computer B) ru nning merge sort. They each \\nmust sort an array of 10  million numbers. (Although 10  million numbers might \\nseem  like  a lot,  if the  numbers  are  eight-byte  integers,  then  the input occupies \\nabout 80  megabytes,  which  ûts  in the  memory  of even  an inexpensive  laptop  com-  \\nputer many times over.) Suppose that computer A exe cutes 10  billion instructions \\nper second (faster than any single sequential compu ter at the time of this writing) \\nand computer B executes only 10  million instructions per second (much slower \\nthan most contemporary computers), so that computer  A is 1000  times faster than \\ncomputer B in raw computing power. To make the diff erence even more dramatic, \\nsuppose  that  the  world’s  craftiest  programmer  codes  insertion  sort ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 34}),\n",
       "  '8a69f397-d286-4534-9345-bf12354d2991': Document(page_content='iest  programmer  codes  insertion  sort  in machine  lan-  \\nguage for computer A, and the resulting code requir es 2n  2 instructions to sort n \\nnumbers. Suppose further that just an average progr ammer implements merge \\nsort,  using  a high-level  language  with  an inefûcient  compil er, with the resulting \\ncode taking 50n  lg n instructions. To sort 10  million numbers, computer A takes \\n2 \\ue001 .10  7 / 2 instructions \\n10  10  instructions/second D 20,000 seconds (more than 5:5  hours) ; \\nwhile computer B takes \\n50  \\ue001 10  7 lg 10  7 instructions \\n10  7 instructions/second \\ue002 1163  seconds (under 20  minutes) : \\nBy using an algorithm whose running time grows more  slowly, even with a poor \\ncompiler, computer B runs more than 17  times  faster  than  computer  A!  The  ad-  \\nvantage of merge sort is even more pronounced when sorting 100  million numbers: \\nwhere insertion sort', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 34}),\n",
       "  'af62160c-d6d9-4141-b306-eee51b3a3996': Document(page_content=' 100  million numbers: \\nwhere insertion sort takes more than 23  days, merge sort takes under four hours. \\nAlthough 100  million might seem like a large number, there are m ore than 100  mil-  \\nlion web searches every half hour, more than 100  million emails sent every minute, \\nand  some  of the  smallest  galaxies  (known  as ultra-compact  dwarf  galaxies)  con-  \\ntain about 100  million stars. In general, as the problem size incr eases, so does the \\nrelative advantage of merge sort. \\nAlgorithms  and  other  technologies  \\nThe example above shows that you should consider al gorithms, like  computer  hard-  \\nware, as a technology. Total  system  performance  depends  on  choosing  efûcient  \\nalgorithms  as much  as on  choosing  fast  hardware.  Just  as rapid advances are being \\nmade in other computer technologies, they are being  made in algorithms as well. \\nYou might wonder whether algorithms are truly that important on contemporary \\ncomputers in light of other advanced technologies,', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 34}),\n",
       "  '29a51617-7253-4658-b45d-f1fc9bcf5241': Document(page_content='\\ncomputers in light of other advanced technologies, such as ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 34}),\n",
       "  '448cde9c-683c-469e-8b31-13cba5e47515': Document(page_content='14  Chapter  1 The  Role  of Algorithms  in Computing  \\n\\ue001 advanced computer architectures and fabrication tec hnologies, \\n\\ue001 easy-to-use,  intuitive,  graphical  user  interfaces  (GUIs) , \\n\\ue001 object-oriented  systems,  \\n\\ue001 integrated web technologies, \\n\\ue001 fast networking, both wired and wireless, \\n\\ue001 machine learning, \\n\\ue001 and mobile devices. \\nThe answer is yes. Although some applications do no t explicitly  require  algorith-  \\nmic content at the application level (such as some simple, web-based  applications),  \\nmany  do.  For  example,  consider  a web-based  service  that  determines how to travel \\nfrom one location to another. Its implementation wo uld rely on fast hardware, a \\ngraphical  user  interface,  wide-area  networking,  and  also  possibly  on  object  ori-  \\nentation. It would also require algorithms for oper ations such  as ûnding  routes  \\n(probably  using  a shortest-path ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 35}),\n",
       "  'e61e7b5f-b4ed-4c65-974c-4a8b87a924e3': Document(page_content='(probably  using  a shortest-path  algorithm),  rendering  maps,  and  interpolating  ad-  \\ndresses. \\nMoreover, even an application that does not require  algorithmic content at the \\napplication level relies heavily upon algorithms. D oes the application rely on fast \\nhardware?  The  hardware  design  used  algorithms.  Does  the  application rely on \\ngraphical  user  interfaces?  The  design  of any  GUI  relies  on  algorithms. Does the \\napplication  rely  on  networking?  Routing  in networks  relies  heavily on algorithms. \\nWas the application written in a language other tha n machine code?  Then  it was  \\nprocessed by a compiler, interpreter, or assembler,  all of which make extensive use \\nof algorithms. Algorithms are at the core of most t echnologies used  in contempo-  \\nrary computers. \\nMachine learning can be thought of as a method for performing algorithmic tasks \\nwithout explicitly designing an algorithm, but inst ead inferring patterns from data \\nand  thereby  automatically  learning  a solution', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 35}),\n",
       "  'b8aea198-a6b1-4730-9bdb-1504c6db960a': Document(page_content='and  thereby  automatically  learning  a solution.  At  ûrst  glance, machine learning, \\nwhich automates the process of algorithmic design, may seem to make learning \\nabout algorithms obsolete. The opposite is true, ho wever. Machine learning is \\nitself a collection of algorithms, just under a dif ferent name.  Furthermore,  it cur-  \\nrently seems that the successes of machine learning  are mainly for problems for \\nwhich we, as humans, do not really understand what the right algorithm  is. Promi-  \\nnent examples include computer vision and automatic  language translation. For \\nalgorithmic problems that humans understand well, s uch as most of the problems \\nin this  book,  efûcient  algorithms  designed  to solve  a speciû c problem are typically \\nmore  successful  than  machine-learning  approaches.  \\nData  science  is an interdisciplinary  ûeld  with  the  goal  of extracting knowledge \\nand insights from structured and unstructured data.  Data science uses methods ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 35}),\n",
       "  '96c0c747-4356-4b35-af8d-1243e7e6bbf1': Document(page_content='Problems for Chapter 1 15 \\nfrom statistics, computer science, and optimization . The design and analysis of \\nalgorithms  is fundamental  to the  ûeld.  The  core  techniques  of data science, which \\noverlap  signiûcantly  with  those  in machine  learning,  include  many  of the  algo-  \\nrithms in this book. \\nFurthermore,  with  the  ever-increasing  capacities  of compu ters, we use them to \\nsolve larger problems than ever before. As we saw i n the above comparison  be-  \\ntween insertion sort and merge sort, it is at large r problem sizes that the differences \\nin efûciency  between  algorithms  become  particularly  promi nent. \\nHaving a solid base of algorithmic knowledge and te chnique is one characteristic \\nthat  deûnes  the  truly  skilled  programmer.  With  modern  compu ting technology, you \\ncan accomplish some tasks without knowing much abou t algorithms, but with a \\ngood background in algorithms, you can do much,', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 36}),\n",
       "  '03e11ade-b12f-4df2-91bd-4080a299690c': Document(page_content='good background in algorithms, you can do much, muc h more. \\nExercises  \\n1.2-1  \\nGive  an example  of an application  that  requires  algorithmic  content  at the  applica-  \\ntion level, and discuss the function of the algorit hms involved. \\n1.2-2  \\nSuppose that for inputs of size n on a particular computer, insertion sort runs in 8n  2 \\nsteps and merge sort runs in 64n  lg n steps. For which values of n does insertion \\nsort  beat  merge  sort?  \\n1.2-3  \\nWhat is the smallest value of n such that an algorithm whose running time is 100n  2 \\nruns faster than an algorithm whose running time is  2 n on  the  same  machine?  \\nProblems  \\n1-1  Comparison  of running  times  \\nFor each function f.n/  and time t in the following table, determine the largest \\nsize n of a problem that can be solved in time t , assuming that the algorithm to \\nsolve the problem takes', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 36}),\n",
       "  'cf8c2717-92c7-4bcb-af58-a3c349d47e44': Document(page_content=' the algorithm to \\nsolve the problem takes f.n/  microseconds. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 36}),\n",
       "  'b5f3e308-7dd9-4ed0-90e4-3360c946a80d': Document(page_content='16  Chapter  1 The  Role  of Algorithms  in Computing  \\n1 1 1 1 1 1 1 \\nsecond minute hour day month year century \\nlg n \\np n \\nn \\nn lg n \\nn 2 \\nn 3 \\n2 n \\nnŠ \\nChapter  notes  \\nThere are many excellent texts on the general topic  of algorithms, including those \\nby  Aho,  Hopcroft,  and  Ullman  [5,  6],  Dasgupta,  Papadimitriou,  and  Vazirani  [107],  \\nEdmonds  [133],  Erickson  [135],  Goodrich  and  Tamassia  [195,  196],  Kleinberg  \\nand  Tardos  [257],  Knuth  [259,  260,  261,  262,  263],  Levitin  [298],  Louridas  [305],  \\nMehlhorn  and  Sanders  [325],  Mitzenmacher  and  Upfal  [331],  Neapolitan ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 37}),\n",
       "  'd0a8b837-3991-470e-9fc1-2b2638f7227f': Document(page_content='fal  [331],  Neapolitan  [342],  \\nRoughgarden  [385,  386,  387,  388],  Sanders,  Mehlhorn,  Dietzfelbinger,  and  De-  \\nmentiev  [393],  Sedgewick  and  Wayne  [402],  Skiena  [414],  Soltys-Kulinicz  [419],  \\nWilf  [455],  and  Williamson  and  Shmoys  [459].  Some  of the  more  practical  as-  \\npects  of algorithm  design  are  discussed  by  Bentley  [49,  50,  51],  Bhargava  [54],  \\nKochenderfer  and  Wheeler  [268],  and  McGeoch  [321].  Surveys  of the  ûeld  of al-  \\ngorithms  can  also  be found  in books  by  Atallah  and  Blanton  [27,  28]  and  Mehta  and  \\nSahhi  [326].  For', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 37}),\n",
       "  '40bbc6bd-2fc0-45b3-9395-74029b1994e0': Document(page_content=' \\nSahhi  [326].  For  less  technical  material,  see  the  books  by  Christian  and  Grifûths  \\n[92],  Cormen  [104],  Erwig  [136],  MacCormick  [307],  and  V¨  ocking  et al.  [448].  \\nOverviews  of the  algorithms  used  in computational  biology  can be found in books \\nby  Jones  and  Pevzner  [240],  Elloumi  and  Zomaya  [134],  and  Marchisio  [315].  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 37}),\n",
       "  '1121cd99-76ad-469e-9f2b-a15fa09044f6': Document(page_content='2 Getting  Started  \\nThis  chapter  will  familiarize  you  with  the  framework  we’ll  use throughout the book \\nto think about the design and analysis of algorithm s. It is self-contained,  but  it does  \\ninclude several references to material that will be  introduced  in Chapters  3 and  4. \\n(It also contains several summations, which Appendi x A shows how to solve.) \\nWe’ll  begin  by  examining  the  insertion  sort  algorithm  to solve  the  sorting  prob-  \\nlem  introduced  in Chapter  1. We’ll  specify  algorithms  using  a pseudocode that \\nshould be understandable to you if you have done co mputer programming.  We’ll  \\nsee why insertion sort correctly sorts and analyze its running time. The analysis \\nintroduces a notation that describes how running ti me increases with the number \\nof items to be sorted. Following a discussion of in sertion sort, we’ll  use  a method  \\ncalled  divide-and-conquer  to develop  a sorting ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 38}),\n",
       "  'a0b8cc0b-f01f-4087-88c3-ef734ddcc9a4': Document(page_content='-conquer  to develop  a sorting  algorithm  called  merge  sort.  We’ll  \\nend  with  an analysis  of merge  sort’s  running  time.  \\n2.1  Insertion  sort  \\nOur  ûrst  algorithm,  insertion  sort,  solves  the  sorting  problem  introduced  in Chap-  \\nter  1: \\nInput:  A sequence of n numbers ha 1 ;a  2 ;:::;a  n i. \\nOutput:  A permutation (reordering) ha 0 \\n1 ;a  0 \\n2 ;:::;a  0 \\nn i of the input sequence such \\nthat a 0 \\n1 හ a 0 \\n2 හ \\ue001 \\ue001 \\ue001 හ  a 0 \\nn . \\nThe numbers to be sorted are also known as the keys. Although  the  problem  is con-  \\nceptually about sorting a sequence, the input comes  in the form of an array with \\nn elements.  When  we  want ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 38}),\n",
       "  '0eb2c581-7bff-450a-8435-8d292d8f8997': Document(page_content='n elements.  When  we  want  to sort  numbers,  it’s  often  because  they are the keys \\nassociated with other data, which we call satellite  data. Together,  a key  and  satel-  \\nlite data form a record . For example, consider a spreadsheet containing st udent \\nrecords with many associated pieces of data such as  age, grade-point  average,  and  \\nnumber of courses taken. Any one of these quantitie s could be a key, but when the ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 38}),\n",
       "  '0af4a2ec-e87e-41f8-9071-6f9ba01b794d': Document(page_content='18 Chapter 2 Getting Started \\nspreadsheet sorts, it moves the associated record ( the satellite data) with the key. \\nWhen describing a sorting algorithm, we focus on th e keys, but it is important to \\nremember that there usually is associated satellite  data. \\nIn this  book,  we’ll  typically  describe  algorithms  as proced ures written in a pseu-  \\ndocode  that  is similar  in many  respects  to C, C++,  Java,  Python,  1 or JavaScript.  \\n(Apologies  if we’ve  omitted  your  favorite  programming  language.  We  can’t  list  \\nthem all.) If you have been introduced to any of th ese languages, you should have \\nlittle trouble understanding algorithms <coded= in pseudocode. What separates \\npseudocode from real code is that in pseudocode, we  employ wh atever  expres-  \\nsive method is most clear and concise to specify a given algorithm. Sometimes \\nthe clearest method is English, so do not be surpri sed if you come  across  an', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 39}),\n",
       "  'c02f1178-5f48-4765-8d13-ebc728837054': Document(page_content=' surpri sed if you come  across  an En-  \\nglish phrase or sentence embedded within a section that looks more like real code. \\nAnother difference between pseudocode and real code  is that pseudocode  often  ig-  \\nnores  aspects  of software  engineering4such  as data  abstrac tion, modularity, and \\nerror  handling4in  order  to convey  the  essence  of the  algorit hm more concisely. \\nWe start with insertion  sort, which  is an efûcient  algorithm  for  sorting  a small  \\nnumber of elements. Insertion sort works the way yo u might sort a hand of playing \\ncards. Start with an empty left hand and the cards in a pile on the table. Pick up \\nthe  ûrst  card  in the  pile  and  hold  it with  your  left  hand.  Then,  with your right hand, \\nremove one card at a time from the pile, and insert  it into the correct position in \\nyour  left  hand.  As  Figure  2.1  illustrates,', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 39}),\n",
       "  '4090b217-2581-488b-b39d-447628667d64': Document(page_content=' As  Figure  2.1  illustrates,  you  ûnd  the  correc t position for a card \\nby comparing it with each of the cards already in y our left hand, starting at the \\nright and moving left. As soon as you see a card in  your left hand whose value is \\nless  than  or equal  to the  card  you’re  holding  in your  right  hand, insert the card that \\nyou’re  holding  in your  right  hand  just  to the  right  of this  card in your left hand. If \\nall the cards in your left hand have values greater  than the card in your right hand, \\nthen place this card as the leftmost card in your l eft hand. At all times, the cards \\nheld in your left hand are sorted, and these cards were originally the top cards of \\nthe pile on the table. \\nThe pseudocode for insertion sort is given as the p rocedure I NSERTION-SORT  \\non the facing page. It takes two parameters: an arr ay A containing the values to \\nbe sorted and the', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 39}),\n",
       "  '2e777def-41d7-4454-9e97-cd10318feaa5': Document(page_content=' containing the values to \\nbe sorted and the number n of values of sort. The values occupy positions AŒ1�  \\nthrough AŒn�  of the array, which we denote by AŒ1  W n�. When the I NSERTION- \\nSORT  procedure  is ûnished,  array  AŒ1  W n� contains the original values, but in sorted \\norder. \\n1 If you’re  familiar  with  only  Python,  you  can  think  of arrays  as similar to Python lists. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 39}),\n",
       "  '2feff186-1764-4a9b-a25f-2f3e78cc51b0': Document(page_content='2.1 Insertion sort 19 \\n2 \\n♥ ♥ ♥ \\n2 \\n♥ 4 \\n♥ ♥ ♥ ♥ ♥ 4 \\n♥ 5 ♥ ♥ ♥  ♥ ♥  5 ♥ ♥ 7 ♥ \\n♥ ♥ \\n♥ ♥ ♥ ♥ 7 ♥ ♥ \\n10  ♥ ♥ ♥ ♥ \\n♥ ♥ ♥ ♥ ♥ \\n♥ ♥ 10  ♥ \\nFigure  2.1  Sorting a hand of cards using insertion sort. \\nI NSERTION-SORT  .A;n/  \\n1 for  i D 2 to n \\n2 key  D AŒi�  \\n3 / / Insert AŒi�  into the sorted subarray AŒ1  W i \\ue003 1�. \\n4 j D i \\ue003 1 \\n5 while  j >0  and AŒj�>  key  \\n6 AŒj  C 1� D AŒj�  \\n7 j D j \\ue003 1 \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 40}),\n",
       "  '9b4fe8ea-1ee1-4950-93da-09985a745e41': Document(page_content='7 j D j \\ue003 1 \\n8 AŒj  C 1� D key  \\nLoop  invariants  and  the  correctness  of insertion  sort  \\nFigure 2.2 shows how this algorithm works for an ar ray A that starts out with \\nthe sequence h5;2;4;6;1;3 i. The index i indicates the <current card= being \\ninserted into the hand. At the beginning of each it eration of the for  loop, which \\nis indexed by i , the subarray  (a contiguous portion of the array) consisting of \\nelements AŒ1  W i \\ue003 1� (that is, AŒ1�  through AŒi  \\ue003 1�) constitutes the currently sorted \\nhand, and the remaining subarray AŒi  C 1 W n� (elements AŒi  C 1� through AŒn�) \\ncorresponds to the pile of cards still on the table . In fact, elements AŒ1  W i \\ue003 1� are \\nthe elements originally  in positions  1 through  i', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 40}),\n",
       "  '993b35e1-d1ec-4b30-9c94-aa24ec90413e': Document(page_content=' elements originally  in positions  1 through  i \\ue003 1, but now in sorted order. We \\nstate these properties of AŒ1  W i \\ue003 1� formally as a loop  invariant : ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 40}),\n",
       "  '7e8e6043-62dd-4462-bfed-6accd34e64ad': Document(page_content='20 Chapter 2 Getting Started \\n1 2 3 4 5 6 \\n5 2 4 6 1 3 (a) 1 2 3 4 5 6 \\n2 5 4 6 1 3 (b) 1 2 3 4 5 6 \\n2 4 5 6 1 3 (c) \\n1 2 3 4 5 6 \\n2 4 5 6 1 3 (d) 1 2 3 4 5 6 \\n2 4 5 6 1 3 (e) 1 2 3 4 5 6 \\n2 4 5 6 1 3 (f) \\nFigure  2.2  The operation of I NSERTION-SORT.A;n/ , where A initially contains the sequence \\nh5;2;4;6;1;3 i and n D 6. Array indices appear above the rectangles, and va lues stored in the \\narray positions appear within the rectangles. (a)–(e)  The iterations of the for  loop  of lines  138.  In \\neach iteration, the blue rectangle holds the key ta ken from AŒi�, which is compared with the values \\nin tan  rectangles  to its  left  in the  test  of line', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 41}),\n",
       "  '9e069fbf-dfac-47ed-9b20-1118acf1ec45': Document(page_content='  left  in the  test  of line  5. Orange  arrows show array values moved one position \\nto the  right  in line  6, and  blue  arrows  indicate  where  the  key  moves  to in line  8. (f)  The  ûnal  sorted  \\narray. \\nAt the start of each iteration of the for  loop  of lines  138,  the  subarray  \\nAŒ1  W i \\ue003 1� consists of the elements originally in AŒ1  W i \\ue003 1�, but in sorted \\norder. \\nLoop invariants help us understand why an algorithm  is correct.  When  you’re  \\nusing a loop invariant, you need to show three thin gs: \\nInitialization:  It is true  prior  to the  ûrst  iteration  of the  loop.  \\nMaintenance:  If it is true before an iteration of the loop, it r emains true before \\nthe next iteration. \\nTermination:  The  loop  terminates,  and  when  it terminates', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 41}),\n",
       "  '35e547c7-921f-4f8e-abe8-8a2910139cef': Document(page_content='ates,  and  when  it terminates,  the  invariant4 usually \\nalong  with  the  reason  that  the  loop  terminated4gives  us a useful property that \\nhelps show that the algorithm is correct. \\nWhen  the  ûrst  two  properties  hold,  the  loop  invariant  is true  prior to every iteration \\nof the  loop.  (Of  course,  you  are  free  to use  established  facts  other than the loop \\ninvariant itself to prove that the loop invariant r emains true before each iteration.) \\nA loop-invariant  proof  is a form  of mathematical  induction,  where to prove that a \\nproperty holds, you prove a base case and an induct ive step. Here, showing that the \\ninvariant  holds  before  the  ûrst  iteration  corresponds  to the base case, and showing \\nthat the invariant holds from iteration to iteratio n corresponds to the inductive step. \\nThe third property is perhaps the most important on e, since you are using the \\nloop invari', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 41}),\n",
       "  '0b58bde7-216f-4e54-808b-51e708bbae77': Document(page_content=', since you are using the \\nloop invariant to show correctness. Typically, you use the loop invariant along with \\nthe condition that caused the loop to terminate. Ma thematical induction typically \\napplies  the  inductive  step  inûnitely,  but  in a loop  invarian t the <induction= stops \\nwhen the loop terminates. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 41}),\n",
       "  '250ea6b6-2367-4006-b5ef-f1fc870404e1': Document(page_content='2.1 Insertion sort 21 \\nLet’s  see  how  these  properties  hold  for  insertion  sort.  \\nInitialization:  We  start  by  showing  that  the  loop  invariant  holds  before  the  ûrst \\nloop iteration, when i D 2. 2 The subarray AŒ1  W i \\ue003 1� consists of just the \\nsingle element AŒ1�, which is in fact the original element in AŒ1�. Moreover, \\nthis subarray is sorted (after all, how could a sub array with just one value not \\nbe sorted?),  which  shows  that  the  loop  invariant  holds  prior  to the  ûrst  iteration  \\nof the loop. \\nMaintenance:  Next, we tackle the second property: showing that e ach iteration \\nmaintains the loop invariant. Informally, the body of the for  loop works by \\nmoving the values in AŒi  \\ue003 1�, AŒi  \\ue003 2�, AŒi  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 42}),\n",
       "  '48abd84d-3c87-4544-9602-fece04b525b1': Document(page_content='� 2�, AŒi  \\ue003 3�, and so on by one position \\nto the  right  until  it ûnds  the  proper  position  for  AŒi�  (lines  437),  at which  point  \\nit inserts the value of AŒi�  (line  8).  The  subarray  AŒ1  W i� then consists of the \\nelements originally in AŒ1  W i�, but in sorted order. Incrementing  i (increasing \\nits value by 1) for the next iteration of the for  loop then preserves the loop \\ninvariant. \\nA more formal treatment of the second property woul d require us to state and \\nshow a loop invariant for the while  loop  of lines  537.  Let’s  not  get  bogged  \\ndown  in such  formalism  just  yet.  Instead,  we’ll  rely  on  our  informal analysis to \\nshow that the second property holds for the outer l oop. \\nTermination:  Finally,', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 42}),\n",
       "  '64c90dee-23b7-4ee0-9308-ef1aab747542': Document(page_content='op. \\nTermination:  Finally, we examine loop termination. The loop vari able i starts \\nat 2 and increases by 1 in each  iteration.  Once  i ’s value  exceeds  n in line  1, the  \\nloop terminates. That is, the loop terminates once i equals n C 1. Substituting \\nn C 1 for i in the wording of the loop invariant yields that th e subarray AŒ1  W n� \\nconsists of the elements originally in AŒ1  W n�, but in sorted order. Hence, the \\nalgorithm is correct. \\nThis method of loop invariants is used to show corr ectness in various places \\nthroughout this book. \\nPseudocode  conventions  \\nWe use the following conventions in our pseudocode.  \\n\\ue001 Indentation indicates block structure. For example,  the body of the for  loop that \\nbegins  on  line  1 consists  of lines  238,  and  the  body  of the  while  loop that \\n2 When the loop is a for  loop,  the', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 42}),\n",
       "  '88d03a47-13a4-4c56-981a-3ca6f9bd20af': Document(page_content=' the loop is a for  loop,  the  loop-invariant  check  just  prior  to the  ûrst  iteration  occurs  immedi-  \\nately  after  the  initial  assignment  to the  loop-counter  variable  and  just  before  the  ûrst  test  in the  loop  \\nheader. In the case of I NSERTION-SORT, this time is after assigning 2 to the variable i but before the \\nûrst  test  of whether  i හ n. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 42}),\n",
       "  '30efd1c8-7c83-45c9-91a9-79a6e2ac10cb': Document(page_content='22 Chapter 2 Getting Started \\nbegins  on  line  5 contains  lines  637  but  not  line  8. Our  indenta tion style applies \\nto if-else  statements 3 as well. Using indentation instead of textual indic ators \\nof block structure, such as begin  and end  statements or curly braces, reduces \\nclutter while preserving, or even enhancing, clarit y. 4 \\n\\ue001 The looping constructs while , for, and repeat-until  and the if-else  conditional \\nconstruct  have  interpretations  similar  to those  in C, C++,  Java, Python, and \\nJavaScript.  5 In this book, the loop counter retains its value af ter the loop is \\nexited,  unlike  some  situations  that  arise  in C++  and  Java.  Thus, immediately \\nafter a for  loop,  the  loop  counter’s  value  is the  value  that  ûrst  exceede d the for  \\nloop bound. 6 We used this property in our correctness argument f or insertion \\nsort. The for  loop ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 43}),\n",
       "  '5b550bea-56c4-4310-ab86-38f7f93540ce': Document(page_content=' insertion \\nsort. The for  loop  header  in line  1 is for  i D 2 to n, and so when this loop \\nterminates, i equals nC1. We use the keyword to when a for  loop increments its \\nloop counter in each iteration, and we use the keyw ord downto  when a for  loop \\ndecrements  its loop counter (reduces its value by 1 in each iteration). When \\nthe loop counter changes by an amount greater than 1, the amount of change \\nfollows the optional keyword by. \\n\\ue001 The symbol < / /= indicates that the remainder of the line is a com ment. \\n\\ue001 Variables (such as i , j , and key) are  local  to the  given  procedure.  We  won’t  use  \\nglobal variables without explicit indication. \\n\\ue001 We access array elements by specifying the array na me followed by the index \\nin square brackets. For example, AŒi�  indicates the i th element of the array A. \\nAlthough many programming languages enforce 0-origin  indexing  for  arrays  (0 \\nis', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 43}),\n",
       "  '48a9254e-083d-4d0e-8416-57588977097a': Document(page_content='  for  arrays  (0 \\nis the smallest valid index), we choose whichever i ndexing scheme is clearest \\nfor human readers to understand. Because people usu ally start counting at 1, \\nnot 0, most4but  not  all4of  the  arrays  in this  book  use  1-origin  indexing.  To  be \\n3 In an if-else  statement, we indent else  at the same level as its matching if. The  ûrst  executable  line  \\nof an else  clause appears on the same line as the keyword else. For multiway tests, we use elseif  for \\ntests  after  the  ûrst  one.  When  it is the  ûrst  line  in an  else  clause, an if statement appears on the line \\nfollowing else  so that you do not misconstrue it as elseif . \\n4 Each pseudocode procedure in this book appears on o ne page so that you do not need to discern \\nlevels of indentation in pseudocode that is split a cross pages. \\n5 Most  block-struct', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 43}),\n",
       "  '34e71320-3160-4d06-99cf-14540d81b2ce': Document(page_content=' pages. \\n5 Most  block-structured  languages  have  equivalent  construc ts, though the exact syntax may differ. \\nPython lacks repeat-until  loops, and its for  loops operate differently from the for  loops in this book. \\nThink of the pseudocode line < for  i D 1 to n= as equivalent  to <for  i in range(1,  n+1)=  in Python.  \\n6 In Python, the loop counter retains its value after  the loop is exited, but the value it retains is th e \\nvalue  it had  during  the  ûnal  iteration  of the  for  loop, rather than the value that exceeded the loop \\nbound. That is because a Python for  loop iterates through a list, which may contain non numeric \\nvalues. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 43}),\n",
       "  'fe65f22f-3f44-4787-bcc7-ce78966f1686': Document(page_content='2.1 Insertion sort 23 \\nclear about whether a particular algorithm assumes 0-origin  or 1-origin  index-  \\ning,  we’ll  specify  the  bounds  of the  arrays  explicitly.  If you are implementing \\nan algorithm that we specify using 1-origin  indexing,  but  you’re  writing  in a \\nprogramming language that enforces 0-origin  indexing  (such  as C, C++,  Java,  \\nPython,  or JavaScript),  then  give  yourself  credit  for  being  able to adjust. You \\ncan either always subtract 1 from each index or allocate each array with one \\nextra position and just ignore position 0. \\nThe notation < W= denotes a subarray. Thus, AŒi  W j� indicates the subarray of A \\nconsisting of the elements AŒi�;AŒi  C 1�;:::;AŒj� . 7 We also use this notation \\nto indicate the bounds of an array, as we did earli er when discussing the array \\nAŒ1  W', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 44}),\n",
       "  '782425fe-bcaf-401d-933c-f3a7b7744378': Document(page_content=' the array \\nAŒ1  W n�. \\n\\ue001 We typically organize compound data into objects , which are composed of \\nattributes . We access a particular attribute using the syntax  found in many \\nobject-oriented  programming  languages:  the  object  name,  followed by a dot, \\nfollowed by the attribute name. For example, if an object x has attribute f , we \\ndenote this attribute by x: f . \\nWe treat a variable representing an array or object  as a pointer (known as a \\nreference in some programming languages) to the dat a representing the array \\nor object. For all attributes f of an object x , setting y D x causes y: f to \\nequal x: f . Moreover, if we now set x: f D 3, then afterward not only does x: f \\nequal 3, but y: f equals 3 as well. In other words, x and y point to the same \\nobject after the assignment y D x . This way of treating arrays and objects is \\nconsistent with most contemporary programming langu ages. \\nOur  attribute  notation  can  <cascade.=  For', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 44}),\n",
       "  '1893c9e6-63ad-4aaf-b286-caeea3be3920': Document(page_content='  can  <cascade.=  For  example,  suppose  that the attribute f \\nis itself a pointer to some type of object that has  an attribute g. Then the notation \\nx: f : g is implicitly parenthesized as .x:  f /: g. In other words, if we had assigned \\ny D x: f , then x: f : g is the same as y: g. \\nSometimes a pointer refers to no object at all. In this case, we give it the special \\nvalue NIL. \\n\\ue001 We pass parameters to a procedure by value : the called procedure receives its \\nown copy of the parameters, and if it assigns a val ue to a parameter, the change \\nis not seen by the calling procedure. When objects are pas sed, the pointer to \\nthe  data  representing  the  object  is copied,  but  the  object’s  attributes are not. For \\nexample, if x is a parameter of a called procedure, the assignmen t x D y within \\n7 If you’re  used  to programming  in Python,  bear  in mind  that ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 44}),\n",
       "  '989f4a7c-72e9-4dd4-8cc8-496cb298a80e': Document(page_content=' Python,  bear  in mind  that  in this book, the subarray AŒi  W j� \\nincludes the element AŒj� . In Python, the last element of AŒi  W j� is AŒj  \\ue003 1�. Python allows negative \\nindices, which count from the back end of the list.  This book does not use negative array indices. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 44}),\n",
       "  'a57ff3d4-28b2-4682-ab9e-b26f2503aed8': Document(page_content='24 Chapter 2 Getting Started \\nthe called procedure is not visible to the calling procedure. The assignment \\nx: f D 3, however, is visible if the calling procedure has a pointer to the same \\nobject as x . Similarly, arrays are passed by pointer, so that a pointer to the array \\nis passed, rather than the entire array, and change s to individual array elements \\nare visible to the calling procedure. Again, most c ontemporary programming \\nlanguages work this way. \\n\\ue001 A return  statement immediately transfers control back to the  point of call in \\nthe calling procedure. Most return  statements also take a value to pass back to \\nthe  caller.  Our  pseudocode  differs  from  many  programming  languages in that \\nwe allow multiple values to be returned in a single  return  statement without \\nhaving to create objects to package them together. 8 \\n\\ue001 The boolean operators <and= and <or= are short  circuiting . That is, evaluate \\nthe expression < x and y = by  ûrst  evaluating  x . If x evaluates to FALSE , then \\nthe entire expression cannot evaluate to', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 45}),\n",
       "  'a98f2a71-7559-4ba2-ae1b-79581fcd9364': Document(page_content=' , then \\nthe entire expression cannot evaluate to TRUE , and therefore y is not evaluated. \\nIf, on the other hand, x evaluates to TRUE , y must be evaluated to determine \\nthe value of the entire expression. Similarly, in t he expression < x or y = the  ex-  \\npression y is evaluated only if x evaluates to FALSE. Short-circuiting  operators  \\nallow us to write boolean expressions such as < x ¤ NIL and x: f D y = without \\nworrying about what happens upon evaluating x: f when x is NIL. \\n\\ue001 The keyword error  indicates that an error occurred because conditions  were \\nwrong for the procedure to have been called, and th e procedure immediately \\nterminates. The calling procedure is responsible fo r handling the error, and so \\nwe do not specify what action to take. \\nExercises  \\n2.1-1  \\nUsing Figure 2.2 as a model, illustrate the operati on of I NSERTION-SORT  on an \\narray initially containing the sequence h31;41;59;26;41;58 i. \\n2', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 45}),\n",
       "  '8146f013-0d8c-47c8-8488-d5b125da9f7d': Document(page_content='26;41;58 i. \\n2.1-2  \\nConsider the procedure S UM-ARRAY on the facing page. It computes the sum of \\nthe n numbers in array AŒ1  W n�. State a loop invariant for this procedure, and us e \\nits initialization, maintenance, and termination pr operties to show that the S UM- \\nARRAY procedure returns the sum of the numbers in AŒ1  W n�. \\n8 Python’s  tuple  notation  allows  return  statements to return multiple values without creati ng objects \\nfrom  a programmer-deûned  class.  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 45}),\n",
       "  '3b351ed3-e854-4c83-8ef1-777421eeb96f': Document(page_content='2.2  Analyzing  algorithms  25 \\nSUM-ARRAY.A;n/  \\n1 sum  D 0 \\n2 for  i D 1 to n \\n3 sum  D sum  C AŒi�  \\n4 return  sum  \\n2.1-3  \\nRewrite the I NSERTION-SORT  procedure  to sort  into  monotonically  decreasing  in-  \\nstead of monotonically increasing order. \\n2.1-4  \\nConsider the searching  problem : \\nInput:  A sequence of n numbers ha 1 ;a  2 ;:::;a  n i stored in array AŒ1  W n� and a \\nvalue x . \\nOutput:  An index i such that x equals AŒi�  or the special value NIL if x does not \\nappear in A. \\nWrite pseudocode for linear  search, which  scans  through  the  array  from  begin-  \\nning to end, looking for x . Using a loop invariant, prove that your algorithm  is \\ncorrect.  Make', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 46}),\n",
       "  '7f19df8e-dfd6-4330-a01f-2e6f4bc51eec': Document(page_content=' your algorithm  is \\ncorrect.  Make  sure  that  your  loop  invariant  fulûlls  the  three necessary properties. \\n2.1-5  \\nConsider the problem of adding two n-bit  binary  integers  a and b, stored in two \\nn-element  arrays  AŒ0  W n \\ue003 1� and BŒ0  W n \\ue003 1�, where each element is either 0 \\nor 1, a D P  n\\ue0021 \\ni D0 AŒi�  \\ue001 2 i , and b D P  n\\ue0021 \\ni D0 BŒi�  \\ue001 2 i . The sum c D a C b of the \\ntwo integers should be stored in binary form in an .n C 1/-element  array  CŒ0  W n�, \\nwhere c D P  n \\ni D0 CŒi�  \\ue001 2 i . Write a procedure A DD-BINARY-I NTEGERS  that takes \\nas input arrays A and B , along with the length n,', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 46}),\n",
       "  '3f435acb-777f-4ec7-be1f-6e8fa254767b': Document(page_content=' A and B , along with the length n, and returns array C holding the \\nsum. \\n2.2  Analyzing  algorithms  \\nAnalyzing  an algorithm has come to mean predicting the resour ces that the algo-  \\nrithm requires. You might consider resources such a s memory, communication \\nbandwidth, or energy consumption. Most often, howev er, you’ll want  to measure  \\ncomputational time. If you analyze several candidat e algorithms for a problem, ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 46}),\n",
       "  '13d7e925-5233-43a2-9058-3ac7f0603167': Document(page_content='26 Chapter 2 Getting Started \\nyou  can  identify  the  most  efûcient  one.  There  might  be more  than just one viable \\ncandidate, but you can often rule out several infer ior algorithms in the process. \\nBefore you can analyze an algorithm, you need a mod el of the technology that \\nit runs on, including the resources of that technol ogy and a way to express their \\ncosts.  Most  of this  book  assumes  a generic  one-processor,  random-access  ma-  \\nchine  (RAM)  model of computation as the implementation technolo gy, with the \\nunderstanding that algorithms are implemented as co mputer programs. In the RAM \\nmodel, instructions execute one after another, with  no concurrent operations. The \\nRAM model assumes that each instruction takes the s ame amount of time as any \\nother  instruction  and  that  each  data  access4using  the  value  of a variable or storing \\ninto  a variable4takes  the  same  amount  of time  as any  other  data access. In other \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 47}),\n",
       "  '84fe1d42-d04c-4d75-bc68-a9222e3eefed': Document(page_content='  other  data access. In other \\nwords, in the RAM model each instruction or data ac cess takes a constant amount \\nof time4even  indexing  into  an array.  9 \\nStrictly  speaking,  we  should  precisely  deûne  the  instructi ons of the RAM model \\nand their costs. To do so, however, would be tediou s and yield little  insight  into  al-  \\ngorithm design and analysis. Yet we must be careful  not to abuse the RAM model. \\nFor  example,  what  if a RAM  had  an instruction  that  sorts?  Then  you could sort \\nin just one step. Such a RAM would be unrealistic, since such instructions do \\nnot  appear  in real  computers.  Our  guide,  therefore,  is how  real  computers  are  de-  \\nsigned. The RAM model contains instructions commonl y found in real computers: \\narithmetic (such as add, subtract, multiply, divide , remainder,  üoor,  ceiling),  data  \\nmovement (load, store, copy), and', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 47}),\n",
       "  '8bb3f95e-81d2-414a-a630-7501dbc83e67': Document(page_content='movement (load, store, copy), and control (conditio nal and unconditional branch, \\nsubroutine call and return). \\nThe  data  types  in the  RAM  model  are  integer,  üoating  point  (for  storing  real-  \\nnumber approximations), and character. Real compute rs do not usually have a \\nseparate data type for the boolean values TRUE and FALSE . Instead, they often test \\nwhether an integer value is 0 (FALSE ) or nonzero ( TRUE ), as in C. Although we \\ntypically  do  not  concern  ourselves  with  precision  for  üoating-point  values  in this  \\nbook  (many  numbers  cannot  be represented  exactly  in üoating  point), precision is \\ncrucial for most applications. We also assume that each word of data has a limit on \\nthe number of bits. For example, when working with inputs of size n, we typically \\n9 We assume that each element of a given array occupi es the same number of bytes and that the \\nelements of a given array are stored in', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 47}),\n",
       "  'bb3d7d4b-ed9d-45d1-85a6-12fbcca0bbb8': Document(page_content='\\nelements of a given array are stored in contiguous memory locations. For example, if array AŒ1  W n� \\nstarts at memory address 1000  and each element occupies four bytes, then element AŒi�  is at address \\n1000  C 4.i  \\ue003 1/. In general, computing the address in memory of a particular array element requires \\nat most one subtraction (no subtraction for a 0-origin  array),  one  multiplication  (often  implemented  \\nas a shift operation if the element size is an exac t power of 2), and one addition. Furthermore, for \\ncode that iterates through the elements of an array  in order, an optimizing compiler can generate the \\naddress of each element using just one addition, by  adding the element size to the address of the \\npreceding element. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 47}),\n",
       "  '4c3e3ad2-97b4-492b-8c7f-fe3a9b72a601': Document(page_content='2.2  Analyzing  algorithms  27 \\nassume that integers are represented by c log 2 n bits for some constant c \\ue004 1. We \\nrequire c \\ue004 1 so that each word can hold the value of n, enabling us to index \\nthe individual input elements, and we restrict c to be a constant so that the word \\nsize does not grow arbitrarily. (If the word size c ould grow arbitrarily, we could \\nstore huge amounts of data in one word and operate on it all in constant  time4an  \\nunrealistic scenario.) \\nReal computers contain instructions not listed abov e, and such  instructions  rep-  \\nresent a gray area in the RAM model. For example, i s exponentiation  a constant-  \\ntime  instruction?  In the  general  case,  no:  to compute  x n when x and n are general \\nintegers typically takes time logarithmic in n (see  equation  (31.34)  on  page  934),  \\nand  you  must  worry  about  whether  the  result  ûts  into  a compute', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 48}),\n",
       "  '9802e7a0-f4f3-46b4-a4d5-b80ae3ac3035': Document(page_content=' result  ûts  into  a compute r word. If n is an \\nexact power of 2, however, exponentiation can usually be viewed as a constant-time  \\noperation. Many computers have a <shift left= instr uction, which in constant time \\nshifts the bits of an integer by n positions to the left. In most computers, shifting \\nthe bits of an integer by 1 position to the left is equivalent to multiplying b y 2, so \\nthat shifting the bits by n positions to the left is equivalent to multiplying by 2 n . \\nTherefore, such computers can compute 2 n in 1 constant-time  instruction  by  shift-  \\ning the integer 1 by n positions to the left, as long as n is no more than the number \\nof bits  in a computer  word.  We’ll  try  to avoid  such  gray  areas  in the RAM model \\nand treat computing 2 n and multiplying by 2 n as constant-time  operations  when  \\nthe  result  is small  enough  to ût in a computer  word.  \\nThe RAM model does not account for the memory hiera', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 48}),\n",
       "  'f1dc20da-dfad-4441-8f75-dddaf0bcf0f1': Document(page_content=' RAM model does not account for the memory hiera rchy that is common \\nin contemporary computers. It models neither caches  nor virtual  memory.  Sev-  \\neral other computational models attempt to account for memory-hierarchy  effects,  \\nwhich  are  sometimes  signiûcant  in real  programs  on  real  machines.  Section  11.5  \\nand  a handful  of problems  in this  book  examine  memory-hierar chy effects, but for \\nthe most part, the analyses in this book do not con sider them. Models that include \\nthe memory hierarchy are quite a bit more complex t han the RAM model, and so \\nthey  can  be difûcult  to work  with.  Moreover,  RAM-model  analy ses are usually \\nexcellent predictors of performance on actual machi nes. \\nAlthough it is often straightforward to analyze an algorithm in the RAM model, \\nsometimes it can be quite a challenge. You might ne ed to employ mathematical \\ntools such as combinatorics, probability theory, al gebraic dexterity, and the ability \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 48}),\n",
       "  '39be63c3-2962-4e97-8f64-d8b2afed65fe': Document(page_content=' gebraic dexterity, and the ability \\nto identify  the  most  signiûcant  terms  in a formula.  Because  an algorithm might \\nbehave differently for each possible input, we need  a means for summarizing that \\nbehavior in simple, easily understood formulas. \\nAnalysis  of insertion  sort  \\nHow long does the I NSERTION-SORT  procedure  take?  One  way  to tell  would  be for  \\nyou to run it on your computer and time how long it  takes to run. Of  course,  you’d  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 48}),\n",
       "  '96f03f77-beae-41cd-bdbf-e63c1418d7b2': Document(page_content='28 Chapter 2 Getting Started \\nûrst  have  to implement  it in a real  programming  language,  since you cannot run our \\npseudocode  directly.  What  would  such  a timing  test  tell  you?  You  would  ûnd  out  \\nhow long insertion sort takes to run on your partic ular computer, on that particular \\ninput, under the particular implementation that you  created, with the particular \\ncompiler or interpreter that you ran, with the part icular libraries that you linked \\nin, and with the particular background tasks that w ere running on your computer \\nconcurrently with your timing test (such as checkin g for incoming information over \\na network). If you run insertion sort again on your  computer with the same input, \\nyou might even get a different timing result. From running just one implementation \\nof insertion sort on just one computer and on just one input, what would you be able \\nto determine  about  insertion  sort’s  running  time  if you  were  to give it a different \\ninput, if you were to run it on a different compute r', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 49}),\n",
       "  '3e1befbc-3845-4570-a2ac-9b2ca0a950b2': Document(page_content=' you were to run it on a different compute r, or if you were to implement it \\nin a different  programming  language?  Not  much.  We  need  a way  to predict, given \\na new input, how long insertion sort will take. \\nInstead of timing a run, or even several runs, of i nsertion sort, we can determine \\nhow  long  it takes  by  analyzing  the  algorithm  itself.  We’ll  examine how many times \\nit executes each line of pseudocode and how long ea ch line of pseudocode takes \\nto run.  We’ll  ûrst  come  up  with  a precise  but  complicated  form ula for the running \\ntime.  Then,  we’ll  distill  the  important  part  of the  formula  using  a convenient  no-  \\ntation that can help us compare the running times o f different algorithms for the \\nsame problem. \\nHow  do  we  analyze  insertion  sort?  First,  let’s  acknowledge  that the running time \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 49}),\n",
       "  '2b89fcb0-b76b-49ef-89e8-ee247200b0a2': Document(page_content='s  acknowledge  that the running time \\ndepends  on  the  input.  You  shouldn’t  be terribly  surprised  that sorting a thousand \\nnumbers takes longer than sorting three numbers. Mo reover, insertion sort can take \\ndifferent amounts of time to sort two input arrays of the same size, depending on \\nhow nearly sorted they already are. Even though the  running time can depend on \\nmany  features  of the  input,  we’ll  focus  on  the  one  that  has  been shown to have \\nthe greatest effect, namely the size of the input, and describe the running time of a \\nprogram as a function of the size of its input. To do so, we need to deûne  the  terms  \\n<running time= and <input size= more carefully. We also need to be clear about \\nwhether we are discussing the running time for an i nput that elicits  the  worst-case  \\nbehavior,  the  best-case  behavior,  or some  other  case.  \\nThe best notion for input  size  depends on the problem being studied.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 49}),\n",
       "  'fddddebf-0599-44ee-9e0d-9234d1e71ba1': Document(page_content='  size  depends on the problem being studied. For many \\nproblems, such as sorting or computing discrete Fou rier transforms,  the  most  nat-  \\nural measure is the number  of items  in the  input4for  example,  the  number  n of \\nitems being sorted. For many other problems, such a s multiplying two integers, \\nthe best measure of input size is the total  number  of bits  needed to represent the \\ninput in ordinary binary notation. Sometimes it is more appropriate to describe the \\nsize of the input with more than just one number. F or example, if the input to an \\nalgorithm is a graph, we usually characterize the i nput size by both the number ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 49}),\n",
       "  '5b146c4a-1a92-4c7a-bcec-f709d1c192af': Document(page_content='2.2  Analyzing  algorithms  29 \\nof vertices  and  the  number  of edges  in the  graph.  We’ll  indica te which input size \\nmeasure is being used with each problem we study. \\nThe running  time  of an algorithm  on  a particular  input  is the  number  of in-  \\nstructions and data accesses executed. How we accou nt for these costs should be \\nindependent of any particular computer, but within the framework of the RAM \\nmodel. For the moment, let us adopt the following v iew. A constant amount of \\ntime  is required  to execute  each  line  of our  pseudocode.  One  line might take more \\nor less  time  than  another  line,  but  we’ll  assume  that  each  execution of the kth line \\ntakes c k time, where c k is a constant. This viewpoint is in keeping with th e RAM \\nmodel,  and  it also  reüects  how  the  pseudocode  would  be implem ented on most', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 50}),\n",
       "  'f6c4ac78-b485-4657-8d3c-28ccc60efef9': Document(page_content='  would  be implem ented on most \\nactual computers. 10  \\nLet’s  analyze  the  I NSERTION-SORT  procedure.  As  promised,  we’ll  start  by  de-  \\nvising a precise formula that uses the input size a nd all the statement costs c k . \\nThis  formula  turns  out  to be messy,  however.  We’ll  then  switch  to a simpler  no-  \\ntation that is more concise and easier to use. This  simpler notation makes clear \\nhow to compare the running times of algorithms, esp ecially as the size of the input \\nincreases. \\nTo analyze the I NSERTION-SORT  procedure,  let’s  view  it on  the  following  page  \\nwith the time cost of each statement and the number  of times each statement is \\nexecuted. For each i D 2;3;:::;n , let t i denote the number of times the while  \\nloop  test  in line  5 is executed  for  that  value  of i', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 50}),\n",
       "  '3e0b4e4c-020a-4f61-9aca-1afa78013b89': Document(page_content=' executed  for  that  value  of i . When a for  or while  loop exits \\nin the  usual  way4because  the  test  in the  loop  header  comes  up  FALSE4the  test  is \\nexecuted one time more than the loop body. Because comments are not executable \\nstatements, assume that they take no time. \\nThe running time of the algorithm is the sum of run ning times for each  state-  \\nment executed. A statement that takes c k steps to execute and executes m times \\ncontributes c k m to the total running time. 11  We usually denote the running time of \\nan algorithm on an input of size n by T.n/ . To compute T.n/ , the running time \\nof I NSERTION-SORT  on an input of n values, we sum the products of the cost  and \\ntimes columns, obtaining \\n10  There are some subtleties here. Computational steps  that we specify in English are often variants \\nof a procedure that requires more than just a const ant amount of time. For example, in the R ADIX- \\nS', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 50}),\n",
       "  '7644a339-c925-4d6c-8308-8170fc8b0169': Document(page_content=', in the R ADIX- \\nSORT  procedure  on  page  213,  one  line  reads  <use  a stable  sort  to sort array A on digit i ,= which, \\nas we shall see, takes more than a constant amount of time. Also, although a statement that calls a \\nsubroutine takes only constant time, the subroutine  itself, once invoked, may take more. That is, we \\nseparate the process of calling  the  subroutine4passing  parameters  to it, etc.4from  the  process of \\nexecuting  the subroutine. \\n11  This characteristic does not necessarily hold for a  resource such as memory. A statement that \\nreferences m words of memory and is executed n times does not necessarily reference mn  distinct \\nwords of memory. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 50}),\n",
       "  '977916d6-352e-464c-98a7-e2a4e09be0bd': Document(page_content='30 Chapter 2 Getting Started \\nI NSERTION-SORT  .A;n/  cost  times  \\n1 for  i D 2 to n c 1 n \\n2 key  D AŒi�  c 2 n \\ue003 1 \\n3 / / Insert AŒi�  into the sorted subarray AŒ1  W i \\ue003 1�. 0 n \\ue003 1 \\n4 j D i \\ue003 1 c 4 n \\ue003 1 \\n5 while  j >0  and AŒj�>  key  c 5 P  n \\ni D2 t i \\n6 AŒj  C 1� D AŒj�  c 6 P  n \\ni D2 .t i \\ue003 1/ \\n7 j D j \\ue003 1 c 7 P  n \\ni D2 .t i \\ue003 1/ \\n8 AŒj  C 1� D key  c 8 n \\ue003 1 \\nT.n/  D c 1 n C c 2 .n \\ue003 1/ C c 4 .n \\ue003 1', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 51}),\n",
       "  'ab80d803-7ca7-4d60-bddc-f70b86129fcb': Document(page_content='/ C c 4 .n \\ue003 1/ C c 5 n X  \\ni D2 t i C c 6 n X  \\ni D2 .t i \\ue003 1/ \\nC c 7 n X  \\ni D2 .t i \\ue003 1/ C c 8 .n \\ue003 1/:  \\nEven  for  inputs  of a given  size,  an algorithm’s  running  time  may depend on \\nwhich  input of that size is given. For example, in I NSERTION-SORT, the best case \\noccurs when the array is already sorted. In this ca se, each time  that  line  5 executes,  \\nthe value of key4the  value  originally  in AŒi�4is  already  greater  than  or equal  to \\nall values in AŒ1  W i \\ue003 1�, so that the while  loop  of lines  537  always  exits  upon  the  \\nûrst  test  in line  5. Therefore,  we  have  that  t i D 1 for i', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 51}),\n",
       "  '21813489-2949-4901-a09e-b5274ae75986': Document(page_content=' have  that  t i D 1 for i D 2;3;:::;n , and the \\nbest-case  running  time  is given  by  \\nT.n/  D c 1 n C c 2 .n \\ue003 1/ C c 4 .n \\ue003 1/ C c 5 .n \\ue003 1/ C c 8 .n \\ue003 1/ \\nD .c 1 C c 2 C c 4 C c 5 C c 8 /n \\ue003 .c 2 C c 4 C c 5 C c 8 /: (2.1)  \\nWe can express this running time as an  C b for constants  a and b that depend on \\nthe statement costs c k (where a D c 1 Cc 2 Cc 4 Cc 5 Cc 8 and b D c 2 Cc 4 Cc 5 Cc 8 ). \\nThe running time is thus a linear  function  of n. \\nThe worst case arises when the array is in reverse sorted order4that  is, it starts  \\nout in decreasing order. The procedure must compare  each element AŒi�  with each \\nelement in', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 51}),\n",
       "  '7367a4a3-294d-4f76-8b72-8b9129a39f8c': Document(page_content='�i�  with each \\nelement in the entire sorted subarray AŒ1  W i \\ue003 1�, and so t i D i for i D 2;3;:::;n . \\n(The  procedure  ûnds  that  AŒj�>  key  every  time  in line  5, and  the  while  loop exits \\nonly when j reaches 0.) Noting that \\nn X  \\ni D2 i D \\ue001 n X  \\ni D1 i ! \\n\\ue003 1 \\nD n.n  C 1/ \\n2 \\ue003 1 (by  equation  (A.2)  on  page  1141)  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 51}),\n",
       "  'b9ddce95-4495-433c-b0c1-e7f7da922d3c': Document(page_content='2.2  Analyzing  algorithms  31 \\nand \\nn X  \\ni D2 .i \\ue003 1/ D n\\ue0021 X  \\ni D1 i \\nD n.n  \\ue003 1/ \\n2 (again, by equation (A.2)) , \\nwe  ûnd  that  in the  worst  case,  the  running  time  of I NSERTION-SORT  is \\nT.n/  D c 1 n C c 2 .n \\ue003 1/ C c 4 .n \\ue003 1/ C c 5 Î n.n  C 1/ \\n2 \\ue003 1 Ï \\nC c 6 Î n.n  \\ue003 1/ \\n2 Ï \\nC c 7 Î n.n  \\ue003 1/ \\n2 Ï \\nC c 8 .n \\ue003 1/ \\nD \\ue002 c 5 \\n2 C c 6 \\n2 C c 7 \\n2 Í \\nn 2 C \\ue002 \\nc 1 C c 2 C c 4 C c 5 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 52}),\n",
       "  '3a8719d3-c8ca-4102-bc60-7425bb4c6549': Document(page_content=' C c 2 C c 4 C c 5 \\n2 \\ue003 c 6 \\n2 \\ue003 c 7 \\n2 C c 8 Í \\nn \\n\\ue003 .c 2 C c 4 C c 5 C c 8 /: (2.2) \\nWe  can  express  this  worst-case  running  time  as an  2 C bn  C c for constants a, b, \\nand c that again depend on the statement costs c k (now, a D c 5 =2  C c 6 =2  C c 7 =2, \\nb D c 1 C c 2 C c 4 C c 5 =2  \\ue003 c 6 =2  \\ue003 c 7 =2  C c 8 , and c D \\ue003.c 2 C c 4 C c 5 C c 8 /). The \\nrunning time is thus a quadratic  function  of n. \\nTypically, as in insertion sort, the running time o f an algorithm  is ûxed  for  a \\ngiven  input,  although  we’ll  also  see  some  interesting  <rand omized=', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 52}),\n",
       "  'cde7ad3e-6114-4f35-ad45-68a3ce185457': Document(page_content='  some  interesting  <rand omized= algorithms \\nwhose  behavior  can  vary  even  for  a ûxed  input.  \\nWorst-case  and  average-case  analysis  \\nOur  analysis  of insertion  sort  looked  at both  the  best  case,  in which the input array \\nwas already sorted, and the worst case, in which th e input array was reverse sorted. \\nFor  the  remainder  of this  book,  though,  we’ll  usually  (but  not always) concentrate \\non  ûnding  only  the  worst-case  running  time, that is, the longest running time for \\nany  input of size n. Why?  Here  are  three  reasons:  \\n\\ue001 The  worst-case  running  time  of an algorithm  gives  an upper  bound  on  the  run-  \\nning time for any  input. If you know it, then you have a guarantee th at the \\nalgorithm never takes any longer. You need not make  some educated guess \\nabout the running time', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 52}),\n",
       "  '10f7b0df-ce3e-425d-9622-77cb2753db87': Document(page_content='  some educated guess \\nabout the running time and hope that it never gets much worse. This feature is \\nespecially  important  for  real-time  computing,  in which  operations  must  com-  \\nplete by a deadline. \\n\\ue001 For some algorithms, the worst case occurs fairly o ften. For example,  in search-  \\ning a database for a particular piece of informatio n, the searching  algorithm’s  \\nworst case often occurs when the information is not  present in the database. In \\nsome applications, searches for absent information may be frequent. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 52}),\n",
       "  '969b179e-2ccb-4d88-bf61-4d733f0dcad5': Document(page_content='32 Chapter 2 Getting Started \\n\\ue001 The <average case= is often roughly as bad as the w orst case. Suppose that \\nyou run insertion sort on an array of n randomly chosen numbers. How long \\ndoes it take to determine where in subarray AŒ1  W i \\ue003 1� to insert element AŒi�? \\nOn  average,  half  the  elements  in AŒ1  W i \\ue003 1� are less than AŒi�, and half the \\nelements  are  greater.  On  average,  therefore,  AŒi�  is compared with just half \\nof the subarray AŒ1  W i \\ue003 1�, and so t i is about i=2. The  resulting  average-case  \\nrunning time turns out to be a quadratic function o f the input size, just like the \\nworst-case  running  time.  \\nIn some  particular  cases,  we’ll  be interested  in the  average-case  running time of \\nan algorithm.  We’ll  see  the', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 53}),\n",
       "  '40fae9ae-3989-46fc-86c0-7b4a3a765315': Document(page_content='.  We’ll  see  the  technique  of probabilistic  analysis  applied to various \\nalgorithms  throughout  this  book.  The  scope  of average-case  analysis is limited, \\nbecause it may not be apparent what constitutes an <average= input for a particular \\nproblem.  Often,  we’ll  assume  that  all  inputs  of a given  size  are equally likely. In \\npractice, this assumption may be violated, but we c an sometimes use a randomized  \\nalgorithm , which makes random choices, to allow a probabilis tic analysis and yield \\nan expected  running time. We explore randomized algorithms more  in Chapter  5 \\nand in several other subsequent chapters. \\nOrder  of growth  \\nIn order to ease our analysis of the I NSERTION-SORT  procedure, we used some \\nsimplifying abstractions. First, we ignored the act ual cost of each statement, using \\nthe constants c k to represent  these  costs.  Still,  the  best-case  and  worst-case  run-  \\nning  times ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 53}),\n",
       "  'c313fbd8-4437-4754-b76a-7c415bed01ad': Document(page_content='  run-  \\nning  times  in equations  (2.1)  and  (2.2)  are  rather  unwieldy.  The constants in these \\nexpressions  give  us more  detail  than  we  really  need.  That’s  why we also expressed \\nthe  best-case  running  time  as an  C b for constants a and b that  depend  on  the  state-  \\nment costs c k and  why  we  expressed  the  worst-case  running  time  as an  2 C bn  C c \\nfor constants a, b, and c that depend on the statement costs. We thus ignored  not \\nonly the actual statement costs, but also the abstr act costs c k . \\nLet’s  now  make  one  more  simplifying  abstraction:  it is the  rate  of growth , or \\norder  of growth , of the running time that really interests us. We therefore consider \\nonly the leading term of a formula (e.g., an  2 ), since  the  lower-order  terms  are', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 53}),\n",
       "  '66e000e0-c3af-4291-9e41-c3e66f1792cd': Document(page_content='  the  lower-order  terms  are  rela-  \\ntively  insigniûcant  for  large  values  of n. We  also  ignore  the  leading  term’s  constant  \\ncoefûcient,  since  constant  factors  are  less  signiûcant  than  the  rate  of growth  in de-  \\ntermining  computational  efûciency  for  large  inputs.  For  insertion  sort’s  worst-case  \\nrunning  time,  when  we  ignore  the  lower-order  terms  and  the  leading  term’s  con-  \\nstant  coefûcient,  only  the  factor  of n 2 from the leading term remains. That factor, \\nn 2 , is by far the most important part of the running time. For example, suppose that \\nan algorithm implemented on a particular machine ta kes n 2 =100  C 100n  C 17  mi-  \\ncroseconds on an input of size n. Although  the  coefûcients  of 1', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 53}),\n",
       "  '2aea8539-48b3-402a-a136-0d189c897f98': Document(page_content='  the  coefûcients  of 1=100  for the n 2 term \\nand 100  for the n term differ by four orders of magnitude, the n 2 =100  term  domi-  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 53}),\n",
       "  '87c91b95-06df-488e-b45b-8de3ea08a163': Document(page_content='2.2  Analyzing  algorithms  33 \\nnates the 100n  term once n exceeds  10,000.  Although  10,000  might  seem  large,  it \\nis smaller  than  the  population  of an average  town.  Many  real-world problems have \\nmuch larger input sizes. \\nTo highlight the order of growth of the running tim e, we have a special notation \\nthat  uses  the  Greek  letter  ‚ (theta).  We  write  that  insertion  sort  has  a worst-case  \\nrunning time of ‚.n  2 / (pronounced <theta of n-squared=  or just  <theta  n-squared=).  \\nWe  also  write  that  insertion  sort  has  a best-case  running  time of ‚.n/  (<theta of n= \\nor <theta n=). For now, think of ‚-notation  as saying  <roughly  proportional  when  \\nn is large,= so that ‚.n  2 / means <roughly proportional', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 54}),\n",
       "  'a3ce7ee1-35b7-4531-b069-4b6eae429906': Document(page_content='.n  2 / means <roughly proportional to n 2 when n is large= and \\n‚.n/  means <roughly proportional to n when n is large=  We’ll  use  ‚-notation  \\ninformally  in this  chapter  and  deûne  it precisely  in Chapter  3. \\nWe  usually  consider  one  algorithm  to be more  efûcient  than  another  if its  worst-  \\ncase running time has a lower order of growth. Due to constant factors  and  lower-  \\norder terms, an algorithm whose running time has a higher order of growth might \\ntake less time for small inputs than an algorithm w hose running  time  has  a lower  or-  \\nder of growth. But on large enough inputs, an algor ithm whose worst-case  running  \\ntime is ‚.n  2 /, for example, takes less time in the worst case th an an algorithm \\nwhose  worst-case  running  time  is ‚.n  3 /. Regardless of the constants hidden by \\nthe ‚-notation', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 54}),\n",
       "  'ec1b6132-2334-42a9-9868-b2ddace77906': Document(page_content=' constants hidden by \\nthe ‚-notation,  there  is always  some  number,  say  n 0 , such that for all input sizes \\nn \\ue004 n 0 , the ‚.n  2 / algorithm beats the ‚.n  3 / algorithm in the worst case. \\nExercises  \\n2.2-1  \\nExpress the function n 3 =1000  C 100n  2 \\ue003 100n  C 3 in terms of ‚-notation.  \\n2.2-2  \\nConsider sorting n numbers stored in array AŒ1  W n� by  ûrst  ûnding  the  smallest  \\nelement of AŒ1  W n� and exchanging it with the element in AŒ1�. Then  ûnd  the  \\nsmallest element of AŒ2  W n�, and exchange it with AŒ2�. Then  ûnd  the  smallest  \\nelement of AŒ3  W n�, and exchange it with AŒ3�. Continue in this manner for the \\nûrst ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 54}),\n",
       "  '3af6f2ad-135f-4d3c-8d26-187e975dec38': Document(page_content=' this manner for the \\nûrst  n \\ue003 1 elements of A. Write pseudocode for this algorithm, which is kno wn \\nas selection  sort. What  loop  invariant  does  this  algorithm  maintain?  Why  does  it \\nneed  to run  for  only  the  ûrst  n \\ue003 1 elements, rather than for all n elements?  Give  the  \\nworst-case  running  time  of selection  sort  in ‚-notation.  Is the  best-case  running  \\ntime  any  better?  \\n2.2-3  \\nConsider  linear  search  again  (see  Exercise  2.1-4).  How  many  elements of the input \\narray need to be checked on the average, assuming t hat the element being searched \\nfor  is equally  likely  to be any  element  in the  array?  How  about  in the  worst  case?  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 54}),\n",
       "  'b242c380-b7c1-4860-9827-c8b132a9c77e': Document(page_content='34 Chapter 2 Getting Started \\nUsing ‚-notation,  give  the  average-case  and  worst-case  running  times of linear \\nsearch.  Justify  your  answers.  \\n2.2-4  \\nHow  can  you  modify  any  sorting  algorithm  to have  a good  best-case  running  time?  \\n2.3  Designing  algorithms  \\nYou can choose from a wide range of algorithm desig n techniques. Insertion sort \\nuses the incremental  method: for each element AŒi�, insert it into its proper place \\nin the subarray AŒ1  W i�, having already sorted the subarray AŒ1  W i \\ue003 1�. \\nThis section examines another design method, known as <divide-and-conquer,=  \\nwhich  we  explore  in more  detail  in Chapter  4. We’ll  use  divide-and-conquer  to \\ndesign  a sorting  algorithm  whose  worst-case  running  time  is much less than that \\nof insertion ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 55}),\n",
       "  'bc692052-52ed-4a65-aa92-56ee6a285b9b': Document(page_content=' is much less than that \\nof insertion  sort.  One  advantage  of using  an algorithm  that  follows  the  divide-and-  \\nconquer method is that analyzing its running time i s often straightforward, using \\ntechniques  that  we’ll  explore  in Chapter  4. \\n2.3.1  The  divide-and-conquer  method  \\nMany useful algorithms are recursive  in structure: to solve a given problem, they \\nrecurse  (call themselves) one or more times to handle close ly related subprob-  \\nlems. These algorithms typically follow the divide-and-conquer  method: they \\nbreak the problem into several subproblems that are  similar to the  original  prob-  \\nlem but smaller in size, solve the subproblems recu rsively, and then combine these \\nsolutions to create a solution to the original prob lem. \\nIn the  divide-and-conquer  method,  if the  problem  is small  enough4the  base  \\ncase4you  just  solve  it directly  without  recursing.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 55}),\n",
       "  '70a3aa0b-28b4-4ea5-b389-12ca75d9abc8': Document(page_content='  it directly  without  recursing.  Otherwise4t he recursive  case  \\n4you  perform  three  characteristic  steps:  \\nDivide  the problem into one or more subproblems that are s maller instances of the \\nsame problem. \\nConquer  the subproblems by solving them recursively. \\nCombine  the subproblem solutions to form a solution to the original problem. \\nThe merge  sort  algorithm  closely  follows  the  divide-and-conquer  method.  In \\neach step, it sorts a subarray AŒp  W r�, starting with the entire array AŒ1  W n� and \\nrecursing down to smaller and smaller subarrays. He re is how merge sort operates: ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 55}),\n",
       "  'e0de6a34-2f31-4865-abc3-cf26b5342711': Document(page_content='2.3 Designing algorithms 35 \\nDivide  the subarray AŒp  W r� to be sorted into two adjacent subarrays, each of h alf \\nthe size. To do so, compute the midpoint q of AŒp  W r� (taking the average of p \\nand r ), and divide AŒp  W r� into subarrays AŒp  W q� and AŒq  C 1 W r�. \\nConquer  by sorting each of the two subarrays AŒp  W q� and AŒq  C 1 W r� recursively \\nusing merge sort. \\nCombine  by merging the two sorted subarrays AŒp  W q� and AŒq  C 1 W r� back into \\nAŒp  W r�, producing the sorted answer. \\nThe  recursion  <bottoms  out=4it  reaches  the  base  case4when  the subarray AŒp  W r� \\nto be sorted has just 1 element, that is, when p equals r . As  we', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 56}),\n",
       "  '21e3492d-c5fa-487a-819a-0f5693241834': Document(page_content=' is, when p equals r . As  we  noted  in the  ini-  \\ntialization argument for I NSERTION-SORT’s loop  invariant,  a subarray  comprising  \\njust a single element is always sorted. \\nThe key operation of the merge sort algorithm occur s in the <combine= step, \\nwhich merges two adjacent, sorted subarrays. The me rge operation is performed \\nby the auxiliary procedure M ERGE.A;p;q;r/  on the following page, where A is \\nan array and p, q, and r are indices into the array such that p හ q < r  . The \\nprocedure assumes that the adjacent subarrays AŒp  W q� and AŒq  C 1 W r� were  al-  \\nready recursively sorted. It merges  the two sorted subarrays to form a single sorted \\nsubarray that replaces the current subarray AŒp  W r�. \\nTo understand how the M ERGE  procedure  works,  let’s  return  to', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 56}),\n",
       "  '7e13d818-1803-4c15-a213-57e07003ba4a': Document(page_content=',  let’s  return  to our  card-playing  \\nmotif. Suppose that you have two piles of cards fac e up on a table. Each pile is \\nsorted,  with  the  smallest-value  cards  on  top.  You  wish  to merge the two piles \\ninto a single sorted output pile, which is to be fa ce down on the table. The basic \\nstep consists of choosing the smaller of the two ca rds on top of the  face-up  piles,  \\nremoving  it from  its  pile4which  exposes  a new  top  card4and  placing this card \\nface down onto the output pile. Repeat this step un til one input pile is empty, at \\nwhich  time  you  can  just  take  the  remaining  input  pile  and  üip  over the entire pile, \\nplacing it face down onto the output pile. \\nLet’s  think  about  how  long  it takes  to merge  two  sorted  piles  of cards. Each basic \\nstep takes constant time, since you are comparing j u', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 56}),\n",
       "  '495db8c6-57df-4f3e-95ff-7673438fddf5': Document(page_content=' takes constant time, since you are comparing j ust the two top cards. If the two \\nsorted piles that you start with each have n=2  cards, then the number of basic steps \\nis at least n=2  (since in whichever pile was emptied, every card wa s found to be \\nsmaller than some card from the other pile) and at most n (actually, at most n \\ue003 1, \\nsince after n \\ue003 1 basic steps, one of the piles must be empty). With each basic step \\ntaking constant time and the total number of basic steps being between n=2  and n, \\nwe can say that merging takes time roughly proporti onal to n. That is, merging \\ntakes ‚.n/  time. \\nIn detail, the M ERGE  procedure works as follows. It copies the two subar rays \\nAŒp  W q� and AŒq  C 1 W r� into temporary arrays L and R (<left= and <right=), and \\nthen it merges the values in L and R back into AŒp  W r�. Lines  1 and', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 56}),\n",
       "  '62bb5f15-f004-47eb-a7d7-40658da2b4c3': Document(page_content='p  W r�. Lines  1 and  2 compute  the  \\nlengths n L and n R of the subarrays AŒp  W q� and AŒq  C 1 W r�, respectively. Then ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 56}),\n",
       "  '057c99b0-d5d6-4a11-9223-a70a4f06406c': Document(page_content='36 Chapter 2 Getting Started \\nMERGE.A;p;q;r/  \\n1 n L D q \\ue003 p C 1 / / length of AŒp  W q� \\n2 n R D r \\ue003 q / / length of AŒq  C 1 W r� \\n3 let LŒ0  W n L \\ue003 1� and RŒ0  W n R \\ue003 1� be new arrays \\n4 for  i D 0 to n L \\ue003 1 / / copy AŒp  W q� into LŒ0  W n L \\ue003 1� \\n5 LŒi�  D AŒp  C i� \\n6 for  j D 0 to n R \\ue003 1 / / copy AŒq  C 1 W r� into RŒ0  W n R \\ue003 1� \\n7 RŒj�  D AŒq  C j C 1� \\n8 i D 0 / / i indexes the smallest remaining element in L \\n9 j D 0 / / j indexes the smallest remaining element in R ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 57}),\n",
       "  'adf7b760-c78b-4f4d-a5f6-2c5f961d7c7d': Document(page_content=' / j indexes the smallest remaining element in R \\n10  k D p / / k indexes the location in A to ûll  \\n11  / / As long as each of the arrays L and R contains an unmerged element, \\n/ / copy the smallest unmerged element back into AŒp  W r�. \\n12  while  i<n  L and j <n  R \\n13  if LŒi�  හ RŒj�  \\n14  AŒk�  D LŒi�  \\n15  i D i C 1 \\n16  else  AŒk�  D RŒj�  \\n17  j D j C 1 \\n18  k D k C 1 \\n19  / / Having gone through one of L and R entirely, copy the \\n/ / remainder of the other to the end of AŒp  W r�. \\n20 while  i<n  L \\n21  AŒk�  D LŒi�  \\n22 i D i C 1 \\n23  k D k', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 57}),\n",
       "  '2918e699-2f76-4ee8-bf41-d40b4a8353eb': Document(page_content=' i C 1 \\n23  k D k C 1 \\n24  while  j <n  R \\n25  AŒk�  D RŒj�  \\n26  j D j C 1 \\n27  k D k C 1 \\nline  3 creates  arrays  LŒ0  W n L \\ue003 1� and RŒ0  W n R \\ue003 1� with respective lengths n L \\nand n R . 12  The for  loop  of lines  435  copies  the  subarray  AŒp  W q� into L, and the for  \\nloop  of lines  637  copies  the  subarray  AŒq  C 1 W r� into R. \\nLines  8318,  illustrated  in Figure  2.3,  perform  the  basic  steps. The while  loop \\nof lines  12318  repeatedly  identiûes  the  smallest  value  in L and R that has yet to \\n12  This procedure is the rare case that uses both 1-origin  indexing  (for  array  A)', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 57}),\n",
       "  'f9efc6df-240f-4d64-a622-16d6a5219f09': Document(page_content=' indexing  (for  array  A) and 0-origin  indexing  \\n(for arrays L and R). Using 0-origin  indexing  for  L and R makes for a simpler loop invariant in \\nExercise  2.3-3.  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 57}),\n",
       "  '2c0b3303-7259-431f-891a-20e660725414': Document(page_content='2.3 Designing algorithms 37 \\nA \\nL R 1 2 3 \\ni j k \\n(a) 2 4 6 7 1 2 3 5 A \\nL R \\ni j k \\n(b) 2 4 6 7 1 \\n2 3 5 1 2 4 6 7 1 2 3 5 4 6 7 1 2 3 5 \\nA \\nL R 9 10  11  12  13  14  15  16  \\ni j k \\n(c) 2 4 6 7 1 \\n2 3 5 1 6 7 1 2 3 5 2 A \\nL R \\ni j k \\n(d) 2 4 6 7 1 \\n2 3 5 1 7 1 2 3 5 2 2 9 10  11  12  13  14  15  16  \\n9 10  11  12  13  14  15  16  9 10  11  12  13  14  15  16  8 \\n… 17  \\n… \\n8 \\n… 17  \\n… 8 \\n… 17  \\n… \\n8 \\n… 17  \\n… 1 2 3 0 1 2 3 1 2 3', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 58}),\n",
       "  '3ed0bb8c-cfb7-4317-a1f5-2a1f3d01d917': Document(page_content=' 1 2 3 0 1 2 3 1 2 3 \\n1 2 3 1 2 3 1 2 3 1 2 3 0 0 \\n0 0 \\n0 0 0 \\nA \\nL R 1 2 3 \\ni j k \\n(e) 2 4 6 7 1 \\n2 3 5 1 1 2 3 5 2 2 3 A \\nL R \\ni j k \\n(f) 2 4 6 7 1 \\n2 3 5 1 2 3 5 2 2 3 4 \\nA \\nL R \\ni j k \\n(g) 2 4 6 7 1 \\n2 3 5 1 3 5 2 2 3 4 5 A \\nL R 1 2 3 4 1 2 3 4 \\ni j k \\n(h) 2 4 6 7 1 \\n2 3 5 1 7 2 2 3 4 5 6 9 10  11  12  13  14  15  16  \\n9 10  11  12  13  14  15  16  9 10  11  12  13  14  15  16  9 10  11  12  13  14  15  16  8 \\n… 17  \\n… \\n8', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 58}),\n",
       "  'b2eafe9e-3219-440a-bd6b-4f763ddb18f0': Document(page_content='\\n… 17  \\n… \\n8 \\n… 17  \\n… 8 \\n… 17  \\n… 8 \\n… 17  \\n… \\n1 2 3 1 2 3 1 2 3 \\n1 2 3 1 2 3 0 0 \\n0 4 0 0 0 0 0 \\nFigure  2.3  The operation of the while  loop  in lines  8318  in the  call  MERGE.A;9;12;16/ , when \\nthe subarray AŒ9  W 16�  contains the values h2;4;6;7;1;2;3;5 i. After allocating and copying into \\nthe arrays L and R, the array L contains h2;4;6;7 i, and the array R contains h1;2;3;5 i. Tan \\npositions in A contain  their  ûnal  values,  and  tan  positions  in L and R contain values that have yet \\nto be copied back into A. Taken together, the tan positions always comprise  the values originally \\nin AŒ9  W 16�. Blue positions in A contain', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 58}),\n",
       "  '33ca3816-71c9-421b-875d-b6accdd2eb44': Document(page_content='  W 16�. Blue positions in A contain values that will be copied over, and dark p ositions in L \\nand R contain values that have already been copied back i nto A. (a)–(g)  The arrays A, L, and R, and \\ntheir respective indices k, i , and j prior  to each  iteration  of the  loop  of lines  12318.  At  the  point in \\npart (g), all values in R have been copied back into A (indicated by j equaling the length of R), and \\nso the while  loop  in lines  12318  terminates.  (h)  The arrays and indices at termination. The while  \\nloops  of lines  20323  and  24327  copied  back  into  A the remaining values in L and R, which are the \\nlargest values originally in AŒ9  W 16�. Here,  lines  20323  copied  LŒ2  W 3� into AŒ15  W 16�, and because \\nall values in R had already been copied back into A, the while  loop  of lines', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 58}),\n",
       "  '0b58a51d-e91e-47b3-b9fb-e5381a8a1269': Document(page_content=' into A, the while  loop  of lines  24327  iterated  0 times. \\nAt this point, the subarray in AŒ9  W 16�  is sorted. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 58}),\n",
       "  'e4bd4c83-baea-410f-97f8-12a0949dfb1d': Document(page_content='38 Chapter 2 Getting Started \\nbe copied back into AŒp  W r� and copies it back in. As the comments indicate, th e \\nindex k gives the position of A that  is being  ûlled  in,  and  the  indices  i and j give the \\npositions in L and R, respectively, of the smallest remaining values. E ventually, \\neither all of L or all of R is copied back into AŒp  W r�, and this loop terminates. \\nIf the loop terminates because all of R has been copied back, that is, because j \\nequals n R , then i is still less than n L , so that some of L has yet to be copied back, \\nand these values are the greatest in both L and R. In this case, the while  loop \\nof lines  20323  copies  these  remaining  values  of L into the last few positions of \\nAŒp  W r�. Because j equals n R , the while  loop  of lines  24327  iterates  0 times. If \\ninstead the while  loop  of lines  123', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 59}),\n",
       "  '94cf4966-4858-454b-9020-f102876f6a9c': Document(page_content='instead the while  loop  of lines  12318  terminates  because  i equals n L , then all of L \\nhas already been copied back into AŒp  W r�, and the while  loop  of lines  24327  copies  \\nthe remaining values of R back into the end of AŒp  W r�. \\nTo see that the M ERGE  procedure runs in ‚.n/  time, where n D r \\ue003 p C 1, 13  \\nobserve  that  each  of lines  133  and  8310  takes  constant  time,  and the for  loops \\nof lines  437  take  ‚.n  L C n R / D ‚.n/  time. 14  To account for the three while  \\nloops  of lines  12318,  20323,  and  24327,  observe  that  each  iteration of these loops \\ncopies exactly one value from L or R back into A and that every value is copied \\nback into A exactly once. Therefore, these three loops together  make a total of n \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 59}),\n",
       "  '561e41d2-1fdf-4494-bd4b-503ce9fa9641': Document(page_content=' loops together  make a total of n \\niterations. Since each iteration of each of the thr ee loops takes constant time, the \\ntotal time spent in these three loops is ‚.n/ . \\nWe can now use the M ERGE  procedure  as a subroutine  in the  merge  sort  al-  \\ngorithm. The procedure M ERGE-SORT.A;p;r/  on  the  facing  page  sorts  the  ele-  \\nments in the subarray AŒp  W r�. If p equals r , the subarray has just 1 element and \\nis therefore  already  sorted.  Otherwise,  we  must  have  p < r  , and MERGE-SORT  \\nruns the divide, conquer, and combine steps. The di vide step simply computes an \\nindex q that partitions AŒp  W r� into two adjacent subarrays: AŒp  W q�, containing \\ndn=2e elements, and AŒq  C 1 W r�, containing bn=2c elements. 15  The initial call \\nMER', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 59}),\n",
       "  '4bdeda97-476e-4545-94dd-c969a3be09fb': Document(page_content=' elements. 15  The initial call \\nMERGE-SORT  .A;1;n/  sorts the entire array AŒ1  W n�. \\nFigure  2.4  illustrates  the  operation  of the  procedure  for  n D 8, showing also the \\nsequence of divide and merge steps. The algorithm r ecursively divides the array \\ndown to 1-element  subarrays.  The  combine  steps  merge  pairs  of 1-element  subar-  \\n13  If you’re  wondering  where  the  <C1= comes from, imagine that r D p C 1. Then  the  subar-  \\nray AŒp  W r� consists of two elements, and r \\ue003 p C 1 D 2. \\n14  Chapter  3 shows  how  to formally  interpret  equations  contain ing ‚-notation.  \\n15  The expression dxe denotes the least integer greater than or equal to x, and bxc denotes the \\ngreatest integer less than or equal to x. These  notations  are  deûned  in Section ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 59}),\n",
       "  '216732c0-139f-4b43-8707-d36fba773800': Document(page_content='  are  deûned  in Section  3.3.  The  easiest  way  \\nto verify that setting q to b.p  C r/=2c yields subarrays AŒp  W q� and AŒq  C 1 W r� of sizes dn=2e and \\nbn=2c, respectively, is to examine the four cases that a rise depending on whether each of p and r is \\nodd or even. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 59}),\n",
       "  '96fde5f9-938f-4e55-b6af-db6021516a73': Document(page_content='2.3 Designing algorithms 39 \\nMERGE-SORT.A;p;r/  \\n1 if p \\ue004 r / / zero  or one  element?  \\n2 return  \\n3 q D b.p  C r/=2c / / midpoint of AŒp  W r� \\n4 MERGE-SORT  .A;p;q/  / / recursively sort AŒp  W q� \\n5 MERGE-SORT  .A;q  C 1;r/  / / recursively sort AŒq  C 1 W r� \\n6 / / Merge AŒp  W q� and AŒq  C 1 W r� into AŒp  W r�. \\n7 MERGE.A;p;q;r/  \\nrays to form sorted subarrays of length 2, merges those to form sorted subarrays \\nof length 4, and  merges  those  to form  the  ûnal  sorted  subarray  of length  8. If n \\nis not an exact power of', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 60}),\n",
       "  '85c60090-393d-43a2-b96c-573e2befffbc': Document(page_content=' If n \\nis not an exact power of 2, then some divide steps create subarrays whose len gths \\ndiffer by 1. (For example, when dividing a subarray of length 7, one subarray has \\nlength 4 and the other has length 3.) Regardless of the lengths of the two subarrays \\nbeing merged, the time to merge a total of n items is ‚.n/ . \\n2.3.2  Analyzing  divide-and-conquer  algorithms  \\nWhen an algorithm contains a recursive call, you ca n often describe its running \\ntime by a recurrence  equation  or recurrence , which describes the overall running \\ntime on a problem of size n in terms of the running time of the same algorithm on \\nsmaller inputs. You can then use mathematical tools  to solve the recurrence and \\nprovide bounds on the performance of the algorithm.  \\nA recurrence  for  the  running  time  of a divide-and-conquer  algorithm falls out \\nfrom the three steps of the basic method. As we did  for insertion sort, let T.n/  \\nbe the  worst-', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 60}),\n",
       "  '73003ece-fb74-411b-a794-cd7e184d6fe3': Document(page_content='n/  \\nbe the  worst-case  running  time  on  a problem  of size  n. If the problem size is \\nsmall enough, say n<n  0 for some constant n 0 >0, the straightforward solution \\ntakes constant time, which we write as ‚.1/ . 16  Suppose that the division of the \\nproblem yields a subproblems, each with size n=b, that is, 1=b  the size of the \\noriginal. For merge sort, both a and b are 2, but  we’ll  see  other  divide-and-conquer  \\nalgorithms in which a ¤ b. It takes T.n=b/  time to solve one subproblem of \\nsize n=b, and so it takes aT.n=b/  time to solve all a of them. If it takes D.n/  time \\nto divide the problem into subproblems and C.n/  time to combine the solutions to \\nthe subproblems into the solution to the original p roblem, we get the recurrence \\n16  If you’re  wondering', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 60}),\n",
       "  'aeb24690-fead-4a6e-a84b-d7c5ea2af609': Document(page_content='\\n16  If you’re  wondering  where  ‚.1/  comes from, think of it this way. When we say that n 2 =100  \\nis ‚.n  2 /, we  are  ignoring  the  coefûcient  1=100  of the factor n 2 . Likewise, when we say that a \\nconstant c is ‚.1/, we  are  ignoring  the  coefûcient  c of the factor 1 (which you can also think of \\nas n 0 ). ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 60}),\n",
       "  '82a07b37-8946-4abc-86f0-7dcf5d6a1a64': Document(page_content='40 Chapter 2 Getting Started \\n12  3 7 9 14  6 11  2 1 2 3 4 5 6 7 8 \\n12  3 7 9 14  6 11  2 1 2 3 4 5 6 7 8 p q r \\np q r p q r \\n12  3 7 9 1 2 3 4 p,q r \\n3 1 2 p,r \\n12  3 1 2 p,q r divide \\ndivide \\ndivide \\nmerge 1 \\n2 \\n3 \\n5 6 \\n4 11 \\np,q r \\n14  6 11  2 5 6 7 8 p,q r 12 16 \\np,q r \\np,r \\n12  9 3 4 p,r 7 8 \\np,r \\n7 6 5 6 p,r 13 14 \\np,r \\n14  2 7 8 p,r 17 18 \\np,r \\n11  \\n9 7 3 4 p,q r 9 \\n14  6 5 6 p,q r 15 \\n11  2 7 8 p,q r 19 \\nmerge \\n3 7 9 12  2', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 61}),\n",
       "  'cea2ef7a-e598-4745-8f6d-d13a406ede63': Document(page_content='merge \\n3 7 9 12  2 6 11  14  1 2 3 4 5 6 7 8 p q r p q r 10 \\n2 3 6 7 9 11  12  14  1 2 3 4 5 6 7 8 p q r merge 21 20 \\nFigure  2.4  The operation of merge sort on the array A with length 8 that initially contains the \\nsequence h12;3;7;9;14;6;11;2 i. The indices p, q, and r into each subarray appear above their \\nvalues. Numbers in italics indicate the order in wh ich the MERGE-SORT  and MERGE  procedures are \\ncalled following the initial call of M ERGE-SORT.A;1;8/ . \\nT.n/  D ( \\n‚.1/  if n<n  0 ; \\nD.n/  C aT.n=b/  C C.n/  otherwise : \\nChapter  4 shows  how  to solve  common  recurrences  of this  form.  \\nSometimes, the n=b  size  of the  divide  step', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 61}),\n",
       "  'a5fb43c8-4496-4d93-8972-6e38503e5eb8': Document(page_content='b  size  of the  divide  step  isn’t  an integer.  For  example,  the  \\nMERGE-SORT  procedure divides a problem of size n into subproblems of sizes \\ndn=2e and bn=2c. Since the difference between dn=2e and bn=2c is at most 1, ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 61}),\n",
       "  'b03a3ead-190f-48b2-a2c1-c48cc7c30781': Document(page_content='2.3 Designing algorithms 41 \\nwhich for large n is much smaller than the effect of dividing n by 2, we’ll  squint  a \\nlittle and just call them both size n=2. As  Chapter  4 will  discuss,  this  simpliûcation  \\nof ignoring  üoors  and  ceilings  does  not  generally  affect  the  order of growth of a \\nsolution  to a divide-and-conquer  recurrence.  \\nAnother  convention  we’ll  adopt  is to omit  a statement  of the  base cases of the \\nrecurrence,  which  we’ll  also  discuss  in more  detail  in Chapter  4. The  reason  is \\nthat the base cases are pretty much always T.n/  D ‚.1/  if n < n  0 for some \\nconstant n 0 >0. That’s  because  the  running  time  of an algorithm  on  an input  of \\nconstant size is constant. We save ourselves a lot of extra writing by adopting this \\nconvention.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 62}),\n",
       "  '78bfa5ad-e864-46bd-999a-193bb265135d': Document(page_content=' extra writing by adopting this \\nconvention. \\nAnalysis  of merge  sort  \\nHere’s  how  to set  up  the  recurrence  for  T.n/, the  worst-case  running  time  of merge  \\nsort on n numbers. \\nDivide:  The divide step just computes the middle of the sub array, which takes \\nconstant time. Thus, D.n/  D ‚.1/ . \\nConquer:  Recursively solving two subproblems, each of size n=2, contributes \\n2T.n=2/  to the  running  time  (ignoring  the  üoors  and  ceilings,  as we  discussed). \\nCombine:  Since the M ERGE  procedure on an n-element  subarray  takes  ‚.n/  \\ntime, we have C.n/  D ‚.n/ . \\nWhen we add the functions D.n/  and C.n/  for the merge sort analysis, we are \\nadding a function that is ‚.n/  and a function', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 62}),\n",
       "  '1ff61387-ed5b-4379-b317-a5f35026095a': Document(page_content=' is ‚.n/  and a function that is ‚.1/ . This sum is a linear \\nfunction of n. That is, it is roughly proportional to n when n is large, and so \\nmerge  sort’s  dividing  and  combining  times  together  are  ‚.n/ . Adding ‚.n/  to \\nthe 2T.n=2/  term from the conquer step gives the recurrence for  the worst-case  \\nrunning time T.n/  of merge sort: \\nT.n/  D 2T.n=2/  C ‚.n/:  (2.3)  \\nChapter  4 presents  the  <master  theorem,=  which  shows  that  T.n/  D ‚.n  lg n/. 17  \\nCompared  with  insertion  sort,  whose  worst-case  running  time is ‚.n  2 /, merge sort \\ntrades away a factor of n for a factor of lg n. Because the logarithm function grows \\nmore  slowly  than  any ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 62}),\n",
       "  'adc46fc9-b078-430b-81f0-ca75da61678f': Document(page_content=' \\nmore  slowly  than  any  linear  function,  that’s  a good  trade.  For large enough inputs, \\nmerge sort, with its ‚.n  lg n/ worst-case  running  time,  outperforms  insertion  sort,  \\nwhose  worst-case  running  time  is ‚.n  2 /. \\n17  The notation lg n stands for log 2 n, although  the  base  of the  logarithm  doesn’t  matter  here,  but  as \\ncomputer scientists, we like logarithms base 2. Section  3.3  discusses  other  standard  notation.  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 62}),\n",
       "  'ec201156-ae74-459b-b7bf-5615bacd9d0c': Document(page_content='42 Chapter 2 Getting Started \\nWe do not need the master theorem, however, to unde rstand intuitively why the \\nsolution  to recurrence  (2.3)  is T.n/  D ‚.n  lg n/. For simplicity, assume that n is \\nan exact power of 2 and that the implicit base case is n D 1. Then  recurrence  (2.3)  \\nis essentially \\nT.n/  D ( \\nc 1 if n D 1;  \\n2T.n=2/  C c 2 n if n>1;  (2.4)  \\nwhere the constant c 1 >0  represents the time required to solve a problem of size 1, \\nand c 2 >0  is the time per array element of the divide and com bine steps. 18  \\nFigure  2.5  illustrates  one  way  of ûguring  out  the  solution  to recurrence  (2.4).  \\nPart  (a)  of the  ûgure  shows  T.n/ , which part (b) expands into an equivalent', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 63}),\n",
       "  'f720680b-a78e-4e5a-8cbe-3f400fabb4c3': Document(page_content=' , which part (b) expands into an equivalent tree \\nrepresenting the recurrence. The c 2 n term  denotes  the  cost  of dividing  and  com-  \\nbining at the top level of recursion, and the two s ubtrees of the root are the two \\nsmaller recurrences T.n=2/ . Part (c) shows this process carried one step furt her by \\nexpanding T.n=2/ . The cost for dividing and combining at each of th e two nodes \\nat the second level of recursion is c 2 n=2. Continue to expand each node in the tree \\nby breaking it into its constituent parts as determ ined by the recurrence, until the \\nproblem sizes get down to 1, each with a cost of c 1 . Part (d) shows the resulting \\nrecursion  tree. \\nNext, add the costs across each level of the tree. The top level has total cost c 2 n, \\nthe next level down has total cost c 2 .n=2/  C c 2 .n=2/  D c 2 n, the level after that has \\ntotal cost', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 63}),\n",
       "  '49db2f54-17c3-4f30-8e86-e7d1adbdcd77': Document(page_content=', the level after that has \\ntotal cost c 2 .n=4/  C c 2 .n=4/  C c 2 .n=4/  C c 2 .n=4/  D c 2 n, and so on. Each level \\nhas twice as many nodes as the level above, but eac h node contributes only half \\nthe cost of a node from the level above. From one l evel to the next, doubling and \\nhalving cancel each other out, so that the cost acr oss each level is the same: c 2 n. In \\ngeneral, the level that is i levels below the top has 2 i nodes, each contributing a cost \\nof c 2 .n=2  i /, so that the i th level below the top has total cost 2 i \\ue001 c 2 .n=2  i / D c 2 n. \\nThe bottom level has n nodes, each contributing a cost of c 1 , for a total cost of c 1 n. \\nThe  total  number  of levels  of the  recursion  tree  in Figure  2.5  is lg n C 1, where \\nn', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 63}),\n",
       "  'bb8ac3be-ad5a-4d5b-b66c-39743f7c60e7': Document(page_content=' lg n C 1, where \\nn is the number of leaves, corresponding to the input  size. An informal inductive \\nargument  justiûes  this  claim.  The  base  case  occurs  when  n D 1, in which case \\nthe tree has only 1 level. Since lg 1 D 0, we have that lg n C 1 gives the correct \\nnumber of levels. Now assume as an inductive hypoth esis that the number of levels \\nof a recursion tree with 2 i leaves is lg 2 i C 1 D i C 1 (since for any value of i , we \\nhave that lg 2 i D i ). Because we assume that the input size is an exac t power of 2, \\nthe next input size to consider is 2 i C1 . A tree with n D 2 i C1 leaves has 1 more \\n18  It is unlikely that c 1 is exactly the time to solve problems of size 1 and that c 2 n is exactly the \\ntime  of the  divide  and  combine  steps.  We’ll  look  more  closely  at bounding  recurrences  in Chapter', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 63}),\n",
       "  '69a271a3-83a6-4e5f-92fd-032dce6c56c9': Document(page_content=' at bounding  recurrences  in Chapter  4, \\nwhere  we’ll  be more  careful  about  this  kind  of detail.  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 63}),\n",
       "  '45adb5f5-6a5f-42e6-8fe2-2a24400a0221': Document(page_content='2.3 Designing algorithms 43 \\n… \\n… \\n(d) (c) (b) (a) T.n/  \\nc 2 n c 2 n c 2 n \\nT.n=2/  T.n=2/  \\nc 2 n=2  c 2 n=2  c 2 n=2  c 2 n=2  \\nT.n=4/  T.n=4/  T.n=4/  T.n=4/  \\nc 2 n=4  c 2 n=4  c 2 n=4  c 2 n=4  \\nc 1 c 1 c 1 c 1 c 1 c 1 c 1 c 1 c 1 c 1 c 1 c 1 \\nn lg n C 1 c 2 n \\nc 2 n c 2 n \\nc 1 n \\nTotal: c 2 n lg n C c 1 n \\nFigure  2.5  How  to construct  a recursion  tree  for  the  recurrence  (2.4).  Part (a)  shows T.n/ , which \\nprogressively expands in (', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 64}),\n",
       "  '2da16fb7-c813-4c8a-a382-ac095ad1cbc2': Document(page_content='/ , which \\nprogressively expands in (b)–(d)  to form the recursion tree. The fully expanded tree  in part (d) \\nhas lg n C 1 levels. Each level above the leaves contributes a t otal cost of c 2 n, and the leaf level \\ncontributes c 1 n. The total cost, therefore, is c 2 n lg n C c 1 n D ‚.n  lg n/. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 64}),\n",
       "  '5f898433-40ba-43b2-930f-b37e52925e14': Document(page_content='44 Chapter 2 Getting Started \\nlevel than a tree with 2 i leaves, and so the total number of levels is .i C 1/ C 1 D \\nlg 2 i C1 C 1. \\nTo  compute  the  total  cost  represented  by  the  recurrence  (2.4), simply add up the \\ncosts of all the levels. The recursion tree has lg n C 1 levels. The levels above the \\nleaves each cost c 2 n, and the leaf level costs c 1 n, for a total cost of c 2 n lg n Cc 1 n D \\n‚.n  lg n/. \\nExercises  \\n2.3-1  \\nUsing  Figure  2.4  as a model,  illustrate  the  operation  of merg e sort on an array \\ninitially containing the sequence h3;41;52;26;38;57;9;49 i. \\n2.3-2  \\nThe  test  in line  1 of the  MERGE-SORT  procedure reads < if p \\ue004 r = rather than < if \\np', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 65}),\n",
       "  'e2ba2d4b-a0b8-4735-9257-46479aa76f16': Document(page_content='� r = rather than < if \\np ¤ r .= If MERGE-SORT  is called with p>r  , then the subarray AŒp  W r� is empty. \\nArgue that as long as the initial call of M ERGE-SORT.A;1;n/  has n \\ue004 1, the test \\n<if p ¤ r = sufûces  to ensure  that  no  recursive  call  has  p>r  . \\n2.3-3  \\nState a loop invariant for the while  loop  of lines  12318  of the  MERGE  procedure. \\nShow how to use it, along with the while  loops  of lines  20323  and  24327,  to prove  \\nthat the M ERGE  procedure is correct. \\n2.3-4  \\nUse mathematical induction to show that when n \\ue004 2 is an exact power of 2, the \\nsolution of the recurrence \\nT.n/  D ( \\n2 if n D 2;  \\n2T.n=2/  C', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 65}),\n",
       "  '8f71a8e3-c4a3-43ed-90c5-88650a15082c': Document(page_content='\\n2T.n=2/  C n if n>2  \\nis T.n/  D n lg n. \\n2.3-5  \\nYou can also think of insertion sort as a recursive  algorithm. In order to sort \\nAŒ1  W n�, recursively sort the subarray AŒ1  W n \\ue003 1� and then insert AŒn�  into the \\nsorted subarray AŒ1  W n \\ue003 1�. Write  pseudocode  for  this  recursive  version  of inser-  \\ntion  sort.  Give  a recurrence  for  its  worst-case  running  time. \\n2.3-6  \\nReferring  back  to the  searching  problem  (see  Exercise  2.1-4 ), observe that if the \\nsubarray being searched is already sorted, the sear ching algorithm can check the \\nmidpoint of the subarray against v and eliminate half of the subarray from further ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 65}),\n",
       "  '89f054ef-ad7a-4ef5-9818-f9a0798bc1c0': Document(page_content='Problems for Chapter 2 45 \\nconsideration. The binary  search  algorithm repeats this procedure, halving the \\nsize of the remaining portion of the subarray each time. Write pseudocode, either \\niterative or recursive, for binary search. Argue th at the worst-case  running  time  of \\nbinary search is ‚.lg n/. \\n2.3-7  \\nThe while  loop  of lines  537  of the  I NSERTION-SORT  procedure  in Section  2.1  \\nuses a linear search to scan (backward) through the  sorted subarray AŒ1  W j \\ue003 1�. \\nWhat  if insertion  sort  used  a binary  search  (see  Exercise  2.3-6)  instead  of a linear  \\nsearch?  Would  that  improve  the  overall  worst-case  running  time of insertion sort \\nto ‚.n  lg n/? \\n2.3-8  \\nDescribe an algorithm that, given a set S of n integers and another integer x , de-  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 66}),\n",
       "  'bfb0611b-7385-43d6-8fce-53a94074315c': Document(page_content=' integers and another integer x , de-  \\ntermines whether S contains two elements that sum to exactly x . Your algorithm \\nshould take ‚.n  lg n/ time in the worst case. \\nProblems  \\n2-1  Insertion  sort  on  small  arrays  in merge  sort  \\nAlthough merge sort runs in ‚.n  lg n/ worst-case  time  and  insertion  sort  runs  \\nin ‚.n  2 / worst-case  time,  the  constant  factors  in insertion  sort  can  make it faster \\nin practice for small problem sizes on many machine s. Thus it makes sense to \\ncoarsen  the leaves of the recursion by using insertion sort  within merge sort when \\nsubproblems  become  sufûciently  small.  Consider  a modiûcat ion to merge sort in \\nwhich n=k  sublists of length k are sorted using insertion sort and then merged \\nusing the standard merging mechanism, where k is a value to be determined. \\na. Show that insertion sort can sort the n=k  sub', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 66}),\n",
       "  'aec343e8-1763-4654-84e6-bee6024d913c': Document(page_content=' insertion sort can sort the n=k  sublists, each of length k, in ‚.nk/  \\nworst-case  time.  \\nb. Show how to merge the sublists in ‚.n  lg.n=k//  worst-case  time.  \\nc. Given  that  the  modiûed  algorithm  runs  in ‚.nk  C n lg.n=k//  worst-case  time,  \\nwhat is the largest value of k as a function of n for  which  the  modiûed  algorithm  \\nhas the same running time as standard merge sort, i n terms of ‚-notation?  \\nd. How should you choose k in practice?  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 66}),\n",
       "  '1ea4cee4-e1af-49a4-ba77-fb3bd1855b1c': Document(page_content='46 Chapter 2 Getting Started \\n2-2  Correctness  of bubblesort  \\nBubblesort  is a popular,  but  inefûcient,  sorting  algorithm . It works by repeatedly \\nswapping adjacent elements that are out of order. T he procedure B UBBLESORT  \\nsorts array AŒ1  W n�. \\nBUBBLESORT  .A;n/  \\n1 for  i D 1 to n \\ue003 1 \\n2 for  j D n downto  i C 1 \\n3 if AŒj�<AŒj  \\ue003 1� \\n4 exchange AŒj�  with AŒj  \\ue003 1� \\na. Let A 0 denote the array A after BUBBLESORT  .A;n/  is executed. To prove that \\nBUBBLESORT  is correct, you need to prove that it terminates an d that \\nA 0 Œ1�  හ A 0 Œ2�  හ \\ue001 \\ue001 \\ue001 හ  A', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 67}),\n",
       "  'ae3dc29d-8763-44c0-bf59-1b39389a8e70': Document(page_content='� \\ue001 හ  A 0 Œn�:  (2.5)  \\nIn order to show that B UBBLESORT  actually sorts, what else do you need to \\nprove?  \\nThe  next  two  parts  prove  inequality  (2.5).  \\nb. State precisely a loop invariant for the for  loop  in lines  234,  and  prove  that  this  \\nloop invariant holds. Your proof should use the str ucture of the  loop-invariant  \\nproof presented in this chapter. \\nc. Using the termination condition of the loop invaria nt proved in part (b), state \\na loop invariant for the for  loop  in lines  134  that  allows  you  to prove  inequal-  \\nity  (2.5).  Your  proof  should  use  the  structure  of the  loop-invariant  proof  pre-  \\nsented in this chapter. \\nd. What  is the  worst-case  running  time  of BUBBLESORT ? How ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 67}),\n",
       "  '15685ad3-90c5-45d3-b29d-5719ad9c9f2d': Document(page_content='  of BUBBLESORT ? How  does  it compare  \\nwith the running time of I NSERTION-SORT? \\n2-3  Correctness  of Horner’s  rule  \\nYou  are  given  the  coefûcents  a 0 ;a  1 ;a  2 ;:::;a  n of a polynomial \\nP.x/  D n X  \\nkD0 a k x k \\nD a 0 C a 1 x C a 2 x 2 C \\ue001 \\ue001 \\ue001 C  a n\\ue0021 x n\\ue0021 C a n x n ; \\nand you want to evaluate this polynomial for a give n value of x . Horner’s  rule  \\nsays to evaluate the polynomial according to this p arenthesization: ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 67}),\n",
       "  '2f4333c8-ac20-440c-8631-b1c5b033c676': Document(page_content='Problems for Chapter 2 47 \\nP.x/  D a 0 C x \\ue002 \\na 1 C x ã \\na 2 C \\ue001 \\ue001 \\ue001 C  x.a  n\\ue0021 C xa  n / \\ue001 \\ue001 \\ue001  ä Í \\n: \\nThe procedure H ORNER  implements  Horner’s  rule  to evaluate  P.x/ , given the \\ncoefûcients  a 0 ;a  1 ;a  2 ;:::;a  n in an array AŒ0  W n� and the value of x . \\nHORNER.A;n;x/  \\n1 p D 0 \\n2 for  i D n downto  0 \\n3 p D AŒi�  C x \\ue001 p \\n4 return  p \\na. In terms of ‚-notation,  what  is the  running  time  of this  procedure?  \\nb. Write  pseudocode  to implement  the  naive  polynomial-evalua tion algorithm that \\ncomp', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 68}),\n",
       "  '7c438f39-badf-4a2f-be35-a118cc046cb3': Document(page_content='omial-evalua tion algorithm that \\ncomputes each term of the polynomial from scratch. What is the running time \\nof this  algorithm?  How  does  it compare  with  HORNER? \\nc. Consider the following loop invariant for the proce dure HORNER : \\nAt the start of each iteration of the for  loop  of lines  233,  \\np D n\\ue002.i C1/  X  \\nkD0 AŒk  C i C 1� \\ue001 x k : \\nInterpret a summation with no terms as equaling 0. Following the structure \\nof the  loop-invariant  proof  presented  in this  chapter,  use  this loop invariant to \\nshow that, at termination, p D P  n \\nkD0 AŒk�  \\ue001 x k . \\n2-4  Inversions  \\nLet AŒ1  W n� be an array of n distinct numbers. If i <j  and AŒi�>AŒj� , then the \\npair .i;j/  is called', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 68}),\n",
       "  'd0d24d87-5996-4bea-a3a2-ff4b6c93401d': Document(page_content='\\npair .i;j/  is called an inversion  of A. \\na. List  the  ûve  inversions  of the  array  h2;3;8;6;1 i. \\nb. What array with elements from the set f1;2;:::;n g has  the  most  inversions?  \\nHow  many  does  it have?  \\nc. What is the relationship between the running time o f insertion sort and the \\nnumber  of inversions  in the  input  array?  Justify  your  answer  . \\nd. Give  an algorithm  that  determines  the  number  of inversions  in any permutation \\non n elements in ‚.n  lg n/ worst-case  time.  (Hint: Modify merge sort.) ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 68}),\n",
       "  '99f8fe7a-210e-42d0-9afa-0b5e998893c9': Document(page_content='48 Chapter 2 Getting Started \\nChapter  notes  \\nIn 1968,  Knuth  published  the  ûrst  of three  volumes  with  the  general title The Art of \\nComputer  Programming  [259,  260,  261].  The  ûrst  volume  ushered  in the  modern  \\nstudy of computer algorithms with a focus on the an alysis of running time. The \\nfull series remains an engaging and worthwhile refe rence for many of the topics \\npresented  here.  According  to Knuth,  the  word  <algorithm=  is derived from the \\nname  <al-Khowˆ  arizmˆ  ı,= a ninth-century  Persian  mathemat ician. \\nAho,  Hopcroft,  and  Ullman  [5]  advocated  the  asymptotic  analysis of algorithms \\n4using  notations  that  Chapter  3 introduces,  including  ‚-notation4as  a means  \\nof comparing relative performance. They also popula rized the use of recurrence \\nrelations to', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 69}),\n",
       "  'a9b175b1-c4f2-4bf8-89e2-43f52cd007e4': Document(page_content='ized the use of recurrence \\nrelations to describe the running times of recursiv e algorithms. \\nKnuth  [261]  provides  an encyclopedic  treatment  of many  sorting algorithms. His \\ncomparison  of sorting  algorithms  (page  381)  includes  exact  step-counting  analyses,  \\nlike  the  one  we  performed  here  for  insertion  sort.  Knuth’s  discussion of insertion \\nsort encompasses several variations of the algorith m. The most important of these \\nis Shell’s  sort,  introduced  by  D.  L. Shell,  which  uses  insert ion sort on periodic \\nsubarrays of the input to produce a faster sorting algorithm. \\nMerge  sort  is also  described  by  Knuth.  He  mentions  that  a mechanical  colla-  \\ntor capable of merging two decks of punched cards i n a single pass was invented \\nin 1938.  J. von  Neumann,  one  of the  pioneers  of computer  scien ce, apparently \\nwrote  a program  for', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 69}),\n",
       "  '3a12a487-3d4b-4293-9410-5464d321125d': Document(page_content=', apparently \\nwrote  a program  for  merge  sort  on  the  EDVAC  computer  in 1945.  \\nThe early history of proving programs correct is de scribed by Gries  [200],  who  \\ncredits  P. Naur  with  the  ûrst  article  in this  ûeld.  Gries  attributes loop invariants to \\nR. W.  Floyd.  The  textbook  by  Mitchell  [329]  is a good  referenc e on how to prove \\nprograms correct. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 69}),\n",
       "  'ac2e2400-1051-40f4-b366-cfc26bb7f89d': Document(page_content='3 Characterizing  Running  Times  \\nThe  order  of growth  of the  running  time  of an algorithm,  deûne d in Chapter 2, \\ngives  a simple  way  to characterize  the  algorithm’s  efûcienc y and also allows us \\nto compare  it with  alternative  algorithms.  Once  the  input  size n becomes large \\nenough, merge sort, with its ‚.n  lg n/ worst-case  running  time,  beats  insertion  sort,  \\nwhose  worst-case  running  time  is ‚.n  2 /. Although we can sometimes determine \\nthe exact running time of an algorithm, as we did f or insertion sort in Chapter 2, \\nthe extra precision is rarely worth the effort of c omputing it. For large enough \\ninputs,  the  multiplicative  constants  and  lower-order  terms of an exact running time \\nare dominated by the effects of the input size itse lf. \\nWhen we look at input sizes large enough to make re levant only the order of \\ngrowth of the running', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 70}),\n",
       "  'e172d32f-f813-46b1-9227-3cfb0ebfeee8': Document(page_content=' only the order of \\ngrowth of the running time, we are studying the asymptotic  efûciency  of algo-  \\nrithms. That is, we are concerned with how the runn ing time of an algorithm \\nincreases with the size of the input in the limit , as the size of the input increases \\nwithout bound. Usually, an algorithm that is asympt otically more  efûcient  is the  \\nbest choice for all but very small inputs. \\nThis chapter gives several standard methods for sim plifying the  asymptotic  anal-  \\nysis of algorithms. The next section presents infor mally the three most commonly \\nused types of <asymptotic notation,= of which we ha ve already seen an example \\nin ‚-notation.  It also  shows  one  way  to use  these  asymptotic  notations to reason \\nabout  the  worst-case  running  time  of insertion  sort.  Then  we  look at asymptotic \\nnotations more formally and present several notatio nal con ventions  used  through-', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 70}),\n",
       "  'ce554008-d20b-4fc8-8bf3-4c5f8e5e5b0d': Document(page_content=' nal con ventions  used  through-  \\nout this book. The last section reviews the behavio r of functions that commonly \\narise when analyzing algorithms. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 70}),\n",
       "  'b1f53141-a5dd-41f8-889e-718f1934cfdf': Document(page_content='50  Chapter  3 Characterizing  Running  Times  \\n3.1  O-notation,  \\ue001-notation,  and  ‚-notation  \\nWhen  we  analyzed  the  worst-case  running  time  of insertion  sort in Chapter 2, we \\nstarted with the complicated expression \\n\\ue002 c 5 \\n2 C c 6 \\n2 C c 7 \\n2 Í \\nn 2 C \\ue002 \\nc 1 C c 2 C c 4 C c 5 \\n2 \\ue003 c 6 \\n2 \\ue003 c 7 \\n2 C c 8 Í \\nn \\n\\ue003 .c 2 C c 4 C c 5 C c 8 /: \\nWe  then  discarded  the  lower-order  terms  .c 1 C c 2 C c 4 C c 5 =2  \\ue003 c 6 =2  \\ue003 c 7 =2  C c 8 /n \\nand c 2 C c 4 C c 5 C c 8 , and  we  also  ignored  the  coefûcient  c 5 =2  C c 6 =2  C c 7', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 71}),\n",
       "  '39456b01-1a25-4c65-9b1a-b9cdd857d30c': Document(page_content='  C c 6 =2  C c 7 =2  \\nof n 2 . That left just the factor n 2 , which we put into ‚-notation  as ‚.n  2 /. We \\nuse this style to characterize running times of alg orithms: discard  the  lower-order  \\nterms  and  the  coefûcient  of the  leading  term,  and  use  a notati on that focuses on the \\nrate of growth of the running time. \\n‚-notation  is not  the  only  such  <asymptotic  notation.=  In this  section,  we’ll  \\nsee other forms of asymptotic notation as well. We start with intuitive looks at \\nthese notations, revisiting insertion sort to see h ow we can apply them. In the next \\nsection,  we’ll  see  the  formal  deûnitions  of our  asymptotic  notations, along with \\nconventions for using them. \\nBefore  we  get  into  speciûcs,  bear  in mind  that', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 71}),\n",
       "  '974b766a-f210-4a96-a210-fd7c8f24ecfa': Document(page_content='ûcs,  bear  in mind  that  the  asymptotic  notations  we’ll  see  \\nare designed so that they characterize functions in  general. It so happens that the \\nfunctions we are most interested in denote the runn ing times of algorithms. But \\nasymptotic notation can apply to functions that cha racterize some other aspect of \\nalgorithms (the amount of space they use, for examp le), or even to functions that \\nhave nothing whatsoever to do with algorithms. \\nO-notation  \\nO-notation  characterizes  an upper  bound  on the asymptotic behavior of a function. \\nIn other words, it says that a function grows no faster than a certain rate, based on \\nthe  highest-order  term.  Consider,  for  example,  the  functio n 7n  3 C 100n  2 \\ue003 20n  C 6. \\nIts  highest-order  term  is 7n  3 , and  so we  say  that  this  function’s  rate  of growth  is n 3 . \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 71}),\n",
       "  '2dce78f4-d1d9-42a2-936c-12074de30f6c': Document(page_content='  of growth  is n 3 . \\nBecause this function grows no faster than n 3 , we can write that it is O.n  3 /. You \\nmight be surprised that we can also write that the function 7n  3 C 100n  2 \\ue003 20n  C 6 \\nis O.n  4 /. Why?  Because  the  function  grows  more  slowly  than  n 4 , we are correct \\nin saying that it grows no faster. As you might hav e guessed, this function is also \\nO.n  5 /, O.n  6 /, and so on. More generally, it is O.n  c / for any constant c \\ue004 3. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 71}),\n",
       "  '228631a1-5097-4c70-9783-bb845297cb0e': Document(page_content='3.1 O-notation, �-notation, and ‚-notation 51 \\n\\ue001-notation  \\n�-notation  characterizes  a lower  bound  on the asymptotic behavior of a function. \\nIn other words, it says that a function grows at least as fast as a certain rate, based \\n4as  in O-notation4on  the  highest-order  term.  Because  the  highest- order term \\nin the function 7n  3 C 100n  2 \\ue003 20n  C 6 grows at least as fast as n 3 , this function \\nis �.n  3 /. This function is also �.n  2 / and �.n/ . More generally, it is �.n  c / for \\nany constant c හ 3. \\n‚-notation  \\n‚-notation  characterizes  a tight  bound  on the asymptotic behavior of a function. It \\nsays that a function grows precisely  at a certain  rate,  based4once  again4on  the  \\nhighest-order  term.  Put  another ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 72}),\n",
       "  '0dc68676-4208-43a9-b6e8-acdbc9eccf45': Document(page_content='-order  term.  Put  another  way,  ‚-notation  characterizes  the  rate  of growth  of \\nthe function to within a constant factor from above  and to within a constant factor \\nfrom below. These two constant factors need not be equal. \\nIf you can show that a function is both O.f.n//  and �.f  .n//  for  some  func-  \\ntion f.n/ , then you have shown that the function is ‚.f.n// . (The next section \\nstates this fact as a theorem.) For example, since the function 7n  3 C100n  2 \\ue00320nC6 \\nis both O.n  3 / and �.n  3 /, it is also ‚.n  3 /. \\nExample:  Insertion  sort  \\nLet’s  revisit  insertion  sort  and  see  how  to work  with  asymptotic  notation  to charac-  \\nterize its ‚.n  2 / worst-case  running  time  without  evaluating  summations ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 72}),\n",
       "  'a7298481-d56c-4ba8-8e75-ef538a4d2bc2': Document(page_content='  time  without  evaluating  summations  as we did \\nin Chapter 2. Here is the I NSERTION-SORT  procedure once again: \\nI NSERTION-SORT  .A;n/  \\n1 for  i D 2 to n \\n2 key  D AŒi�  \\n3 / / Insert AŒi�  into the sorted subarray AŒ1  W i \\ue003 1�. \\n4 j D i \\ue003 1 \\n5 while  j >0  and AŒj�>  key  \\n6 AŒj  C 1� D AŒj�  \\n7 j D j \\ue003 1 \\n8 AŒj  C 1� D key  \\nWhat  can  we  observe  about  how  the  pseudocode  operates?  The  procedure has \\nnested loops. The outer loop is a for  loop that runs n \\ue003 1 times, regardless of the \\nvalues being sorted. The inner loop is a while  loop, but the number of iterations \\nit makes depends on the values being sorted', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 72}),\n",
       "  'f11fe25c-7e62-4d6f-a745-d2e1b3324a23': Document(page_content=' \\nit makes depends on the values being sorted. The lo op variable j starts at i \\ue003 1 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 72}),\n",
       "  '5ec16d25-6e15-4418-b744-85633df6494e': Document(page_content='52  Chapter  3 Characterizing  Running  Times  \\neach of the \\nn/3 largest  \\nvalues moves through each \\nof these \\nn/3 positions  to somewhere \\nin these \\nn/3 positions  AŒ1  W n=3�  AŒn=3  C 1 W 2n=3�  AŒ2n=3  C 1 W n� \\nFigure  3.1  The �.n  2 / lower  bound  for  insertion  sort.  If the  ûrst  n=3  positions contain the n=3  \\nlargest values, each of these values must move thro ugh each of the middle n=3  positions, one position \\nat a time, to end up somewhere in the last n=3  positions. Since each of n=3  values moves through at \\nleast each of n=3  positions, the time taken in this case is at least proportional to .n=3/.n=3/  D n 2 =9, \\nor �.n  2 /. \\nand decreases by 1 in each iteration until either it', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 73}),\n",
       "  '9cade853-0b52-4d07-911f-cc84a789debf': Document(page_content='and decreases by 1 in each iteration until either it reaches 0 or AŒj�  හ key. For a \\ngiven value of i , the while  loop might iterate 0 times, i \\ue003 1 times, or anywhere in \\nbetween. The body of the while  loop  (lines  637)  takes  constant  time  per  iteration  \\nof the while  loop. \\nThese  observations  sufûce  to deduce  an O.n  2 / running time for any case of \\nI NSERTION-SORT, giving us a blanket statement that covers all inp uts. The run- \\nning time is dominated by the inner loop. Because e ach of the n \\ue003 1 iterations of \\nthe outer loop causes the inner loop to iterate at most i \\ue003 1 times, and because i is \\nat most n, the total number of iterations of the inner loop is at most .n \\ue003 1/.n  \\ue003 1/, \\nwhich is less than n 2 . Since each iteration of the inner loop takes cons tant time, \\nthe total time spent in the inner', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 73}),\n",
       "  '8ec188b2-71ee-40be-b479-a69bebfe8198': Document(page_content=', \\nthe total time spent in the inner loop is at most a  constant times n 2 , or O.n  2 /. \\nWith  a little  creativity,  we  can  also  see  that  the  worst-case  running time of \\nI NSERTION-SORT  is �.n  2 /. By  saying  that  the  worst-case  running  time  of an \\nalgorithm is �.n  2 /, we mean that for every input size n above a certain threshold, \\nthere is at least one input of size n for which the algorithm takes at least cn  2 time, \\nfor some positive constant c . It does not necessarily mean that the algorithm t akes \\nat least cn  2 time for all inputs. \\nLet’s  now  see  why  the  worst-case  running  time  of I NSERTION-SORT  is �.n  2 /. \\nFor a value to end up to the right of where it star ted, it must have been moved in \\nline  6. In fact,  for  a value  to end ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 73}),\n",
       "  '009786cb-056b-438c-abf1-de13e542828e': Document(page_content=',  for  a value  to end  up  k positions to the right of where it started, \\nline  6 must  have  executed  k times.  As  Figure  3.1  shows,  let’s  assume  that  n is \\na multiple of 3 so that we can divide the array A into groups of n=3  positions. \\nSuppose that in the input to I NSERTION-SORT, the n=3  largest values occupy the \\nûrst  n=3  array positions AŒ1  W n=3�. (It does not matter what relative order they \\nhave  within  the  ûrst  n=3  positions.)  Once  the  array  has  been  sorted,  each  of these  \\nn=3  values ends up somewhere in the last n=3  positions AŒ2n=3  C 1 W n�. For that \\nto happen, each of these n=3  values must pass through each of the middle n=3  \\npositions AŒn=3  C 1 W 2n=3� .', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 73}),\n",
       "  'dbeb1a6c-31d3-41b3-a858-bf2e7a1b4250': Document(page_content='  C 1 W 2n=3� . Each of these n=3  values passes through these middle ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 73}),\n",
       "  '55c84f95-4d56-41fe-b0d6-627408abd11a': Document(page_content='3.2  Asymptotic  notation:  formal  deﬁnitions  53 \\nn=3  positions one position at a time, by at least n=3  executions  of line  6. Because  \\nat least n=3  values have to pass through at least n=3  positions, the time taken by \\nI NSERTION-SORT  in the worst case is at least proportional to .n=3/.n=3/  D n 2 =9, \\nwhich is �.n  2 /. \\nBecause we have shown that I NSERTION-SORT  runs in O.n  2 / time in all cases \\nand that there is an input that makes it take �.n  2 / time, we can conclude that the \\nworst-case  running  time  of I NSERTION-SORT  is ‚.n  2 /. It does not matter that \\nthe constant factors for upper and lower bounds mig ht differ. What matters is \\nthat  we  have  characterized  the  worst-case  running  time  to within constant factors \\n(discounting ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 74}),\n",
       "  '9614703d-e206-447b-8345-4df14ca5fe3b': Document(page_content=' within constant factors \\n(discounting  lower-order  terms).  This  argument  does  not  show that I NSERTION- \\nSORT  runs in ‚.n  2 / time in all cases.  Indeed,  we  saw  in Chapter  2 that  the  best-  \\ncase running time is ‚.n/ . \\nExercises  \\n3.1-1  \\nModify  the  lower-bound  argument  for  insertion  sort  to handl e input sizes that are \\nnot necessarily a multiple of 3. \\n3.1-2  \\nUsing reasoning similar to what we used for inserti on sort, analyze the running \\ntime  of the  selection  sort  algorithm  from  Exercise  2.2-2.  \\n3.1-3  \\nSuppose that ˛ is a fraction in the range 0 < ˛ < 1 . Show how to generalize \\nthe  lower-bound  argument  for  insertion  sort  to consider  an input in which the ˛n  \\nlargest  values  start  in the', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 74}),\n",
       "  '38869de9-77f0-4e2c-b3a0-87c250d458eb': Document(page_content=' \\nlargest  values  start  in the  ûrst  ˛n  positions. What additional restriction do you \\nneed to put on ˛? What  value  of ˛ maximizes the number of times that the ˛n  \\nlargest values must pass through each of the middle  .1 \\ue003 2˛/n  array  positions?  \\n3.2  Asymptotic  notation:  formal  deûnitions  \\nHaving  seen  asymptotic  notation  informally,  let’s  get  more  formal. The notations \\nwe use to describe the asymptotic running time of a n algorithm are  deûned  in \\nterms of functions whose domains are typically the set N of natural numbers or \\nthe set R of real numbers. Such notations are convenient for describing  a running-  \\ntime function T.n/. This  section  deûnes  the  basic  asymptotic  notations  and  also \\nintroduces some common <proper= notational abuses. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 74}),\n",
       "  'c1dac160-b258-4eea-b03c-225dbc7c06bb': Document(page_content='54  Chapter  3 Characterizing  Running  Times  \\n(a) (b) (c) n n n n 0 n 0 n 0 f.n/  D ‚.g.n//  f.n/  D O.g.n//  f.n/  D �.g.n//  f.n/  \\nf.n/  f.n/  \\ncg.n/  cg.n/  \\nc 1 g.n/  c 2 g.n/  \\nFigure  3.2  Graphic  examples  of the  O, �, and ‚ notations. In each part, the value of n 0 shown \\nis the minimum possible value, but any greater valu e also works. (a)  O-notation  gives  an  upper  \\nbound for a function to within a constant factor. W e write f.n/  D O.g.n//  if there are positive \\nconstants n 0 and c such that at and to the right of n 0 , the value of f.n/  always  lies  on  or be-  \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 75}),\n",
       "  '618d4ae7-657b-4643-8f43-4c35c64f30a7': Document(page_content=' lies  on  or be-  \\nlow cg.n/ . (b)  �-notation  gives  a lower  bound  for  a function  to within  a const ant factor. We write \\nf.n/  D �.g.n//  if there are positive constants n 0 and c such that at and to the right of n 0 , the value \\nof f.n/  always lies on or above cg.n/ . (c)  ‚-notation  bounds  a function  to within  constant  factors.  \\nWe write f.n/  D ‚.g.n//  if there exist positive constants n 0 , c 1 , and c 2 such that at and to the right \\nof n 0 , the value of f.n/  always lies between c 1 g.n/  and c 2 g.n/  inclusive. \\nO-notation  \\nAs  we  saw  in Section  3.1,  O-notation  describes  an asymptotic  upper  bound . We \\nuse O-notation  to give  an upper  bound  on  a function', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 75}),\n",
       "  'b1ebe574-34c1-4ddb-979a-6fa9926d16d9': Document(page_content='  an upper  bound  on  a function,  to within  a constant factor. \\nHere  is the  formal  deûnition  of O-notation.  For  a given  function  g.n/ , we denote \\nby O.g.n//  (pronounced  <big-oh  of g of n= or sometimes just <oh of g of n=) the \\nset  of functions  \\nO.g.n//  D ff.n/  W there exist positive constants c and n 0 such that \\n0 හ f.n/  හ cg.n/  for all n \\ue004 n 0 g : 1 \\nA function f.n/  belongs to the set O.g.n//  if there exists a positive constant c such \\nthat f.n/  හ cg.n/  for  sufûciently  large  n. Figure  3.2(a)  shows  the  intuition  behind  \\nO-notation.  For  all  values  n at and to the right of n 0 , the value of', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 75}),\n",
       "  'f658db0c-e090-4517-b0cc-3a310987bc7e': Document(page_content=' to the right of n 0 , the value of the function f.n/  \\nis on or below cg.n/ . \\nThe  deûnition  of O.g.n//  requires that every function f.n/  in the set O.g.n//  \\nbe asymptotically  nonnegative : f.n/  must be nonnegative whenever n is sufû-  \\nciently large. (An asymptotically  positive  function is one that is positive for all \\n1 Within set notation, a colon means <such that.= ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 75}),\n",
       "  '7e03cb6b-c541-4e6e-a58c-fe3e6b9a7ae1': Document(page_content='3.2  Asymptotic  notation:  formal  deﬁnitions  55 \\nsufûciently  large  n.) Consequently, the function g.n/  itself must be asymptotically \\nnonnegative, or else the set O.g.n//  is empty. We therefore assume that every \\nfunction used within O-notation  is asymptotically  nonnegative.  This  assumption  \\nholds  for  the  other  asymptotic  notations  deûned  in this  chap ter as well. \\nYou  might  be surprised  that  we  deûne  O-notation  in terms  of sets.  Indeed,  you  \\nmight expect that we would write < f.n/  2 O.g.n// = to indicate that f.n/  be-  \\nlongs to the set O.g.n// . Instead, we usually write < f.n/  D O.g.n// = and say \\n<f.n/  is big-oh  of g.n/= to express  the  same', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 76}),\n",
       "  'e4679107-c59b-4163-91c7-9bc0d0e96fcf': Document(page_content='.n/= to express  the  same  notion.  Although  it may  seem  con-  \\nfusing  at ûrst  to abuse  equality  in this  way,  we’ll  see  later  in this section that doing \\nso has its advantages. \\nLet’s  explore  an example  of how  to use  the  formal  deûnition  of O-notation  to \\njustify  our  practice  of discarding  lower-order  terms  and  ignoring  the  constant  coef-  \\nûcient  of the  highest-order  term.  We’ll  show  that  4n  2 C100n  C500  D O.n  2 /, even \\nthough  the  lower-order  terms  have  much  larger  coefûcients  than the leading term. \\nWe  need  to ûnd  positive  constants  c and n 0 such that 4n  2 C 100n  C 500  හ cn  2 \\nfor all n \\ue004 n 0 . Divid', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 76}),\n",
       "  '9ffa1bce-8453-4c52-b510-dd3fa3386a75': Document(page_content=' all n \\ue004 n 0 . Dividing both sides by n 2 gives 4 C 100=n  C 500=n  2 හ c . This \\ninequality  is satisûed  for  many  choices  of c and n 0 . For example, if we choose \\nn 0 D 1, then this inequality holds for c D 604. If we choose n 0 D 10, then c D 19  \\nworks, and choosing n 0 D 100  allows us to use c D 5:05. \\nWe  can  also  use  the  formal  deûnition  of O-notation  to show  that  the  function  \\nn 3 \\ue003 100n  2 does not belong to the set O.n  2 /, even  though  the  coefûcient  of n 2 \\nis a large negative number. If we had n 3 \\ue003 100n  2 D O.n  2 /, then there would be \\npositive constants c and n 0 such that n 3 \\ue003 100n  2 හ cn  2 for all n \\ue004 n 0 . Again', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 76}),\n",
       "  'b8c0086c-1b7e-4b9a-a19e-1f253b362cf6': Document(page_content=' for all n \\ue004 n 0 . Again, we \\ndivide both sides by n 2 , giving n \\ue003 100  හ c . Regardless of what value we choose \\nfor the constant c , this inequality does not hold for any value of n>c  C 100. \\n\\ue001-notation  \\nJust  as O-notation  provides  an asymptotic  upper  bound on a function, �-notation  \\nprovides an asymptotic  lower  bound . For a given function g.n/ , we denote \\nby �.g.n//  (pronounced  <big-omega  of g of n= or sometimes just <omega of g \\nof n=) the set of functions \\n�.g.n//  D ff.n/  W there exist positive constants c and n 0 such that \\n0 හ cg.n/  හ f.n/  for all n \\ue004 n 0 g : \\nFigure  3.2(b)  shows  the  intuition  behind  �-notation.  For  all  values  n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 76}),\n",
       "  '3dfa0627-e167-4d64-96a0-8e73e336f280': Document(page_content='notation.  For  all  values  n at or to the \\nright of n 0 , the value of f.n/  is on or above cg.n/ . \\nWe’ve  already  shown  that  4n  2 C 100n  C 500  D O.n  2 /. Now  let’s  show  that  \\n4n  2 C 100n  C 500  D �.n  2 /. We  need  to ûnd  positive  constants  c and n 0 such that \\n4n  2 C 100n  C 500  \\ue004 cn  2 for all n \\ue004 n 0 . As before, we divide both sides by n 2 , ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 76}),\n",
       "  'c1311efe-f151-4226-8c6e-202bf71b2064': Document(page_content='56  Chapter  3 Characterizing  Running  Times  \\ngiving 4 C 100=n  C 500=n  2 \\ue004 c . This inequality holds when n 0 is any positive \\ninteger and c D 4. \\nWhat  if we  had  subtracted  the  lower-order  terms  from  the  4n  2 term instead of \\nadding  them?  What  if we  had  a small  coefûcient  for  the  n 2 term?  The  function  \\nwould still be �.n  2 /. For  example,  let’s  show  that  n 2 =100  \\ue003 100n  \\ue003 500  D �.n  2 /. \\nDividing by n 2 gives 1=100  \\ue003 100=n  \\ue003 500=n  2 \\ue004 c . We can choose any value \\nfor n 0 that  is at least  10,005  and  ûnd  a positive  value  for  c . For example, when \\nn 0 D 10,005,  we  can  choose  c D 2', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 77}),\n",
       "  'ba5b98e3-3de2-44da-a59a-cfff005224f2': Document(page_content='  we  can  choose  c D 2:49  \\ue005 10  \\ue0029 . Yes,  that’s  a tiny  value  for  c , but it \\nis positive. If we select a larger value for n 0 , we can also increase c . For example, \\nif n 0 D 100,000,  then  we  can  choose  c D 0:0089 . The higher the value of n 0 , the \\ncloser  to the  coefûcient  1=100  we can choose c . \\n‚-notation  \\nWe use ‚-notation  for  asymptotically  tight  bounds . For a given function g.n/ , we \\ndenote by ‚.g.n//  (<theta of g of n=) the set of functions \\n‚.g.n//  D ff.n/  W there exist positive constants c 1 , c 2 , and n 0 such that \\n0 හ c 1 g.n/  හ f.n/  හ c 2 g.n/  for all n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 77}),\n",
       "  '1b50ba49-a6af-4253-9617-3ad0b8b7cecf': Document(page_content=' c 2 g.n/  for all n \\ue004 n 0 g : \\nFigure  3.2(c)  shows  the  intuition  behind  ‚-notation.  For  all  values  of n at and to \\nthe right of n 0 , the value of f.n/  lies at or above c 1 g.n/  and at or below c 2 g.n/ . In \\nother words, for all n \\ue004 n 0 , the function f.n/  is equal to g.n/  to within constant \\nfactors. \\nThe  deûnitions  of O-, �-, and  ‚-notations  lead  to the  following  theorem,  whose  \\nproof  we  leave  as Exercise  3.2-4.  \\nTheorem  3.1  \\nFor any two functions f.n/  and g.n/ , we have f.n/  D ‚.g.n//  if and only if \\nf.n/  D O.g.n//  and f.n/  D �.g.n// . ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 77}),\n",
       "  'c559074d-fe22-4e5f-ada4-3bd51deabd42': Document(page_content='  D �.g.n// . \\nWe  typically  apply  Theorem  3.1  to prove  asymptotically  tight  bounds  from  asymp-  \\ntotic upper and lower bounds. \\nAsymptotic  notation  and  running  times  \\nWhen you use asymptotic notation to characterize an  algorithm’s  running  time,  \\nmake sure that the asymptotic notation you use is a s precise as possible without \\noverstating which running time it applies to. Here are some examples of using \\nasymptotic notation properly and improperly to char acterize running times. \\nLet’s  start  with  insertion  sort.  We  can  correctly  say  that  insertion  sort’s  worst-  \\ncase running time is O.n  2 /, �.n  2 /, and4due  to Theorem  3.14‚.n  2 /. Although ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 77}),\n",
       "  '731417ba-ab5c-4b2d-8473-fef57fb3331c': Document(page_content='3.2  Asymptotic  notation:  formal  deﬁnitions  57 \\nall  three  ways  to characterize  the  worst-case  running  times  are correct, the ‚.n  2 / \\nbound is the most precise and hence the most prefer red. We can also correctly say \\nthat  insertion  sort’s  best-case  running  time  is O.n/ , �.n/ , and ‚.n/ , again with \\n‚.n/  the most precise and therefore the most preferred. \\nHere is what we cannot  correctly  say:  insertion  sort’s  running  time  is ‚.n  2 /. \\nThat  is an overstatement  because  by  omitting  <worst-case=  from the statement, \\nwe’re  left  with  a blanket  statement  covering  all  cases.  The  error  here  is that  inser-  \\ntion sort does not run in ‚.n  2 / time  in all  cases  since,  as we’ve  seen', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 78}),\n",
       "  '6d9841f5-6108-4fa7-8739-3732dc517163': Document(page_content=' since,  as we’ve  seen,  it runs  in \\n‚.n/  time in the best case. We can correctly say that in sertion sort’s  running  time  \\nis O.n  2 /, however, because in all cases, its running time g rows no faster than n 2 . \\nWhen we say O.n  2 / instead of ‚.n  2 /, there is no problem in having cases whose \\nrunning time grows more slowly than n 2 . Likewise, we cannot correctly say that \\ninsertion  sort’s  running  time  is ‚.n/ , but we can say that its running time is �.n/ . \\nHow  about  merge  sort?  Since  merge  sort  runs  in ‚.n  lg n/ time in all cases, \\nwe can just say that its running time is ‚.n  lg n/ without  specifying  worst-case,  \\nbest-case,  or any  other  case.  \\nPeople  occasionally  conüate  O-notation  with  ‚-notation', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 78}),\n",
       "  'df83eafe-30d7-4152-a0aa-5cf5796d56b8': Document(page_content=' O-notation  with  ‚-notation  by  mistakenly  using  \\nO-notation  to indicate  an asymptotically  tight  bound.  They  say things like <an \\nO.n  lg n/-time  algorithm  runs  faster  than  an O.n  2 /-time  algorithm.=  Maybe  it \\ndoes,  maybe  it doesn’t.  Since  O-notation  denotes  only  an asymptotic  upper  bound,  \\nthat  so-called  O.n  2 /-time  algorithm  might  actually  run  in ‚.n/  time. You should \\nbe careful to choose the appropriate asymptotic not ation. If you want to indicate \\nan asymptotically tight bound, use ‚-notation.  \\nWe typically use asymptotic notation to provide the  simplest and most precise \\nbounds possible. For example, if an algorithm has a  running time of 3n  2 C 20n  \\nin all cases, we use asymptotic notation to write t hat its running time', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 78}),\n",
       "  'cdf1a2ba-8f0e-4a44-aa0a-a2003a71765a': Document(page_content='ptotic notation to write t hat its running time is ‚.n  2 /. \\nStrictly speaking, we are also correct in writing t hat the running time is O.n  3 / or \\n‚.3n  2 C 20n/ . Neither of these expressions is as useful as writ ing ‚.n  2 / in this \\ncase, however: O.n  3 / is less precise than ‚.n  2 / if the running time is 3n  2 C 20n, \\nand ‚.3n  2 C 20n/  introduces complexity that obscures the order of gr owth. By \\nwriting the simplest and most precise bound, such a s ‚.n  2 /, we can categorize \\nand compare different algorithms. Throughout the bo ok, you will see asymptotic \\nrunning times that are almost always based on polyn omials and logarithms:  func-  \\ntions such as n, n lg 2 n, n 2 lg n, or n 1=2  . You will also see some other functions, \\nsuch as exponentials, lg lg', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 78}),\n",
       "  'cb217bef-bee5-4a6f-9adf-c2068d5cbc74': Document(page_content='such as exponentials, lg lg n, and lg \\ue003 n (see  Section  3.3).  It is usually  fairly  easy  \\nto compare  the  rates  of growth  of these  functions.  Problem  3-3  gives  you  good  \\npractice. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 78}),\n",
       "  'c988f52b-565a-4df0-af7b-34cb75103ec1': Document(page_content='58  Chapter  3 Characterizing  Running  Times  \\nAsymptotic  notation  in equations  and  inequalities  \\nAlthough  we  formally  deûne  asymptotic  notation  in terms  of sets, we use the equal \\nsign ( D) instead of the set membership sign ( 2) within formulas. For example, we \\nwrote that 4n  2 C 100n  C 500  D O.n  2 /. We might also write 2n  2 C 3n  C 1 D \\n2n  2 C ‚.n/. How  do  we  interpret  such  formulas?  \\nWhen the asymptotic notation stands alone (that is,  not within a larger formula) \\non  the  right-hand  side  of an equation  (or  inequality),  as in 4n  2 C 100n  C 500  D \\nO.n  2 /, the equal sign means set membership: 4n  2 C 100n  C 500  2 O.n  2 /. In \\ngeneral, however, when asymptotic notation appears in a formula, we interpret it', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 79}),\n",
       "  '80b2198e-cc89-4138-9177-3c4481fe732e': Document(page_content='otic notation appears in a formula, we interpret it as \\nstanding for some anonymous function that we do not  care to name. For example, \\nthe formula 2n  2 C 3n  C 1 D 2n  2 C ‚.n/  means that 2n  2 C 3n  C 1 D 2n  2 C f.n/ , \\nwhere f.n/  2 ‚.n/ . In this case, we let f.n/  D 3n  C 1, which indeed belongs \\nto ‚.n/ . \\nUsing asymptotic notation in this manner can help e liminate inessential detail \\nand clutter in an equation. For example, in Chapter  2 we expressed  the  worst-case  \\nrunning time of merge sort as the recurrence \\nT.n/  D 2T.n=2/  C ‚.n/:  \\nIf we are interested only in the asymptotic behavio r of T.n/ , there is no point in \\nspecifying  all  the  lower-order  terms  exactly,  because  they  are all understood to be', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 79}),\n",
       "  'e3dd8b05-3e62-4d35-a8b3-d7fff3e20a62': Document(page_content='  because  they  are all understood to be \\nincluded in the anonymous function denoted by the t erm ‚.n/ . \\nThe number of anonymous functions in an expression is understood to be equal \\nto the number of times the asymptotic notation appe ars. For example,  in the  ex-  \\npression \\nn X  \\ni D1 O.i/;  \\nthere is only a single anonymous function (a functi on of i ). This expression is thus \\nnot the same as O.1/  C O.2/  C \\ue001 \\ue001 \\ue001 C  O.n/, which  doesn’t  really  have  a clean  \\ninterpretation. \\nIn some  cases,  asymptotic  notation  appears  on  the  left-hand  side of an equation, \\nas in \\n2n  2 C ‚.n/  D ‚.n  2 /: \\nInterpret such equations using the following rule: No  matter  how  the  anonymous  \\nfunctions  are  chosen  on  the', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 79}),\n",
       "  'c22eac79-d15b-4ff6-b856-93803538e516': Document(page_content='functions  are  chosen  on  the  left  of the  equal  sign,  there  is a way  to choose  the  \\nanonymous  functions  on  the  right  of the  equal  sign  to make  the  equation  valid . \\nThus, our example means that for any  function f.n/  2 ‚.n/ , there is some function \\ng.n/  2 ‚.n  2 / such that 2n  2 Cf.n/  D g.n/  for all n. In other  words,  the  right-hand  \\nside of an equation provides a coarser level of det ail than the left-hand  side.  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 79}),\n",
       "  '9a1171c6-69a2-4ba3-b3cd-a2f267041abc': Document(page_content='3.2  Asymptotic  notation:  formal  deﬁnitions  59 \\nWe can chain together a number of such relationship s, as in \\n2n  2 C 3n  C 1 D 2n  2 C ‚.n/  \\nD ‚.n  2 /: \\nBy the rules above, interpret each equation separat ely. The ûrst  equation  says  that  \\nthere is some function f.n/  2 ‚.n/  such that 2n  2 C3n  C1 D 2n  2 C f.n/  for all n. \\nThe second equation says that for any  function g.n/  2 ‚.n/  (such as the f.n/  just \\nmentioned), there is some function h.n/  2 ‚.n  2 / such that 2n  2 C g.n/  D h.n/  for \\nall n. This interpretation implies that 2n  2 C 3n  C 1 D ‚.n  2 /, which is what the \\nchaining of equations intuitively', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 80}),\n",
       "  'b115d5d6-35a0-4f1c-8c50-0dc2c532dc68': Document(page_content=' what the \\nchaining of equations intuitively says. \\nProper  abuses  of asymptotic  notation  \\nBesides the abuse of equality to mean set membershi p, which we now see has a \\nprecise mathematical interpretation, another abuse of asymptotic notation occurs \\nwhen the variable tending toward 1  must be inferred from context. For example, \\nwhen we say O.g.n//, we  can  assume  that  we’re  interested  in the  growth  of g.n/  \\nas n grows, and if we say O.g.m//  we’re  talking  about  the  growth  of g.m/  as m \\ngrows. The free variable in the expression indicate s what variable is going to 1. \\nThe most common situation requiring contextual know ledge of which variable \\ntends to 1  occurs when the function inside the asymptotic nota tion is a constant, \\nas in the expression O.1/ . We cannot infer from the expression which variabl e is \\ngoing to 1, because no variable appears there. The context mu st disambiguate', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 80}),\n",
       "  'f4b1ea7f-172c-4b3c-82dd-69835f9a004b': Document(page_content=' there. The context mu st disambiguate. For \\nexample, if the equation using asymptotic notation is f.n/  D O.1/, it’s  apparent  \\nthat  the  variable  we’re  interested  in is n. Knowing  from  context  that  the  variable  of \\ninterest is n, however, allows us to make perfect sense of the e xpression by using \\nthe  formal  deûnition  of O-notation:  the  expression  f.n/  D O.1/  means that the \\nfunction f.n/  is bounded from above by a constant as n goes to 1. Technically, it \\nmight be less ambiguous if we explicitly indicated the variable tending to 1  in the \\nasymptotic notation itself, but that would clutter the notation. Instead, we simply \\nensure that the context makes it clear which variab le (or variables) tend to 1. \\nWhen the function inside the asymptotic notation is  bounded by  a positive  con-  \\nstant, as in T.n/  D O.1/ ,', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 80}),\n",
       "  '15e2d21c-787d-48f6-8bd4-2867dc1a6838': Document(page_content='.n/  D O.1/ , we often abuse asymptotic notation in yet another  way, \\nespecially when stating recurrences. We may write s omething like T.n/  D O.1/  \\nfor n < 3. According  to the  formal  deûnition  of O-notation,  this  statement  is \\nmeaningless,  because  the  deûnition  only  says  that  T.n/  is bounded above by a \\npositive constant c for n \\ue004 n 0 for some n 0 > 0. The value of T.n/  for n < n  0 \\nneed not be so bounded. Thus, in the example T.n/  D O.1/  for n<3 , we cannot \\ninfer any constraint on T.n/  when n<3 , because it might be that n 0 >3. \\nWhat is conventionally meant when we say T.n/  D O.1/  for n<3  is that there \\nexists a positive constant c such that T.n/  හ c for n<', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 80}),\n",
       "  '8c90b157-2bda-4b2d-9031-bd65c3d1a726': Document(page_content='/  හ c for n<3 . This convention saves ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 80}),\n",
       "  '01304316-7871-4009-8931-dfbeb43e0ea8': Document(page_content='60  Chapter  3 Characterizing  Running  Times  \\nus the trouble of naming the bounding constant, all owing it to remain anonymous \\nwhile we focus on more important variables in an an alysis. Similar abuses occur \\nwith the other asymptotic notations. For example, T.n/  D ‚.1/  for n<3  means \\nthat T.n/  is bounded above and below by positive constants wh en n<3 . \\nOccasionally,  the  function  describing  an algorithm’s  running time may not be \\ndeûned  for  certain  input  sizes,  for  example,  when  an algorit hm assumes that the \\ninput size is an exact power of 2. We still use asymptotic notation to describe the \\ngrowth of the running time, understanding that any constraints apply only when \\nthe  function  is deûned.  For  example,  suppose  that  f.n/  is deûned  only  on  a subset  \\nof the natural or nonnegative real numbers. Then f.n/  D O.g.n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 81}),\n",
       "  'c5cb5670-a334-4116-85c8-2ea54f6a4699': Document(page_content='.n/  D O.g.n//  means that the \\nbound 0 හ T.n/  හ cg.n/  in the  deûnition  of O-notation  holds  for  all  n \\ue004 n 0 over \\nthe domain of f.n/ , that is, where f.n/  is deûned.  This  abuse  is rarely  pointed  \\nout, since what is meant is generally clear from co ntext. \\nIn mathematics,  it’s  okay4and  often  desirable4to  abuse  a notation, as long as \\nwe  don’t  misuse  it. If we  understand  precisely  what  is meant  by  the  abuse  and  don’t  \\ndraw incorrect conclusions, it can simplify our mat hematical language, contribute \\nto our  higher-level  understanding,  and  help  us focus  on  what  really matters. \\no-notation  \\nThe asymptotic upper bound provided by O-notation  may  or may  not  be asymp-', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 81}),\n",
       "  '0cbb08ac-d0e0-4d3b-b2ab-aabb87f0b429': Document(page_content=' or may  not  be asymp-  \\ntotically tight. The bound 2n  2 D O.n  2 / is asymptotically tight, but the bound \\n2n  D O.n  2 / is not. We use o-notation  to denote  an upper  bound  that  is not  asymp-  \\ntotically  tight.  We  formally  deûne  o.g.n//  (<little-oh  of g of n=) as the set \\no.g.n//  D ff.n/  W for any positive constant c>0 , there exists a constant \\nn 0 >0  such that 0 හ f.n/<cg.n/  for all n \\ue004 n 0 g : \\nFor example, 2n  D o.n  2 /, but 2n  2 ¤ o.n  2 /. \\nThe  deûnitions  of O-notation  and  o-notation  are  similar.  The  main  difference  \\nis that in f.n/  D O.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 81}),\n",
       "  '9f20bf60-9808-4d65-84bc-fc498c7419e6': Document(page_content=' that in f.n/  D O.g.n// , the bound 0 හ f.n/  හ cg.n/  holds for some con-  \\nstant c>0 , but in f.n/  D o.g.n// , the bound 0 හ f.n/  < cg.n/  holds for all \\nconstants c>0 . Intuitively, in o-notation,  the  function  f.n/  becomes  insigniûcant  \\nrelative to g.n/  as n gets large: \\nlim \\nn!1  f.n/  \\ng.n/  D 0:  \\nSome  authors  use  this  limit  as a deûnition  of the  o-notation,  but  the  deûnition  in \\nthis book also restricts the anonymous functions to  be asymptotically nonnegative. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 81}),\n",
       "  '54afd006-c3a9-4409-9022-a09a63c59cfc': Document(page_content='3.2  Asymptotic  notation:  formal  deﬁnitions  61 \\n!-notation  \\nBy analogy, !-notation  is to �-notation  as o-notation  is to O-notation.  We  use  \\n!-notation  to denote  a lower  bound  that  is not  asymptotically  tight.  One  way  to \\ndeûne  it is by  \\nf.n/  2 !.g.n//  if and only if g.n/  2 o.f.n//:  \\nFormally,  however,  we  deûne  !.g.n//  (<little-omega  of g of n=) as the set \\n!.g.n//  D ff.n/  W for any positive constant c>0 , there exists a constant \\nn 0 >0  such that 0 හ cg.n/<f.n/  for all n \\ue004 n 0 g : \\nWhere  the  deûnition  of o-notation  says  that  f.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 82}),\n",
       "  '9dcaa769-308b-40fc-bd69-98751e39cb75': Document(page_content=' o-notation  says  that  f.n/  < cg.n/, the  deûnition  of \\n!-notation  says  the  opposite:  that  cg.n/  < f.n/ . For examples of !-notation,  \\nwe have n 2 =2  D !.n/ , but n 2 =2  ¤ !.n  2 /. The relation f.n/  D !.g.n//  implies \\nthat \\nlim \\nn!1  f.n/  \\ng.n/  D 1  ; \\nif the limit exists. That is, f.n/  becomes arbitrarily large relative to g.n/  as n gets \\nlarge. \\nComparing  functions  \\nMany of the relational properties of real numbers a pply to asymptotic comparisons \\nas well. For the following, assume that f.n/  and g.n/  are asymptotically positive. \\nTransitivity:  \\nf.n/  D ‚.g.n//  and g.n/  D ‚.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 82}),\n",
       "  '999c76c5-d317-43d6-9f17-8ab821b8a073': Document(page_content=' and g.n/  D ‚.h.n//  imply f.n/  D ‚.h.n//;  \\nf.n/  D O.g.n//  and g.n/  D O.h.n//  imply f.n/  D O.h.n//;  \\nf.n/  D �.g.n//  and g.n/  D �.h.n//  imply f.n/  D �.h.n//  ; \\nf.n/  D o.g.n//  and g.n/  D o.h.n//  imply f.n/  D o.h.n//;  \\nf.n/  D !.g.n//  and g.n/  D !.h.n//  imply f.n/  D !.h.n//:  \\nReüexivity:  \\nf.n/  D ‚.f.n//;  \\nf.n/  D O.f.n//;  \\nf.n/  D', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 82}),\n",
       "  '514caf05-1f2d-479f-9fb3-9837c0f8ee95': Document(page_content=';  \\nf.n/  D �.f  .n//  : \\nSymmetry:  \\nf.n/  D ‚.g.n//  if and only if g.n/  D ‚.f.n//:  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 82}),\n",
       "  '74e9a7d9-6726-4bee-9a4d-71bf92ad3fef': Document(page_content='62  Chapter  3 Characterizing  Running  Times  \\nTranspose  symmetry:  \\nf.n/  D O.g.n//  if and only if g.n/  D �.f  .n//  ; \\nf.n/  D o.g.n//  if and only if g.n/  D !.f.n//:  \\nBecause these properties hold for asymptotic notati ons, we can draw an analogy \\nbetween the asymptotic comparison of two functions f and g and the comparison \\nof two real numbers a and b: \\nf.n/  D O.g.n//  is like a හ b;  \\nf.n/  D �.g.n//  is like a \\ue004 b;  \\nf.n/  D ‚.g.n//  is like a D b;  \\nf.n/  D o.g.n//  is like a<b;  \\nf.n/  D !.g.n//  is like a>b:  \\nWe say', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 83}),\n",
       "  '5b7a8e90-ac11-488c-990e-7c7ce389a44b': Document(page_content=' like a>b:  \\nWe say that f.n/  is asymptotically  smaller  than g.n/  if f.n/  D o.g.n// , and f.n/  \\nis asymptotically  larger  than g.n/  if f.n/  D !.g.n// . \\nOne  property  of real  numbers,  however,  does  not  carry  over  to asymptotic  nota-  \\ntion: \\nTrichotomy:  For any two real numbers a and b, exactly one of the following \\nmust hold: a<b , a D b, or a>b . \\nAlthough any two real numbers can be compared, not all functions  are  asymptot-  \\nically comparable. That is, for two functions f.n/  and g.n/ , it may be the case \\nthat neither f.n/  D O.g.n//  nor f.n/  D �.g.n//  holds. For example, we cannot \\ncompare the functions n and n 1', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 83}),\n",
       "  '2cfca7b1-379d-455d-8695-55302e8487e1': Document(page_content=' \\ncompare the functions n and n 1Csin n using asymptotic notation, since the value of \\nthe exponent in n 1Csin n oscillates between 0 and 2, taking on all values in  between. \\nExercises  \\n3.2-1  \\nLet f.n/  and g.n/  be asymptotically nonnegative functions. Using the basic deû-  \\nnition of ‚-notation,  prove  that  max  ff.n/;g.n/ g D  ‚.f.n/  C g.n// . \\n3.2-2  \\nExplain why the statement, <The running time of alg orithm A is at least O.n  2 /,= is \\nmeaningless. \\n3.2-3  \\nIs 2 nC1 D O.2  n /? Is 2 2n  D O.2  n /? \\n3.2-4  \\nProve  Theorem  3.1.  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 83}),\n",
       "  'adf3ca7c-65a6-474b-af13-6a2a1fd45ed6': Document(page_content='3.3  Standard  notations  and  common  functions  63 \\n3.2-5  \\nProve that the running time of an algorithm is ‚.g.n//  if and  only  if its  worst-case  \\nrunning time is O.g.n//  and  its  best-case  running  time  is �.g.n// . \\n3.2-6  \\nProve that o.g.n//  \\\\ !.g.n//  is the empty set. \\n3.2-7  \\nWe can extend our notation to the case of two param eters n and m that can go to \\n1  independently at different rates. For a given funct ion g.n;m/ , we denote by \\nO.g.n;m//  the set of functions \\nO.g.n;m//  D ff.n;m/  W there exist positive constants c , n 0 , and m 0 \\nsuch that 0 හ f.n;m/  හ cg.n;m/  \\nfor all n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 84}),\n",
       "  '8bf7d991-70fc-40a1-afb1-fce58384d729': Document(page_content='n;m/  \\nfor all n \\ue004 n 0 or m \\ue004 m 0 g : \\nGive  corresponding  deûnitions  for  �.g.n;  m//  and ‚.g.n;m// . \\n3.3  Standard  notations  and  common  functions  \\nThis section reviews some standard mathematical fun ctions and  notations  and  ex-  \\nplores the relationships among them. It also illust rates the use of the asymptotic \\nnotations. \\nMonotonicity  \\nA function f.n/  is monotonically  increasing  if m හ n implies f.m/  හ f.n/ . \\nSimilarly, it is monotonically  decreasing  if m හ n implies f.m/  \\ue004 f.n/. A func-  \\ntion f.n/  is strictly  increasing  if m < n  implies f.m/ < f.n/  and strictly  de-  \\ncreasing  if m<n  implies f.m/>f.n/ .', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 84}),\n",
       "  'ba2ccf06-4c04-45d1-8b91-e25300a5119b': Document(page_content=' implies f.m/>f.n/ . \\nFloors  and  ceilings  \\nFor any real number x , we denote the greatest integer less than or equal  to x by bx c \\n(read  <the  üoor  of x =) and the least integer greater than or equal to x by dx e (read \\n<the ceiling of x =).  The  üoor  function  is monotonically  increasing,  as is the  ceiling \\nfunction. \\nFloors and ceilings obey the following properties. For any integer n, we have \\nbnc D  n D dne : (3.1)  \\nFor all real x , we have ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 84}),\n",
       "  '48673105-eccf-4266-b804-41e6c8011b26': Document(page_content='64  Chapter  3 Characterizing  Running  Times  \\nx \\ue003 1 <  bx c හ  x හ dx e < x  C 1:  (3.2)  \\nWe also have \\n\\ue003 bx c D d\\ue003x e ; (3.3)  \\nor equivalently, \\n\\ue003 dx e D b\\ue003x c : (3.4)  \\nFor any real number x \\ue004 0 and integers a;b>0 , we have \\nå dx=ae \\nb æ \\nD å x \\nab  æ \\n; (3.5)  \\nç bx=ac \\nb è \\nD ç x \\nab  è \\n; (3.6)  \\nå a \\nb æ \\nහ a C .b \\ue003 1/ \\nb ; (3.7)  \\nç a \\nb è \\n\\ue004 a \\ue003 .b \\ue003 1/ \\nb : (3.8)  \\nFor any', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 85}),\n",
       "  'b80b76e0-e340-42cb-9351-82f79c46bc54': Document(page_content=' (3.8)  \\nFor any integer n and real number x , we have \\nbn C x c D  n C bx c ; (3.9)  \\ndn C x e D  n C dx e : (3.10)  \\nModular  arithmetic  \\nFor any integer a and any positive integer n, the value a mod n is the remainder  \\n(or residue ) of the quotient a=n: \\na mod n D a \\ue003 n ba=nc : (3.11)  \\nIt follows that \\n0 හ a mod n<n;  (3.12)  \\neven when a is negative. \\nGiven  a well-deûned  notion  of the  remainder  of one  integer  when  divided  by  an-  \\nother, it is convenient to provide special notation  to indicate equality of remainders. \\nIf .a mod n/ D .b mod n/, we write a D b .mod n/ and say that a is equivalent  \\nto b, modulo n. In other words, a D b .mod n/', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 85}),\n",
       "  '8a66a40d-0c4e-4b89-bebc-721e0b37a588': Document(page_content=' other words, a D b .mod n/ if a and b have  the  same  remain-  \\nder when divided by n. Equivalently, a D b .mod n/ if and only if n is a divisor \\nof b \\ue003 a. We write a ¤ b .mod n/ if a is not equivalent to b, modulo n. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 85}),\n",
       "  '48ccc9c2-b12b-4be5-98c8-d996e39b6723': Document(page_content='3.3  Standard  notations  and  common  functions  65 \\nPolynomials  \\nGiven  a nonnegative  integer  d , a polynomial  in n of degree  d is a function p.n/  \\nof the form \\np.n/  D d X  \\ni D0 a i n i ; \\nwhere the constants a 0 ;a  1 ;:::;a  d are the coefﬁcients  of the polynomial and \\na d ¤ 0. A polynomial is asymptotically positive if and on ly if a d > 0. For an \\nasymptotically positive polynomial p.n/  of degree d , we have p.n/  D ‚.n  d /. For \\nany real constant a \\ue004 0, the function n a is monotonically increasing, and for any \\nreal constant a හ 0, the function n a is monotonically decreasing. We say that a \\nfunction f.n/  is polynomially  bounded  if f.n/  D O.n ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 86}),\n",
       "  'bf9905ce-2bbb-4042-8c97-1b8c99d7a937': Document(page_content=' f.n/  D O.n  k / for some constant k. \\nExponentials  \\nFor all real a>0 , m, and n, we have the following identities: \\na 0 D 1;  \\na 1 D a;  \\na \\ue0021 D 1=a  ; \\n.a m / n D a mn  ; \\n.a m / n D .a n / m ; \\na m a n D a mCn : \\nFor all n and a \\ue004 1, the function a n is monotonically increasing in n. When \\nconvenient, we assume that 0 0 D 1. \\nWe can relate the rates of growth of polynomials an d exponentials  by  the  fol-  \\nlowing fact. For all real constants a>1  and b, we have \\nlim \\nn!1  n b \\na n D 0;  \\nfrom which we can conclude that \\nn b D o.a  n /: (3.13)  \\nThus, any exponential function with a base strictly  greater than 1 grows', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 86}),\n",
       "  'c57bb3ed-0c59-492e-9d73-d0a5eb0440a3': Document(page_content=' function with a base strictly  greater than 1 grows faster than \\nany polynomial function. \\nUsing e to denote 2:71828:::, the  base  of the  natural-logarithm  function,  we  \\nhave for all real x , \\ne x D 1 C x C x 2 \\n2Š C x 3 \\n3Š C \\ue001 \\ue001 \\ue001 D  1  X  \\ni D0 x i \\niŠ ; ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 86}),\n",
       "  'c82e896e-0a0d-4a0b-8182-db32519ed9f4': Document(page_content='66  Chapter  3 Characterizing  Running  Times  \\nwhere <Š= denotes  the  factorial  function  deûned  later  in this  sectio n. For all real x , \\nwe have the inequality \\n1 C x හ e x ; (3.14)  \\nwhere equality holds only when x D 0. When jx j හ  1, we have the approximation \\n1 C x හ e x හ 1 C x C x 2 : (3.15)  \\nWhen x !  0, the approximation of e x by 1 C x is quite good: \\ne x D 1 C x C ‚.x  2 /: \\n(In this equation, the asymptotic notation is used to describe the limiting behavior \\nas x !  0 rather than as x ! 1 .) We have for all x , \\nlim \\nn!1  \\ue002 \\n1 C x \\nn Í n \\nD e x : (3.16)  \\nLogarithms  \\nWe use the following notations: \\nl', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 87}),\n",
       "  '78be1fd6-3989-49fe-884d-da59db283c44': Document(page_content='We use the following notations: \\nlg n D log 2 n (binary logarithm) , \\nln n D log e n (natural logarithm) , \\nlg k n D .lg n/ k (exponentiation) , \\nlg lg n D lg.lg n/ (composition) . \\nWe adopt the following notational convention: in th e absence of parentheses, a \\nlogarithm  function  applies  only  to the  next  term  in the  formu la, so that lg n C 1 \\nmeans .lg n/ C 1 and not lg.n C 1/. \\nFor any constant b > 1 , the function log b n is undeûned  if n හ 0, strictly \\nincreasing if n>0 , negative if 0<n<1 , positive if n>1 , and 0 if n D 1. For \\nall real a>0 , b>0 , c>0 , and n, we have \\na D b log b a ; (3.17)  \\nlog c .ab/  D log c', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 87}),\n",
       "  '46c1ee97-cb99-4a75-9d37-788a20b41aca': Document(page_content='\\nlog c .ab/  D log c a C log c b;  (3.18)  \\nlog b a n D n log b a;  \\nlog b a D log c a \\nlog c b ; (3.19)  \\nlog b .1=a/  D \\ue003  log b a;  (3.20)  \\nlog b a D 1 \\nlog a b ; \\na log b c D c log b a ; (3.21)  \\nwhere, in each equation above, logarithm bases are not 1. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 87}),\n",
       "  '1f68f8e5-2d23-4577-853b-bbca1bf0e10f': Document(page_content='3.3  Standard  notations  and  common  functions  67 \\nBy  equation  (3.19),  changing  the  base  of a logarithm  from  one  constant  to an-  \\nother changes the value of the logarithm by only a constant factor. Consequently, \\nwe often use the notation <lg n= when  we  don’t  care  about  constant  factors,  such  \\nas in O-notation.  Computer  scientists  ûnd  2 to be the  most  natural  base  for  loga-  \\nrithms because so many algorithms and data structur es involve splitting a problem \\ninto two parts. \\nThere is a simple series expansion for ln .1 C x/  when jx j <1: \\nln.1 C x/  D x \\ue003 x 2 \\n2 C x 3 \\n3 \\ue003 x 4 \\n4 C x 5 \\n5 \\ue003 \\ue001 \\ue001 \\ue001  : (3.22)  \\nWe also have the following inequalities for x> ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 88}),\n",
       "  '5d1a95d0-f38b-45df-8b38-55515d755dd2': Document(page_content='We also have the following inequalities for x>  \\ue0031: \\nx \\n1 C x හ ln.1 C x/  හ x;  (3.23)  \\nwhere equality holds only for x D 0. \\nWe say that a function f.n/  is polylogarithmically  bounded  if f.n/  D O.lg k n/ \\nfor some constant k. We can relate the growth of polynomials and polyl ogarithms \\nby substituting lg n for n and 2 a for a in equation  (3.13).  For  all  real  constants  \\na>0  and b, we have \\nlg b n D o.n  a /: (3.24)  \\nThus, any positive polynomial function grows faster  than any polylogarithmic  func-  \\ntion. \\nFactorials  \\nThe notation nŠ (read <n factorial=)  is deûned  for  integers  n \\ue004 0 as \\nnŠ D ( \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 88}),\n",
       "  '77578515-f49f-4883-8b0e-5dbae282bfef': Document(page_content=' as \\nnŠ D ( \\n1 if n D 0;  \\nn \\ue001 .n \\ue003 1/Š  if n>0:  \\nThus, nŠ D 1 \\ue001 2 \\ue001 3 \\ue001 \\ue001 \\ue001  n. \\nA weak upper bound on the factorial function is nŠ හ n n , since each of the n \\nterms in the factorial product is at most n. Stirling’s  approximation , \\nnŠ D p \\n2�n  \\ue002 n \\ne Í n Î \\n1 C ‚ Î 1 \\nn ÏÏ  \\n; (3.25)  \\nwhere e is the base of the natural logarithm, gives us a ti ghter upper bound, and a \\nlower  bound  as well.  Exercise  3.3-4  asks  you  to prove  the  three facts \\nnŠ D o.n  n /; (3.26)  \\nnŠ D !.2  n /; (3', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 88}),\n",
       "  'cffb5bbb-af06-4e59-97d3-9db9445941c2': Document(page_content=' D !.2  n /; (3.27)  \\nlg.nŠ/  D ‚.n  lg n/;  (3.28)  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 88}),\n",
       "  'ee62c1a9-4333-4b19-9cc6-dc2e1c720502': Document(page_content='68  Chapter  3 Characterizing  Running  Times  \\nwhere  Stirling’s  approximation  is helpful  in proving  equation  (3.28).  The  following  \\nequation also holds for all n \\ue004 1: \\nnŠ D p \\n2�n  \\ue002 n \\ne Í n \\ne ˛ n (3.29)  \\nwhere \\n1 \\n12n  C 1 <˛  n < 1 \\n12n  : \\nFunctional  iteration  \\nWe use the notation f .i/  .n/  to denote the function f.n/  iteratively applied i times \\nto an initial value of n. Formally, let f.n/  be a function  over  the  reals.  For  non-  \\nnegative integers i , we  recursively  deûne  \\nf .i/  .n/  D ( \\nn if i D 0;  \\nf.f  .i \\ue0021/  .n//  if i >0:  (3.30)', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 89}),\n",
       "  'b9169ca3-7fb5-42b3-a987-8c93ae83472d': Document(page_content=' i >0:  (3.30)  \\nFor example, if f.n/  D 2n, then f .i/  .n/  D 2 i n. \\nThe  iterated  logarithm  function  \\nWe use the notation lg \\ue003 n (read <log star of n=) to denote  the  iterated  logarithm,  de-  \\nûned  as follows.  Let  lg .i/  n be as deûned  above,  with  f.n/  D lg n. Because  the  log-  \\narithm  of a nonpositive  number  is undeûned,  lg .i/  n is deûned  only  if lg .i \\ue0021/  n>0 . \\nBe sure to distinguish lg .i/  n (the logarithm function applied i times in succession, \\nstarting with argument n) from lg i n (the logarithm of n raised to the i th power). \\nThen  we  deûne  the  iterated  log', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 89}),\n",
       "  'e65ba438-ae45-44de-97ca-83678d8f1cb7': Document(page_content=' deûne  the  iterated  logarithm  function  as \\nlg \\ue003 n D min ˚ \\ni \\ue004 0 W lg .i/  n හ 1 \\ue009 \\n: \\nThe iterated logarithm is a very  slowly growing function: \\nlg \\ue003 2 D 1;  \\nlg \\ue003 4 D 2;  \\nlg \\ue003 16  D 3;  \\nlg \\ue003 65536  D 4;  \\nlg \\ue003 .2 65536  / D 5:  \\nSince the number of atoms in the observable univers e is estimated to be about 10  80  , \\nwhich is much less than 2 65536  D 10  65536=  lg 10  \\ue002 10  19;728  , we rarely encounter an \\ninput size n for which lg \\ue003 n>5 . ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 89}),\n",
       "  '45f97cf9-1a3f-4b55-b0c5-6e708d9a8e7e': Document(page_content='3.3  Standard  notations  and  common  functions  69 \\nFibonacci  numbers  \\nWe  deûne  the  Fibonacci  numbers  F i , for i \\ue004 0, as follows: \\nF i D Ĩ \\n0 if i D 0;  \\n1 if i D 1;  \\nF i \\ue0021 C F i \\ue0022 if i \\ue004 2:  (3.31)  \\nThus,  after  the  ûrst  two,  each  Fibonacci  number  is the  sum  of the two previous \\nones, yielding the sequence \\n0; 1; 1; 2; 3; 5; 8; 13;  21;  34;  55;  :::  : \\nFibonacci numbers are related to the golden  ratio  � and its conjugate y � , which are \\nthe two roots of the equation \\nx 2 D x C 1:  \\nAs  Exercise  3.3-7  asks  you  to prove,  the  golden  ratio  is given  by ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 90}),\n",
       "  'cfa507ae-5f22-48e4-818e-5f949f84b7ab': Document(page_content='  golden  ratio  is given  by \\n� D 1 C p \\n5 \\n2 (3.32)  \\nD 1:61803:::  ; \\nand its conjugate, by \\ny � D 1 \\ue003 p \\n5 \\n2 (3.33)  \\nD \\ue003:61803:::  : \\nSpeciûcally,  we  have  \\nF i D � i \\ue003 y � i \\np \\n5 ; \\nwhich  can  be proved  by  induction  (Exercise  3.3-8).  Since  ˇ ˇ y � ˇ ˇ <1, we have \\nˇ ˇ y � i ˇ ˇ \\np \\n5 < 1 p \\n5 \\n< 1 \\n2 ; \\nwhich implies that \\nF i D ç � i \\np \\n5 C 1 \\n2 è \\n; (3.34)  \\nwhich is to say that the i th Fibonacci number F i is equal to � i = p', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 90}),\n",
       "  'afb92eb1-2796-4b9a-8149-a07d73e5ef53': Document(page_content=' number F i is equal to � i = p \\n5 rounded to the \\nnearest integer. Thus, Fibonacci numbers grow expon entially. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 90}),\n",
       "  '8a58307b-5777-42c5-8433-0e47dfb92280': Document(page_content='70  Chapter  3 Characterizing  Running  Times  \\nExercises  \\n3.3-1  \\nShow that if f.n/  and g.n/  are monotonically increasing functions, then so are  \\nthe functions f.n/  C g.n/  and f.g.n// , and if f.n/  and g.n/  are in addition \\nnonnegative, then f.n/  \\ue001 g.n/  is monotonically increasing. \\n3.3-2  \\nProve that b˛ncCd.1 \\ue003 ˛/ne D  n for any integer n and real number ˛ in the range \\n0 හ ˛ හ 1. \\n3.3-3  \\nUse  equation  (3.14)  or other  means  to show  that  .n C o.n//  k D ‚.n  k / for any real \\nconstant k. Conclude that dne k D ‚.n  k / and bnc k D ‚.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 91}),\n",
       "  'a136fefa-7b49-478a-9f20-375543474fce': Document(page_content=' k / and bnc k D ‚.n  k /. \\n3.3-4  \\nProve the following: \\na. Equation  (3.21).  \\nb. Equations  (3.26)3(3.28).  \\nc. lg.‚.n//  D ‚.lg n/. \\n? 3.3-5  \\nIs the function dlg neŠ polynomially  bounded?  Is the  function  dlg lg neŠ polynomi-  \\nally  bounded?  \\n? 3.3-6  \\nWhich is asymptotically larger: lg .lg \\ue003 n/ or lg \\ue003 .lg n/? \\n3.3-7  \\nShow that the golden ratio � and its conjugate y � both satisfy the equation \\nx 2 D x C 1. \\n3.3-8  \\nProve by induction that the i th Fibonacci  number  satisûes  the  equation  \\nF i D .�  i ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 91}),\n",
       "  '89625de1-6fee-48ea-867d-60e4376f2a55': Document(page_content=' \\nF i D .�  i \\ue003 y � i /= p \\n5;  \\nwhere � is the golden ratio and y � is its conjugate. \\n3.3-9  \\nShow that k lg k D ‚.n/  implies k D ‚.n=  lg n/. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 91}),\n",
       "  'a31d5039-d58b-474b-8344-85eaf549e393': Document(page_content='Problems for Chapter 3 71 \\nProblems  \\n3-1  Asymptotic  behavior  of polynomials  \\nLet \\np.n/  D d X  \\ni D0 a i n i ; \\nwhere a d > 0, be a degree-d polynomial in n, and let k be a constant. Use the \\ndeûnitions  of the  asymptotic  notations  to prove  the  followi ng properties. \\na. If k \\ue004 d , then p.n/  D O.n  k /. \\nb. If k හ d , then p.n/  D �.n  k /. \\nc. If k D d , then p.n/  D ‚.n  k /. \\nd. If k>d  , then p.n/  D o.n  k /. \\ne. If k<d  , then p.n/  D !.n  k /. \\n3-2  Relative  asymptotic  growths  \\nIndicate, for each', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 92}),\n",
       "  '20dde93d-b8fd-47c7-9fc3-86b7ed9012c7': Document(page_content=' growths  \\nIndicate, for each pair of expressions .A;B/  in the table below whether A is O, o, \\n�, !, or ‚ of B . Assume that k \\ue004 1, � > 0 , and c>1  are constants. Write your \\nanswer in the form of the table with <yes= or <no= written in each box. \\nA B O o � ! ‚ \\na. lg k n n \\ue001 \\nb. n k c n \\nc. p n n sin n \\nd. 2 n 2 n=2  \\ne. n lg c c lg n \\nf. lg.nŠ/  lg.n n / \\n3-3  Ordering  by asymptotic  growth  rates  \\na. Rank  the  following  functions  by  order  of growth.  That  is, ûnd  an arrange-  \\nment g 1 ;g  2 ;:::;g  30  of the functions satisfying g 1 D �.g  2 /, g 2 D �.g ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 92}),\n",
       "  '5ced0656-6eec-4e2b-8222-ffef0eaf1bd5': Document(page_content=' 2 /, g 2 D �.g  3 /, . . . , \\ng 29  D �.g  30  /. Partition your list into equivalence classes such  that functions \\nf.n/  and g.n/  belong to the same class if and only if f.n/  D ‚.g.n// . ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 92}),\n",
       "  '0ec015f0-5eb4-4bc4-a76b-90d700ee49a2': Document(page_content='72  Chapter  3 Characterizing  Running  Times  \\nlg.lg \\ue003 n/ 2 lg \\ue002 n . p \\n2/ lg n n 2 nŠ .lg n/Š  \\n.3=2/  n n 3 lg 2 n lg.nŠ/  2 2 n n 1=  lg n \\nln ln n lg \\ue003 n n \\ue001 2 n n lg lg n ln n 1 \\n2 lg n .lg n/ lg n e n 4 lg n .n C 1/Š  p \\nlg n \\nlg \\ue003 .lg n/ 2 p  2 lg n n 2 n n lg n 2  2 nC1 \\nb. Give  an example  of a single  nonnegative  function  f.n/  such  that  for  all  func-  \\ntions g i .n/  in part (a), f.n/  is neither O.g  i .n//  nor �.g  i .n//. \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 93}),\n",
       "  '90e3648e-c44d-4d4e-9523-28b6605f8cba': Document(page_content='.g  i .n//. \\n3-4  Asymptotic  notation  properties  \\nLet f.n/  and g.n/  be asymptotically positive functions. Prove or disp rove each of \\nthe following conjectures. \\na. f.n/  D O.g.n//  implies g.n/  D O.f.n// . \\nb. f.n/  C g.n/  D ‚.min ff.n/;g.n/ g/. \\nc. f.n/  D O.g.n//  implies lg f.n/  D O.lg g.n// , where lg g.n/  \\ue004 1 and \\nf.n/  \\ue004 1 for  all  sufûciently  large  n. \\nd. f.n/  D O.g.n//  implies 2 f .n/  D O ã \\n2 g.n/  ä \\n. \\ne. f.n/  D O..f.n//  2 /', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 93}),\n",
       "  'eab7797c-e89c-4146-b5ae-7888051db9ac': Document(page_content=' D O..f.n//  2 /. \\nf. f.n/  D O.g.n//  implies g.n/  D �.f  .n// . \\ng. f.n/  D ‚.f.n=2// . \\nh. f.n/  C o.f.n//  D ‚.f.n// . \\n3-5  Manipulating  asymptotic  notation  \\nLet f.n/  and g.n/  be asymptotically positive functions. Prove the fol lowing iden-  \\ntities: \\na. ‚.‚.f.n///  D ‚.f.n// . \\nb. ‚.f.n//  C O.f.n//  D ‚.f.n// . \\nc. ‚.f.n//  C ‚.g.n//  D ‚.f.n/  C g.n// . \\nd. ‚.f.n//  \\ue001 ‚.g.n//  D', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 93}),\n",
       "  'c39e3512-89e0-4487-899f-95b4c9242355': Document(page_content='� ‚.g.n//  D ‚.f.n/  \\ue001 g.n// . ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 93}),\n",
       "  '98e905d9-d4ed-4251-b87b-d670d61aecb2': Document(page_content='Problems for Chapter 3 73 \\ne. Argue that for any real constants a 1 ;b  1 > 0  and integer constants k 1 ;k  2 , the \\nfollowing asymptotic bound holds: \\n.a 1 n/ k 1 lg k 2 .a 2 n/ D ‚.n  k 1 lg k 2 n/:  \\n? f. Prove that for S ෂ Z, we have \\nX  \\nk2S ‚.f.k//  D ‚ \\ue001 X  \\nk2S f.k/  ! \\n; \\nassuming that both sums converge. \\n? g. Show that for S ෂ Z, the following asymptotic bound does not necessari ly \\nhold, even assuming that both products converge, by  giving a counterexample: \\nY  \\nk2S ‚.f.k//  D ‚ \\ue001 Y  \\nk2S f.k/  ! \\n: \\n3-6  Variations  on  O and  ˝ \\nSome  authors ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 94}),\n",
       "  '4dc58674-d486-4e04-8f29-1cf0105d1176': Document(page_content='  ˝ \\nSome  authors  deûne  �-notation  in a slightly  different  way  than  this  textbook  does. \\nWe’ll  use  the  nomenclature  1  � (read  <omega  inûnity=)  for  this  alternative  deûni-  \\ntion. We say that f.n/  D 1  �.g.n//  if there exists a positive constant c such that \\nf.n/  \\ue004 cg.n/  \\ue004 0 for  inûnitely  many  integers  n. \\na. Show that for any two asymptotically nonnegative fu nctions f.n/  and g.n/ , we \\nhave f.n/  D O.g.n//  or f.n/  D 1  �.g.n//  (or both). \\nb. Show that there exist two asymptotically nonnegativ e functions f.n/  and g.n/  \\nfor which neither f.n/  D O.g.n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 94}),\n",
       "  '3c30c80b-2ebf-4994-b7a1-ecd13bd62084': Document(page_content='.n/  D O.g.n//  nor f.n/  D �.g.n//  holds. \\nc. Describe the potential advantages and disadvantages  of using 1  �-notation  in-  \\nstead of �-notation  to characterize  the  running  times  of programs.  \\nSome  authors  also  deûne  O in a slightly  different  manner.  We’ll  use  O 0 for the \\nalternative  deûnition:  f.n/  D O 0 .g.n//  if and only if jf.n/ j D  O.g.n// . \\nd. What happens to each direction of the <if and only if= in Theor em  3.1  on  \\npage  56  if we  substitute  O 0 for O but still use �? \\nSome  authors  deûne  e O (read  <soft-oh=)  to mean  O with  logarithmic  factors  ig-  \\nnored: ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 94}),\n",
       "  'ce33f35a-89eb-4c19-887f-f9385ab04e31': Document(page_content='74  Chapter  3 Characterizing  Running  Times  \\ne O.g.n//  D ff.n/  W there exist positive constants c , k, and n 0 such that \\n0 හ f.n/  හ cg.n/  lg k .n/  for all n \\ue004 n 0 g : \\ne. Deûne  e � and e ‚ in a similar  manner.  Prove  the  corresponding  analog  to Theo-  \\nrem  3.1.  \\n3-7  Iterated  functions  \\nWe can apply the iteration operator \\ue003 used in the lg \\ue003 function to any monotonically \\nincreasing function f.n/  over the reals. For a given constant c 2 R, we  deûne  the  \\niterated function f \\ue003 \\nc by \\nf \\ue003 \\nc .n/  D min ˚ \\ni \\ue004 0 W f .i/  .n/  හ c \\ue009 \\n; \\nwhich  need  not', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 95}),\n",
       "  '48da734b-c03a-4e03-a368-87476c759b5e': Document(page_content=' \\n; \\nwhich  need  not  be well  deûned  in all  cases.  In other  words,  the  quantity f \\ue003 \\nc .n/  is \\nthe minimum number of iterated applications of the function f required to reduce \\nits argument down to c or less. \\nFor each of the functions f.n/  and constants c in the table below, give as tight \\na bound as possible on f \\ue003 \\nc .n/. If there is no i such that f .i/  .n/  හ c , write  <unde-  \\nûned=  as your  answer.  \\nf.n/  c f \\ue003 \\nc .n/  \\na. n \\ue003 1 0 \\nb. lg n 1 \\nc. n=2  1 \\nd. n=2  2 \\ne. p n 2 \\nf. p n 1 \\ng. n 1=3  2 \\nChapter  notes  \\nKnuth  [259]  traces  the  origin  of the  O', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 95}),\n",
       "  '5d590503-a8b2-46e8-83f4-b5442e4c322e': Document(page_content=' traces  the  origin  of the  O-notation  to a number-theory  text  by  P. Bach-  \\nmann  in 1892.  The  o-notation  was  invented  by  E. Landau  in 1909  for  his  discussio n \\nof the distribution of prime numbers. The � and ‚ notations were advocated by \\nKnuth  [265]  to correct  the  popular,  but  technically  sloppy,  practice  in the  litera-  \\nture of using O-notation  for  both  upper  and  lower  bounds.  As  noted  earlier  in \\nthis chapter, many people continue to use the O-notation  where  the  ‚-notation  is \\nmore  technically  precise.  The  soft-oh  notation  e O in Problem  3-6  was  introduced  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 95}),\n",
       "  '17dea060-acb6-47cd-903e-40910b2a87aa': Document(page_content='Notes for Chapter 3 75 \\nby  Babai,  Luks,  and  Seress  [31],  although  it was  originally  written as O\\ue006. Some \\nauthors  now  deûne  e O.g.n//  as ignoring factors that are logarithmic in g.n/ , rather \\nthan in n. With  this  deûnition,  we  can  say  that  n2  n D e O.2  n /, but  with  the  deû-  \\nnition  in Problem  3-6,  this  statement  is not  true.  Further  discussion of the history \\nand development of asymptotic notations appears in works by Knuth  [259,  265]  \\nand  Brassard  and  Bratley  [70].  \\nNot  all  authors  deûne  the  asymptotic  notations  in the  same  way, although the \\nvarious  deûnitions  agree  in most  common  situations.  Some  of the  alternative  def-  \\ninitions', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 96}),\n",
       "  '812792b0-347e-489b-a225-311659e07f79': Document(page_content=' the  alternative  def-  \\ninitions encompass functions that are not asymptoti cally nonnegative, as long as \\ntheir absolute values are appropriately bounded. \\nEquation  (3.29)  is due  to Robbins  [381].  Other  properties  of elementary  math-  \\nematical functions can be found in any good mathema tical reference, such as \\nAbramowitz  and  Stegun  [1]  or Zwillinger  [468],  or in a calcul us book, such as \\nApostol  [19]  or Thomas  et al.  [433].  Knuth  [259]  and  Graham,  Knuth,  and  Patash-  \\nnik  [199]  contain  a wealth  of material  on  discrete  mathemati cs as used in computer \\nscience. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 96}),\n",
       "  '5461588f-5705-4b04-99e1-a79c861fcb65': Document(page_content='4 Divide-and-Conquer  \\nThe  divide-and-conquer  method  is a powerful  strategy  for  designing asymptotically \\nefûcient  algorithms.  We  saw  an example  of divide-and-conquer  in Section  2.3.1  \\nwhen  learning  about  merge  sort.  In this  chapter,  we’ll  explo re applications of the \\ndivide-and-conquer  method  and  acquire  valuable  mathemati cal tools that you can \\nuse to solve the recurrences that arise when analyz ing divide-and-conquer  algo-  \\nrithms. \\nRecall  that  for  divide-and-conquer,  you  solve  a given  problem  (instance)  recur-  \\nsively.  If the  problem  is small  enough4the  base  case4you  just  solve  it directly  \\nwithout  recursing.  Otherwise4the  recursive  case4you  perform  three  character-  \\nistic steps: \\nDivide ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 97}),\n",
       "  '51214254-e65e-4435-b61e-ef24b582b325': Document(page_content=' \\nistic steps: \\nDivide  the problem into one or more subproblems that are s maller instances of the \\nsame problem. \\nConquer  the subproblems by solving them recursively. \\nCombine  the subproblem solutions to form a solution to the original problem. \\nA divide-and-conquer  algorithm  breaks  down  a large  problem  into  smaller  sub-  \\nproblems, which themselves may be broken down into even smaller subproblems, \\nand so forth. The recursion bottoms  out  when  it reaches  a base  case  and  the  sub-  \\nproblem is small enough to solve directly without f urther recursing. \\nRecurrences  \\nTo  analyze  recursive  divide-and-conquer  algorithms,  we’ll  need  some  mathemat-  \\nical tools. A recurrence  is an equation that describes a function in terms o f its \\nvalue on other, typically smaller, arguments. Recur rences go hand in hand with \\nthe  divide-and-conquer  method  because  they  give ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 97}),\n",
       "  '74a64cf8-3c9d-4ae1-a1c9-7945be59c1ba': Document(page_content='quer  method  because  they  give  us a natura l way to characterize \\nthe running times of recursive algorithms mathemati cally. You saw an example \\nof a recurrence  in Section  2.3.2  when  we  analyzed  the  worst-c ase running time of \\nmerge sort. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 97}),\n",
       "  '7613885e-c739-49a1-a58e-89cc75ae3d71': Document(page_content='Chapter  4 Divide-and-Conquer  77 \\nFor  the  divide-and-conquer  matrix-multiplication  algorithms  presented  in Sec-  \\ntions  4.1  and  4.2,  we’ll  derive  recurrences  that  describe  their  worst-case  running  \\ntimes.  To  understand  why  these  two  divide-and-conquer  algorithms perform the \\nway  they  do,  you’ll  need  to learn  how  to solve  the  recurrences  that describe their \\nrunning  times.  Sections  4.334.7  teach  several  methods  for  solving recurrences. \\nThese sections also explore the mathematics behind recurrences, which can give \\nyou  stronger  intuition  for  designing  your  own  divide-and-c onquer algorithms. \\nWe  want  to get  to the  algorithms  as soon  as possible.  So,  let’s  just cover a few \\nrecurrence  basics  now,  and  then  we’ll  look ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 98}),\n",
       "  'b7680502-ba8d-4ba6-a31a-fb994edd9410': Document(page_content='  then  we’ll  look  more  deeply  at recurrences, especially \\nhow  to solve  them,  after  we  see  the  matrix-multiplication  examples. \\nThe general form of a recurrence is an equation or inequality that describes a \\nfunction over the integers or reals using the funct ion itself. It contains two or more \\ncases, depending on the argument. If a case involve s the recursive invocation of the \\nfunction on different (usually smaller) inputs, it is a recursive  case. If a case does \\nnot involve a recursive invocation, it is a base  case. There may be zero, one, or \\nmany functions that satisfy the statement of the re currence. The recurrence is well  \\ndeﬁned  if there  is at least  one  function  that  satisûes  it, and  ill deﬁned  otherwise. \\nAlgorithmic  recurrences  \\nWe’ll  be particularly  interested  in recurrences  that  descr ibe the running times of \\ndivide-and-conquer  algorithms. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 98}),\n",
       "  'dae56b50-e815-42e2-98c7-097a872dc0fd': Document(page_content='ide-and-conquer  algorithms.  A recurrence  T.n/  is algorithmic  if, for every \\nsufûciently  large  threshold  constant n 0 >0, the following two properties hold: \\n1. For  all  n<n  0 , we have T.n/  D ‚.1/ . \\n2. For all n \\ue004 n 0 , every  path  of recursion  terminates  in a deûned  base  case  within \\na ûnite  number  of recursive  invocations.  \\nSimilar to how we sometimes abuse asymptotic notati on (see page  60),  when  a \\nfunction  is not  deûned  for  all  arguments,  we  understand  that  this  deûnition  is con-  \\nstrained to values of n for which T.n/  is deûned.  \\nWhy would a recurrence T.n/  that  represents  a (correct)  divide-and-conquer  al-  \\ngorithm’s  worst-case  running ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 98}),\n",
       "  '757203a5-579b-49e5-baa3-6f4e1c7c1a82': Document(page_content='’s  worst-case  running  time  satisfy  these  properties  for  all  sufûciently  large  \\nthreshold  constants?  The  ûrst  property  says  that  there  exist constants c 1 ;c 2 such \\nthat 0 <c  1 හ T.n/  හ c 2 for n <n  0 . For every legal input, the algorithm must \\noutput  the  solution  to the  problem  it’s  solving  in ûnite  time  (see  Section  1.1).  Thus  \\nwe can let c 1 be the minimum amount of time to call and return fr om a procedure, \\nwhich must be positive, because machine instruction s need to be executed  to in-  \\nvoke a procedure. The running time of the algorithm  may not be deûned  for  some  \\nvalues of n if there  are  no  legal  inputs  of that  size,  but  it must  be deûned  for at \\nleast  one,  or else  the  <algorithm', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 98}),\n",
       "  '85c7952c-2a1f-4074-a84f-85189b3925a3': Document(page_content=',  or else  the  <algorithm=  doesn’t  solve  any  problem . Thus we can let c 2 be \\nthe  algorithm’s  maximum  running  time  on  any  input  of size  n <n  0 , where n 0 is ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 98}),\n",
       "  '65a7acea-5172-444a-ac34-4ab15d64643f': Document(page_content='78  Chapter  4 Divide-and-Conquer  \\nsufûciently  large  that  the  algorithm  solves  at least  one  problem of size less than n 0 . \\nThe  maximum  is well  deûned,  since  there  are  at most  a ûnite  number of inputs of \\nsize less than n 0 , and there is at least one if n 0 is sufûciently  large.  Consequently,  \\nT.n/  satisûes  the  ûrst  property.  If the  second  property  fails  to hold for T.n/ , then \\nthe  algorithm  isn’t  correct,  because  it would  end  up  in an inûnite recursive loop or \\notherwise fail to compute a solution. Thus, it stan ds to reason that a recurrence for \\nthe  worst-case  running  time  of a correct  divide-and-conque r algorithm would be \\nalgorithmic. \\nConventions  for  recurrences  \\nWe adopt the following convention: \\nWhenever', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 99}),\n",
       "  '0324fca0-4005-4abe-a399-92219c2948ef': Document(page_content='\\nWe adopt the following convention: \\nWhenever  a recurrence  is stated  without  an  explicit  base  case,  we  assume  \\nthat  the  recurrence  is algorithmic.  \\nThat  means  you’re  free  to pick  any  sufûciently  large  thresho ld constant n 0 for the \\nrange of base cases where T.n/  D ‚.1/ . Interestingly, the asymptotic solutions of \\nmost  algorithmic  recurrences  you’re  likely  to see  when  analyzing  algorithms  don’t  \\ndepend  on  the  choice  of threshold  constant,  as long  as it’s  large enough to make \\nthe  recurrence  well  deûned.  \\nAsymptotic  solutions  of algorithmic  divide-and-conquer  recurrences  also  don’t  \\ntend  to change  when  we  drop  any  üoors  or ceilings  in a recurrence  deûned  on  the  \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 99}),\n",
       "  'f19f6b4e-e2a2-4ee3-ba1d-0e76b9b36750': Document(page_content=' deûned  on  the  \\nintegers  to convert  it to a recurrence  deûned  on  the  reals.  Section  4.7  gives  a suf-  \\nûcient  condition  for  ignoring  üoors  and  ceilings  that  applies  to most  of the  divide-  \\nand-conquer  recurrences  you’re  likely  to see.  Consequently,  we’ll  frequently  state  \\nalgorithmic  recurrences  without  üoors  and  ceilings.  Doing  so generally  simpliûes  \\nthe statement of the recurrences, as well as any ma th that we do with them. \\nYou may sometimes see recurrences that are not equa tions, but rather  inequal-  \\nities, such as T.n/  හ 2T.n=2/  C ‚.n/ . Because such a recurrence states only \\nan upper bound on T.n/ , we express its solution using O-notation  rather  than  \\n�', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 99}),\n",
       "  'e1fc07bf-6b14-41c0-baba-1790e8cd9d32': Document(page_content='-notation  rather  than  \\n‚-notation.  Similarly,  if the  inequality  is reversed  to T.n/  \\ue004 2T.n=2/  C ‚.n/ , \\nthen, because the recurrence gives only a lower bou nd on T.n/ , we use �-notation  \\nin its solution. \\nDivide-and-conquer  and  recurrences  \\nThis  chapter  illustrates  the  divide-and-conquer  method  by  presenting and using \\nrecurrences  to analyze  two  divide-and-conquer  algorithms  for multiplying n \\ue005 n \\nmatrices.  Section  4.1  presents  a simple  divide-and-conque r algorithm that solves \\na matrix-multiplication  problem  of size  n by breaking it into four subproblems of \\nsize n=2, which it then solves recursively. The running tim e of the algorithm can \\nbe characterized by the recurrence ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 99}),\n",
       "  '6bcfa9c0-5085-4733-a7d6-40878faea620': Document(page_content='Chapter  4 Divide-and-Conquer  79 \\nT .n/  D 8T .n=2/  C ‚.1/ ;  \\nwhich turns out to have the solution T .n/  D ‚.n  3 /. Although  this  divide-and-  \\nconquer algorithm is no faster than the straightfor ward method that uses a triply \\nnested  loop,  it leads  to an asymptotically  faster  divide-and-conquer  algorithm  due  \\nto V. Strassen,  which  we’ll  explore  in Section  4.2.  Strassen’s  remarkable  algorithm  \\ndivides a problem of size n into seven subproblems of size n=2  which it solves \\nrecursively.  The  running  time  of Strassen’s  algorithm  can  be described by the \\nrecurrence \\nT .n/  D 7T .n=2/  C ‚.n  2 / ; \\nwhich has the solution T .n/  D ‚.n  lg', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 100}),\n",
       "  'a4c25fef-5883-4bf2-b179-8ebdbcdac648': Document(page_content='/  D ‚.n  lg 7 / D O.n  2:81  /. Strassen’s  algorithm  beats  \\nthe straightforward looping method asymptotically. \\nThese  two  divide-and-conquer  algorithms  both  break  a probl em of size n into \\nseveral subproblems of size n=2. Although  it is common  when  using  divide-and-  \\nconquer for all the subproblems to have the same si ze, that isn’t  always  the  case.  \\nSometimes  it’s  productive  to divide  a problem  of size  n into  subproblems  of differ-  \\nent sizes, and then the recurrence describing the r unning time  reüects  the  irregular-  \\nity.  For  example,  consider  a divide-and-conquer  algorithm  that divides a problem \\nof size n into one subproblem of size n=3  and another of size 2n=3 , taking ‚.n/  \\ntime to divide the problem and combine', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 100}),\n",
       "  '9c418d22-c3be-4405-ab49-1d3404c5c297': Document(page_content='  \\ntime to divide the problem and combine the solution s to the subproblems. Then the \\nalgorithm’s  running  time  can  be described  by  the  recurrence  \\nT .n/  D T .n=3/  C T .2n=3/  C ‚.n/ ;  \\nwhich turns out to have solution T .n/  D ‚.n  lg n/. We’ll  even  see  an algorithm  in \\nChapter 9 that solves a problem of size n by recursively solving a subproblem of \\nsize n=5  and another of size 7n=10 , taking ‚.n/  time for the divide and combine \\nsteps.  Its  performance  satisûes  the  recurrence  \\nT .n/  D T .n=5/  C T .7n=10/  C ‚.n/ ;  \\nwhich has solution T .n/  D ‚.n/ . \\nAlthough  divide-and-conquer  algorithms  usually  create  subproblems with sizes \\na constant', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 100}),\n",
       "  '1794a2dd-ca6c-428d-9fae-dabaf43a473b': Document(page_content='  subproblems with sizes \\na constant  fraction  of the  original  problem  size,  that’s  not  always the case. For \\nexample, a recursive version of linear search (see Exercise 2.1-4)  creates  just  one  \\nsubproblem, with one element less than the original  problem. Each recursive call \\ntakes constant time plus the time to recursively so lve a subproblem with one less \\nelement, leading to the recurrence \\nT .n/  D T .n  \\ue003 1/ C ‚.1/ ;  \\nwhich has solution T .n/  D ‚.n/. Nevertheless,  the  vast  majority  of efûcient  \\ndivide-and-conquer  algorithms  solve  subproblems  that  are  a constant fraction of \\nthe  size  of the  original  problem,  which  is where  we’ll  focus  our efforts. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 100}),\n",
       "  '6116e7b3-b30c-4486-b5d9-dc44e69278e0': Document(page_content='80  Chapter  4 Divide-and-Conquer  \\nSolving  recurrences  \\nAfter  learning  about  divide-and-conquer  algorithms  for  matrix multiplication in \\nSections  4.1  and  4.2,  we’ll  explore  several  mathematical  tools  for  solving  recur-  \\nrences4that  is, for  obtaining  asymptotic  ‚-, O-, or �-bounds  on  their  solutions.  \\nWe  want  simple-to-use  tools  that  can  handle  the  most  commonly  occurring  situa-  \\ntions. But we also want general tools that work, pe rhaps with a little more effort, \\nfor less common cases. This chapter offers four met hods for solving recurrences: \\n\\ue001 In the substitution  method  (Section  4.3),  you  guess  the  form  of a bound  and  \\nthen use mathematical induction to prove your guess  correct and  solve  for  con-  \\nstants. This method is perhaps the most robust meth', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 101}),\n",
       "  '66edcd8c-55aa-4b5c-b039-57bdc6dcc879': Document(page_content='ants. This method is perhaps the most robust meth od for solving recurrences, \\nbut it also requires you to make a good guess and t o produce an inductive proof. \\n\\ue001 The recursion-tree  method  (Section  4.4)  models  the  recurrence  as a tree  whose  \\nnodes represent the costs incurred at various level s of the recursion. To solve \\nthe recurrence, you determine the costs at each lev el and add them up, perhaps \\nusing techniques for bounding summations from Secti on A.2. Even  if you  don’t  \\nuse this method to formally prove a bound, it can b e helpful in guessing the form \\nof the bound for use in the substitution method. \\n\\ue001 The master  method  (Sections  4.5  and  4.6)  is the  easiest  method,  when  it applies . \\nIt provides bounds for recurrences of the form \\nT.n/  D aT.n=b/  C f.n/;  \\nwhere a >0  and b >1  are constants', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 101}),\n",
       "  '64180cd8-bdd4-4966-8365-2d642099b1fb': Document(page_content=' >0  and b >1  are constants and f.n/  is a given <driving= function. \\nThis type of recurrence tends to arise more frequen tly in the study of algorithms \\nthan  any  other.  It characterizes  a divide-and-conquer  algorithm that creates \\na subproblems, each of which is 1=b  times the size of the original problem, \\nusing f.n/  time for the divide and combine steps. To apply the  master method, \\nyou need to memorize three cases, but once you do, you can easily determine \\nasymptotic  bounds  on  running  times  for  many  divide-and-con quer algorithms. \\n\\ue001 The Akra-Bazzi  method  (Section  4.7)  is a general  method  for  solving  divide-  \\nand-conquer  recurrences.  Although  it involves  calculus,  it can be used to attack \\nmore complicated recurrences than those addressed b y the master method. \\n4.1  Multiplying  square  matrices  \\nWe ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 101}),\n",
       "  '6a4d0f1f-181e-40d7-96f1-5e6490a20c9f': Document(page_content='  square  matrices  \\nWe  can  use  the  divide-and-conquer  method  to multiply  square  matrices.  If you’ve  \\nseen matrices before, then you probably know how to  multiply them.  (Otherwise,  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 101}),\n",
       "  '9b200cd5-6e97-4344-b958-ad7f43ddd06b': Document(page_content='4.1  Multiplying  square  matrices  81 \\nyou  should  read  Section  D.1.)  Let  A D .a ik  / and B D .b jk  / be square n \\ue005 n \\nmatrices. The matrix product C D A \\ue001 B is also an n \\ue005 n matrix, where for \\ni;j  D 1;2;:::;n , the .i;j/  entry of C is given by \\nc ij D n X  \\nkD1 a ik  \\ue001 b kj  : (4.1)  \\nGenerally,  we’ll  assume  that  the  matrices  are  dense , meaning that most of the n 2 \\nentries are not 0, as opposed to sparse , where most of the n 2 entries are 0 and the \\nnonzero entries can be stored more compactly than i n an n \\ue005 n array. \\nComputing the matrix C requires computing n 2 matrix entries, each of which is \\nthe sum of n pairwise products of input elements from A and B . The M', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 102}),\n",
       "  'fdf516a7-bdc0-4e0b-89bc-44942b8b29d6': Document(page_content=' of input elements from A and B . The M ATRIX- \\nMULTIPLY procedure implements this strategy in a straightfor ward manner, and \\nit generalizes the problem slightly. It takes as in put three n \\ue005 n matrices A, B , \\nand C , and it adds the matrix product A \\ue001 B to C , storing the result in C . Thus, it \\ncomputes C D C C A \\ue001 B , instead of just C D A \\ue001 B . If only the product A \\ue001 B is \\nneeded, just initialize all n 2 entries of C to 0 before calling the procedure, which \\ntakes an additional ‚.n  2 / time.  We’ll  see  that  the  cost  of matrix  multiplication  \\nasymptotically dominates this initialization cost. \\nMATRIX-MULTIPLY .A;B;C;n/  \\n1 for  i D 1 to n / / compute entries in each of n rows \\n2 for  j D 1 to n / / compute n entries in row i \\n3 for  k D 1 to n \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 102}),\n",
       "  '8a00c645-00ae-4ad0-af94-a324ee79f01f': Document(page_content='3 for  k D 1 to n \\n4 c ij D c ij C a ik  \\ue001 b kj  / / add  in another  term  of equation  (4.1)  \\nThe pseudocode for M ATRIX-MULTIPLY works as follows. The for  loop of \\nlines  134  computes  the  entries  of each  row  i , and within a given row i , the for  loop \\nof lines  234  computes  each  of the  entries  c ij for each column j . Each iteration of \\nthe for  loop  of lines  334  adds  in one  more  term  of equation  (4.1).  \\nBecause each of the triply nested for  loops runs for exactly n iterations, and \\neach  execution  of line  4 takes  constant  time,  the  MATRIX-MULTIPLY procedure \\noperates in ‚.n  3 / time. Even if we add in the ‚.n  2 / time for initializing C to 0, \\nthe running time is still ‚.n ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 102}),\n",
       "  '9c9292da-c09f-4f9c-9798-9ec57061bab7': Document(page_content='the running time is still ‚.n  3 /. \\nA simple  divide-and-conquer  algorithm  \\nLet’s  see  how  to compute  the  matrix  product  A \\ue001 B using  divide-and-conquer.  For  \\nn>1 , the divide step partitions the n \\ue005 n matrices into four n=2  \\ue005 n=2  submatrices. \\nWe’ll  assume  that  n is an exact power of 2, so that as the algorithm recurses, we \\nare guaranteed that the submatrix dimensions are in teger. (Exercise  4.1-1  asks  you  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 102}),\n",
       "  '3b06587b-eb7b-4cda-841b-90b5c6654919': Document(page_content='82  Chapter  4 Divide-and-Conquer  \\nto relax this assumption.) As with M ATRIX-MULTIPLY, we’ll  actually  compute  \\nC D C C A \\ue001 B . But  to simplify  the  math  behind  the  algorithm,  let’s  assume  that C \\nhas been initialized to the zero matrix, so that we  are indeed computing C D A \\ue001 B . \\nThe divide step views each of the n \\ue005 n matrices A, B , and C as four n=2  \\ue005 n=2  \\nsubmatrices: \\nA D Î A 11  A 12  \\nA 21  A 22  Ï \\n; B  D Î B 11  B 12  \\nB 21  B 22  Ï \\n; C  D Î C 11  C 12  \\nC 21  C 22  Ï \\n: (4.2)  \\nThen we can write the matrix product as \\nÎ C 11  C 12  \\nC 21  C 22  Ï \\nD �', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 103}),\n",
       "  'f6a50e01-1efe-4b0d-a458-3d9d82675d2b': Document(page_content='  C 22  Ï \\nD Î A 11  A 12  \\nA 21  A 22  ÏÎ  B 11  B 12  \\nB 21  B 22  Ï \\n(4.3)  \\nD Î A 11  \\ue001 B 11  C A 12  \\ue001 B 21  A 11  \\ue001 B 12  C A 12  \\ue001 B 22  \\nA 21  \\ue001 B 11  C A 22  \\ue001 B 21  A 21  \\ue001 B 12  C A 22  \\ue001 B 22  Ï \\n; (4.4)  \\nwhich corresponds to the equations \\nC 11  D A 11  \\ue001 B 11  C A 12  \\ue001 B 21  ; (4.5)  \\nC 12  D A 11  \\ue001 B 12  C A 12  \\ue001 B 22  ; (4.6)  \\nC 21  D A 21  \\ue001 B 11  C A 22  \\ue001 B 21  ; (4.7)  \\nC 22 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 103}),\n",
       "  '8b21ff1d-bcec-4305-8a98-e9f8dc8b3556': Document(page_content='4.7)  \\nC 22  D A 21  \\ue001 B 12  C A 22  \\ue001 B 22  : (4.8)  \\nEquations  (4.5)3(4.8)  involve  eight  n=2  \\ue005 n=2  multiplications and four additions \\nof n=2  \\ue005 n=2  submatrices. \\nAs we look to transform these equations to an algor ithm that can be described \\nwith pseudocode, or even implemented for real, ther e are two common approaches \\nfor implementing the matrix partitioning. \\nOne  strategy  is to allocate  temporary  storage  to hold  A’s four  submatrices  A 11  , \\nA 12  , A 21  , and A 22  and B ’s four  submatrices  B 11  , B 12  , B 21  , and B 22  . Then copy \\neach element in A and B to its corresponding location in the appropriate su bmatrix. \\nAfter the recursive conquer step, copy the elements  in each of C ’s four  sub', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 103}),\n",
       "  'cef47e88-ad94-4174-ab0e-65009e328155': Document(page_content=' in each of C ’s four  submatrices  \\nC 11  , C 12  , C 21  , and C 22  to their corresponding locations in C . This approach takes \\n‚.n  2 / time, since 3n  2 elements are copied. \\nThe second approach uses index calculations and is faster and more practical. A \\nsubmatrix  can  be speciûed  within  a matrix  by  indicating  wher e within the matrix \\nthe submatrix lies without touching any matrix elem ents. Partitioning a matrix \\n(or recursively, a submatrix) only involves arithme tic on this location information, \\nwhich has constant size independent of the size of the matrix. Changes to the \\nsubmatrix elements update the original matrix, sinc e they occupy the same storage. \\nGoing  forward,  we’ll  assume  that  index  calculations  are  used  and  that  partition-  \\ning can be performed in ‚.1/  time.  Exercise  4.1-3  asks  you  to show  that  it makes ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 103}),\n",
       "  '66108fcb-89d5-431a-b2aa-09f5554ed9f3': Document(page_content=' you  to show  that  it makes  \\nno difference to the overall asymptotic running tim e of matrix multiplication,  how-  \\never,  whether  the  partitioning  of matrices  uses  the  ûrst  method of copying or the ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 103}),\n",
       "  '7e334eaf-3032-4848-ab24-706e5272ccea': Document(page_content='4.1  Multiplying  square  matrices  83 \\nsecond  method  of index  calculation.  But  for  other  divide-and-conquer  matrix  cal-  \\nculations, such as matrix addition, it can make a d ifference, as Exercise  4.1-4  asks  \\nyou to show. \\nThe procedure M ATRIX-MULTIPLY-RECURSIVE uses  equations  (4.5)3(4.8)  to \\nimplement  a divide-and-conquer  strategy  for  square-matri x multiplication. Like \\nMATRIX-MULTIPLY , the procedure M ATRIX-MULTIPLY-RECURSIVE computes \\nC D C C A \\ue001 B since, if necessary, C can be initialized to 0 before the procedure \\nis called in order to compute only C D A \\ue001 B . \\nMATRIX-MULTIPLY-RECURSIVE .A;B;C;n/  \\n1 if n == 1 \\n2 / / Base case. \\n3 c 11 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 104}),\n",
       "  '0b22806c-9c3b-40fe-9ea8-5874fbfbf1bf': Document(page_content=' / Base case. \\n3 c 11  D c 11  C a 11  \\ue001 b 11  \\n4 return  \\n5 / / Divide. \\n6 partition A, B , and C into n=2  \\ue005 n=2  submatrices \\nA 11  ;A  12  ;A  21  ;A  22  ; B 11  ;B  12  ;B  21  ;B  22  ; \\nand C 11  ;C  12  ;C  21  ;C  22  ; respectively \\n7 / / Conquer. \\n8 MATRIX-MULTIPLY-RECURSIVE .A  11  ;B  11  ;C  11  ;n=2/  \\n9 MATRIX-MULTIPLY-RECURSIVE .A  11  ;B  12  ;C  12  ;n=2/  \\n10  MATRIX-MULTIPLY-RECURSIVE .A  21  ;B  11  ;C  21  ;n=2/  \\n11  MATRIX-MULTIPLY-', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 104}),\n",
       "  '2c1f2b5d-c38c-4aa9-adb4-b72c53acf0cd': Document(page_content='  MATRIX-MULTIPLY-RECURSIVE .A  21  ;B  12  ;C  22  ;n=2/  \\n12  MATRIX-MULTIPLY-RECURSIVE .A  12  ;B  21  ;C  11  ;n=2/  \\n13  MATRIX-MULTIPLY-RECURSIVE .A  12  ;B  22  ;C  12  ;n=2/  \\n14  MATRIX-MULTIPLY-RECURSIVE .A  22  ;B  21  ;C  21  ;n=2/  \\n15  MATRIX-MULTIPLY-RECURSIVE .A  22  ;B  22  ;C  22  ;n=2/  \\nAs  we  walk  through  the  pseudocode,  we’ll  derive  a recurrence  to characterize \\nits running time. Let T.n/  be the  worst-case  time  to multiply  two  n \\ue005 n matrices \\nusing this procedure.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 104}),\n",
       "  'dc7026b1-c0d8-44f9-8468-aee0c74b7399': Document(page_content='� n matrices \\nusing this procedure. \\nIn the base case, when n D 1, line  3 performs  just  the  one  scalar  multiplica-  \\ntion and one addition, which means that T.1/  D ‚.1/ . As is our convention for \\nconstant base cases, we can omit this base case in the statement of the recurrence. \\nThe recursive case occurs when n>1. As  discussed,  we’ll  use  index  calcula-  \\ntions  to partition  the  matrices  in line  6, taking  ‚.1/  time.  Lines  8315  recursively  \\ncall M ATRIX-MULTIPLY-RECURSIVE a total  of eight  times.  The  ûrst  four  recur-  \\nsive  calls  compute  the  ûrst  terms  of equations  (4.5)3(4.8),  and the subsequent four \\nrecursive calls compute and add in the second terms . Each recursive call adds the \\nproduct of a submatrix', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 104}),\n",
       "  'bce53f5a-5337-4dd5-8573-eee3cda7112f': Document(page_content=' adds the \\nproduct of a submatrix of A and a submatrix of B to the appropriate submatrix ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 104}),\n",
       "  'cc5fd04f-5615-4b9d-ab9b-775deaea7d79': Document(page_content='84  Chapter  4 Divide-and-Conquer  \\nof C in place, thanks to index calculations. Because eac h recursive call multiplies \\ntwo n=2  \\ue005 n=2  matrices, thereby contributing T.n=2/  to the overall running time, \\nthe time taken by all eight recursive calls is 8T.n=2/ . There is no combine step, \\nbecause the matrix C is updated in place. The total time for the recursi ve case, \\ntherefore, is the sum of the partitioning time and the time for all the recursive calls, \\nor ‚.1/  C 8T.n=2/ . \\nThus, omitting the statement of the base case, our recurrence for the running \\ntime of M ATRIX-MULTIPLY-RECURSIVE is \\nT.n/  D 8T.n=2/  C ‚.1/:  (4.9)  \\nAs  we’ll  see  from  the  master  method  in Section  4.5,  recurrence  (4.9)  has', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 105}),\n",
       "  '1ae98c6c-ab7b-43ca-9bc6-a97cddbf0f9f': Document(page_content=' recurrence  (4.9)  has  the  solu-  \\ntion T.n/  D ‚.n  3 /, which means that it has the same asymptotic runni ng time as \\nthe straightforward M ATRIX-MULTIPLY procedure. \\nWhy is the ‚.n  3 / solution to this recurrence so much larger than the  ‚.n  lg n/ \\nsolution  to the  merge-sort  recurrence  (2.3)  on  page  41?  After all, the recurrence \\nfor merge sort contains a ‚.n/  term, whereas the recurrence for recursive matrix \\nmultiplication contains only a ‚.1/  term. \\nLet’s  think  about  what  the  recursion  tree  for  recurrence  (4.9) would look like \\nas compared with the recursion tree for merge sort,  illustrated  in Figure  2.5  on  \\npage  43.  The  factor  of 2 in the  merge-sort  recurrence  determines  how  many  ch', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 105}),\n",
       "  'a73b614b-065d-4178-9046-bd9e53f908a2': Document(page_content=' recurrence  determines  how  many  chil-  \\ndren each tree node has, which in turn determines h ow many terms contribute to the \\nsum at each level of the tree. In comparison, for t he recurrence (4.9)  for  MATRIX- \\nMULTIPLY-RECURSIVE , each internal node in the recursion tree has eigh t children, \\nnot two, leading to a <bushier= recursion tree with  many more leaves, despite the \\nfact that the internal nodes are each much smaller.  Consequently, the solution to \\nrecurrence  (4.9)  grows  much  more  quickly  than  the  solution  to recurrence  (2.3),  \\nwhich is borne out in the actual solutions: ‚.n  3 / versus ‚.n  lg n/. \\nExercises  \\nNote: You  may  wish  to read  Section  4.5  before  attempting  some  of these exercises. \\n4.1-1  \\nGeneralize  MATRIX-MULTIPLY-RECURSIVE to multiply', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 105}),\n",
       "  'a9f50516-29e2-4499-8b27-0f62d342097c': Document(page_content='ULTIPLY-RECURSIVE to multiply n \\ue005 n matrices for which \\nn is not necessarily an exact power of 2. Give  a recurrence  describing  its  running  \\ntime. Argue that it runs in ‚.n  3 / time in the worst case. \\n4.1-2  \\nHow quickly can you multiply a kn  \\ue005 n matrix (kn  rows and n columns) by an \\nn \\ue005 kn  matrix, where k \\ue004 1, using M ATRIX-MULTIPLY-RECURSIVE as a subrou-  \\ntine?  Answer  the  same  question  for  multiplying  an n \\ue005 kn  matrix by a kn  \\ue005 n \\nmatrix.  Which  is asymptotically  faster,  and  by  how  much?  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 105}),\n",
       "  'e4e80f73-b7d3-4069-8cd0-aaa1b185ceba': Document(page_content='4.2  Strassen’s  algorithm  for  matrix  multiplication  85 \\n4.1-3  \\nSuppose that instead of partitioning matrices by in dex calculation in M ATRIX- \\nMULTIPLY-RECURSIVE , you copy the appropriate elements of A, B , and C into \\nseparate n=2  \\ue005 n=2  submatrices A 11  , A 12  , A 21  , A 22  ; B 11  , B 12  , B 21  , B 22  ; and C 11  , \\nC 12  , C 21  , C 22  , respectively. After the recursive calls, you copy  the results from C 11  , \\nC 12  , C 21  , and C 22  back into the appropriate places in C . How  does  recurrence  (4.9)  \\nchange,  and  what  is its  solution?  \\n4.1-4  \\nWrite  pseudocode  for  a divide-and-conquer  algorithm  MATRIX-ADD-RECURSIVE \\nthat sums two n �', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 106}),\n",
       "  '8ba2ab2d-72ce-4956-9bf9-e2db783d3b98': Document(page_content='SIVE \\nthat sums two n \\ue005 n matrices A and B by partitioning each of them into four \\nn=2  \\ue005 n=2  submatrices and then recursively summing correspond ing pairs  of sub-  \\nmatrices. Assume that matrix partitioning uses ‚.1/-time  index  calculations.  \\nWrite  a recurrence  for  the  worst-case  running  time  of MATRIX-ADD-RECURSIVE , \\nand solve your recurrence. What happens if you use ‚.n  2 /-time  copying  to imple-  \\nment  the  partitioning  instead  of index  calculations?  \\n4.2  Strassen’s  algorithm  for  matrix  multiplication  \\nYou  might  ûnd  it hard  to imagine  that  any  matrix  multiplicati on algorithm could \\ntake less than ‚.n  3 / time,  since  the  natural  deûnition  of matrix  multiplication  re-  \\nquires n 3 scalar multiplications. Indeed, many', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 106}),\n",
       "  '38810e05-e40e-4b87-bc50-96bc212f9154': Document(page_content=' n 3 scalar multiplications. Indeed, many mathematicians  presumed that it \\nwas not possible to multiply matrices in o.n  3 / time  until  1969,  when  V. Strassen  \\n[424]  published  a remarkable  recursive  algorithm  for  multi plying n \\ue005 n matrices. \\nStrassen’s  algorithm  runs  in ‚.n  lg 7 / time. Since lg 7 D 2:8073549:::  , Strassen’s  \\nalgorithm runs in O.n  2:81  / time, which is asymptotically better than the ‚.n  3 / \\nMATRIX-MULTIPLY and M ATRIX-MULTIPLY-RECURSIVE procedures. \\nThe  key  to Strassen’s  method  is to use  the  divide-and-conque r idea from the \\nMATRIX-MULTIPLY-RECURSIVE procedure, but make the recursion tree less \\nbushy.  We’ll  actually  increase  the  work', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 106}),\n",
       "  '101d0772-b95d-452e-a830-2985b984d310': Document(page_content='�ll  actually  increase  the  work  for  each  divide  and  combine step by a \\nconstant factor, but the reduction in bushiness wil l pay off. We  won’t  reduce  the  \\nbushiness  from  the  eight-way  branching  of recurrence  (4.9)  all the way down to \\nthe  two-way  branching  of recurrence  (2.3),  but  we’ll  improv e it just a little, and \\nthat will make a big difference. Instead of perform ing eight recursive  multiplica-  \\ntions of n=2  \\ue005 n=2  matrices,  Strassen’s  algorithm  performs  only  seven.  The  cost \\nof eliminating one matrix multiplication is several  new additions and subtractions \\nof n=2  \\ue005 n=2  matrices, but still only a constant number. Rather than saying  <addi-  \\ntions  and  subtractions=  everywhere,  we’ll  adopt  the  common  terminology  of call-  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 106}),\n",
       "  'dfe810da-8b79-4246-a352-c65668679217': Document(page_content='86  Chapter  4 Divide-and-Conquer  \\ning them both <additions= because subtraction is st ructurally the same computation \\nas addition, except for a change of sign. \\nTo get an inkling how the number of multiplications  might be reduced, as well \\nas why reducing the number of multiplications might  be desirable  for  matrix  calcu-  \\nlations, suppose that you have two numbers x and y , and you want to calculate the \\nquantity x 2 \\ue003 y 2 . The straightforward calculation requires two mult iplications to \\nsquare x and y , followed by one subtraction (which you can think of as a <negative \\naddition=).  But  let’s  recall  the  old  algebra  trick  x 2 \\ue003 y 2 D x 2 \\ue003 xy  C xy  \\ue003 y 2 D \\nx.x  \\ue003 y/  C y.x  \\ue003 y/  D .x C y/.x  \\ue003 y/. Using this formulation of the desired \\nquantity, you could instead compute the sum x C', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 107}),\n",
       "  '24aaed57-dcd7-4a39-b2c3-316c15011e1f': Document(page_content='ity, you could instead compute the sum x C y and the difference x \\ue003 y and \\nthen multiply them, requiring only a single multipl ication and two additions. At \\nthe cost of an extra addition, only one multiplicat ion is needed  to compute  an ex-  \\npression that looks as if it requires two. If x and y are  scalars,  there’s  not  much  \\ndifference: both approaches require three scalar op erations. If x and y are large \\nmatrices, however, the cost of multiplying outweigh s the cost of adding, in which \\ncase  the  second  method  outperforms  the  ûrst,  although  not  asymptotically. \\nStrassen’s  strategy  for  reducing  the  number  of matrix  multiplications  at the  ex-  \\npense  of more  matrix  additions  is not  at all  obvious4perhaps  the  biggest  under-  \\nstatement in this book! As with M ATRIX-MULTIPLY-RECURSIVE, Strassen’s', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 107}),\n",
       "  '74f21ff0-a452-4f95-a2df-c89a520c6b65': Document(page_content='URSIVE, Strassen’s  al-  \\ngorithm  uses  the  divide-and-conquer  method  to compute  C D C C A \\ue001 B , where \\nA, B , and C are all n \\ue005 n matrices and n is an exact power of 2. Strassen’s  algo-  \\nrithm computes the four submatrices C 11  , C 12  , C 21  , and C 22  of C from equations \\n(4.5)3(4.8)  on  page  82  in four  steps.  We’ll  analyze  costs  as we go along to develop \\na recurrence T.n/  for  the  overall  running  time.  Let’s  see  how  it works:  \\n1. If n D 1, the matrices each contain a single element. Perfo rm a single scalar \\nmultiplication  and  a single  scalar  addition,  as in line  3 of MATRIX-MULTIPLY- \\nRECURSIVE , taking ‚.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 107}),\n",
       "  '96f45325-9c3a-4399-95bc-96d108d425a3': Document(page_content='\\nRECURSIVE , taking ‚.1/  time,  and  return.  Otherwise,  partition  the  input  ma-  \\ntrices A and B and output matrix C into n=2  \\ue005 n=2  submatrices,  as in equa-  \\ntion  (4.2).  This  step  takes  ‚.1/  time by index calculation, just as in M ATRIX- \\nMULTIPLY-RECURSIVE . \\n2. Create n=2  \\ue005 n=2  matrices S 1 ;S  2 ;:::;S  10  , each  of which  is the  sum  or dif-  \\nference  of two  submatrices  from  step  1. Create  and  zero  the  entries of seven \\nn=2  \\ue005 n=2  matrices P 1 ;P  2 ;:::;P  7 to hold seven n=2  \\ue005 n=2  matrix products. \\nAll 17  matrices can be created, and the P i initialized, in ‚.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 107}),\n",
       "  'a437331e-90c0-4028-8ac3-556548858f45': Document(page_content=' and the P i initialized, in ‚.n  2 / time. \\n3. Using  the  submatrices  from  step  1 and  the  matrices  S 1 ;S  2 ;:::;S  10  created in \\nstep 2, recursively compute each of the seven matri x products P 1 ;P  2 ;:::;P  7 , \\ntaking 7T.n=2/  time. \\n4. Update  the  four  submatrices  C 11  ;C  12  ;C  21  ;C  22  of the result matrix C by adding \\nor subtracting various P i matrices, which takes ‚.n  2 / time. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 107}),\n",
       "  'd8c2bd12-7795-4eb6-b3b7-352a4792e4ca': Document(page_content='4.2  Strassen’s  algorithm  for  matrix  multiplication  87 \\nWe’ll  see  the  details  of steps  234  in a moment,  but  we  already  have enough \\ninformation to set up a recurrence for the running time of Strassen’s  method.  As  is \\ncommon,  the  base  case  in step  1 takes  ‚.1/  time,  which  we’ll  omit  when  stating  \\nthe recurrence. When n > 1, steps  1, 2, and  4 take  a total  of ‚.n  2 / time, and \\nstep  3 requires  seven  multiplications  of n=2  \\ue005 n=2  matrices. Hence, we obtain the \\nfollowing  recurrence  for  the  running  time  of Strassen’s  algorithm: \\nT.n/  D 7T.n=2/  C ‚.n  2 /: (4.10)  \\nCompared with M ATRIX-MULTIPLY-', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 108}),\n",
       "  'ff563de5-5acc-492d-9615-7b4eaaae6400': Document(page_content=' M ATRIX-MULTIPLY-RECURSIVE, we  have  traded  off  one  recur-  \\nsive submatrix multiplication for a constant number  of submatrix  additions.  Once  \\nyou  understand  recurrences  and  their  solutions,  you’ll  be able  to see  why  this  trade-  \\noff actually leads to a lower asymptotic running ti me. By the master  method  in Sec-  \\ntion  4.5,  recurrence  (4.10)  has  the  solution  T.n/  D ‚.n  lg 7 / D O.n  2:81  /, beating \\nthe ‚.n  3 /-time  algorithms.  \\nNow,  let’s  delve  into  the  details.  Step  2 creates  the  followi ng 10  matrices: \\nS 1 D B 12  \\ue003 B 22  ; \\nS 2 D A 11  C A 12  ; \\nS 3 D A 21  C A 22  ; ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 108}),\n",
       "  '4d3c5cf0-ac9b-48b2-8d4e-7268ef4911e4': Document(page_content=' D A 21  C A 22  ; \\nS 4 D B 21  \\ue003 B 11  ; \\nS 5 D A 11  C A 22  ; \\nS 6 D B 11  C B 22  ; \\nS 7 D A 12  \\ue003 A 22  ; \\nS 8 D B 21  C B 22  ; \\nS 9 D A 11  \\ue003 A 21  ; \\nS 10  D B 11  C B 12  : \\nThis step adds or subtracts n=2  \\ue005 n=2  matrices 10  times, taking ‚.n  2 / time. \\nStep  3 recursively  multiplies  n=2  \\ue005 n=2  matrices 7 times  to compute  the  follow-  \\ning n=2  \\ue005 n=2  matrices, each of which is the sum or difference of  products of A \\nand B submatrices: \\nP 1 D A 11  \\ue001 S 1 .D A 11  \\ue001 B 12  \\ue003 A 11  \\ue001 B 22  /; \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 108}),\n",
       "  '10e2ee77-7732-4380-a480-f0af1ba02d4f': Document(page_content=' \\ue001 B 22  /; \\nP 2 D S 2 \\ue001 B 22  .D A 11  \\ue001 B 22  C A 12  \\ue001 B 22  /; \\nP 3 D S 3 \\ue001 B 11  .D A 21  \\ue001 B 11  C A 22  \\ue001 B 11  /; \\nP 4 D A 22  \\ue001 S 4 .D A 22  \\ue001 B 21  \\ue003 A 22  \\ue001 B 11  /; \\nP 5 D S 5 \\ue001 S 6 .D A 11  \\ue001 B 11  C A 11  \\ue001 B 22  C A 22  \\ue001 B 11  C A 22  \\ue001 B 22  /; \\nP 6 D S 7 \\ue001 S 8 .D A 12  \\ue001 B 21  C A 12  \\ue001 B 22  \\ue003 A 22  \\ue001 B 21  \\ue003 A 22  \\ue001 B 22  /; \\nP 7 D S 9 \\ue001 S 10  .D A 11  \\ue001 B 11 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 108}),\n",
       "  '5480995b-e086-4637-a118-f3ada3322ef9': Document(page_content='D A 11  \\ue001 B 11  C A 11  \\ue001 B 12  \\ue003 A 21  \\ue001 B 11  \\ue003 A 21  \\ue001 B 12  /: ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 108}),\n",
       "  'bbed0dd4-6c7e-4396-b848-d2785c74717c': Document(page_content='88  Chapter  4 Divide-and-Conquer  \\nThe only multiplications that the algorithm perform s are those  in the  middle  col-  \\numn  of these  equations.  The  right-hand  column  just  shows  what these products \\nequal  in terms  of the  original  submatrices  created  in step  1, but the terms are never \\nexplicitly calculated by the algorithm. \\nStep  4 adds  to and  subtracts  from  the  four  n=2  \\ue005 n=2  submatrices  of the  prod-  \\nuct C the various P i matrices  created  in step  3. We  start  with  \\nC 11  D C 11  C P 5 C P 4 \\ue003 P 2 C P 6 : \\nExpanding  the  calculation  on  the  right-hand  side,  with  the  expansion of each P i \\non its own line and vertically aligning terms that cancel out, we see that the update \\nto C 11  equals \\nA 11  \\ue001 B 11  C A 11 ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 109}),\n",
       "  'b34ec0ca-f265-4708-8e90-2aabeb6bd9c3': Document(page_content=' \\ue001 B 11  C A 11  \\ue001 B 22  C A 22  \\ue001 B 11  C A 22  \\ue001 B 22  \\n\\ue003 A 22  \\ue001 B 11  C A 22  \\ue001 B 21  \\n\\ue003 A 11  \\ue001 B 22  \\ue003 A 12  \\ue001 B 22  \\n\\ue003 A 22  \\ue001 B 22  \\ue003 A 22  \\ue001 B 21  C A 12  \\ue001 B 22  C A 12  \\ue001 B 21  \\nA 11  \\ue001 B 11  C A 12  \\ue001 B 21  ; \\nwhich  corresponds  to equation  (4.5).  Similarly,  setting  \\nC 12  D C 12  C P 1 C P 2 \\nmeans that the update to C 12  equals \\nA 11  \\ue001 B 12  \\ue003 A 11  \\ue001 B 22  \\nC A 11  \\ue001 B 22  C A 12  \\ue001 B 22  \\nA 11  \\ue001 B 12  C A', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 109}),\n",
       "  '53bb3855-13a5-4d71-8ee0-9948f5e7dfa2': Document(page_content=' 11  \\ue001 B 12  C A 12  \\ue001 B 22  ; \\ncorresponding  to equation  (4.6).  Setting  \\nC 21  D C 21  C P 3 C P 4 \\nmeans that the update to C 21  equals \\nA 21  \\ue001 B 11  C A 22  \\ue001 B 11  \\n\\ue003 A 22  \\ue001 B 11  C A 22  \\ue001 B 21  \\nA 21  \\ue001 B 11  C A 22  \\ue001 B 21  ; \\ncorresponding  to equation  (4.7).  Finally,  setting  \\nC 22  D C 22  C P 5 C P 1 \\ue003 P 3 \\ue003 P 7 \\nmeans that the update to C 22  equals ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 109}),\n",
       "  '150f0450-6492-4bd6-9dd9-32428acd7108': Document(page_content='4.2  Strassen’s  algorithm  for  matrix  multiplication  89 \\nA 11  \\ue001 B 11  C A 11  \\ue001 B 22  C A 22  \\ue001 B 11  C A 22  \\ue001 B 22  \\n\\ue003 A 11  \\ue001 B 22  C A 11  \\ue001 B 12  \\n\\ue003 A 22  \\ue001 B 11  \\ue003 A 21  \\ue001 B 11  \\n\\ue003 A 11  \\ue001 B 11  \\ue003 A 11  \\ue001 B 12  C A 21  \\ue001 B 11  C A 21  \\ue001 B 12  \\nA 22  \\ue001 B 22  C A 21  \\ue001 B 12  ; \\nwhich  corresponds  to equation  (4.8).  Altogether,  since  we  add or subtract n=2\\ue005n=2  \\nmatrices  12  times  in step  4, this  step  indeed  takes  ‚.n  2 / time. \\nWe  can  see  that  Strassen�', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 110}),\n",
       "  '569c3f08-6ec7-411a-9fe9-38e6c8bfaf7a': Document(page_content=' can  see  that  Strassen’s  remarkable  algorithm,  comprising  steps  134,  pro-  \\nduces the correct matrix product using 7 submatrix multiplications and 18  subma-  \\ntrix  additions.  We  can  also  see  that  recurrence  (4.10)  chara cterizes its running time. \\nSince  Section  4.5  shows  that  this  recurrence  has  the  solutio n T.n/  D ‚.n  lg 7 / D \\no.n  3 /, Strassen’s  method  asymptotically  beats  the  ‚.n  3 / MATRIX-MULTIPLY and \\nMATRIX-MULTIPLY-RECURSIVE procedures. \\nExercises  \\nNote: You  may  wish  to read  Section  4.5  before  attempting  some  of these exercises. \\n4.2-1  \\nUse  Strassen’s  algorithm  to compute  the  matrix  product  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 110}),\n",
       "  '0a187cd8-149f-4030-936a-a52194b1e53b': Document(page_content=' to compute  the  matrix  product  \\nÎ 1 3  \\n7 5  ÏÎ  6 8  \\n4 2  Ï \\n: \\nShow your work. \\n4.2-2  \\nWrite  pseudocode  for  Strassen’s  algorithm.  \\n4.2-3  \\nWhat is the largest k such that if you can multiply 3 \\ue005 3 matrices using k multi-  \\nplications (not assuming commutativity of multiplic ation), then you can multiply \\nn \\ue005 n matrices in o.n  lg 7 / time?  What  is the  running  time  of this  algorithm?  \\n4.2-4  \\nV . Pan discovered a way of multiplying 68  \\ue005 68  matrices  using  132,464  multi-  \\nplications, a way of multiplying 70  \\ue005 70  matrices  using  143,640  multiplications,  \\nand a way of multiplying 72  \\ue005 72  matrices  using  155,424  multiplications.  Which', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 110}),\n",
       "  '2bb2bbc7-b7e2-4352-9c88-79281d5f87c0': Document(page_content='  155,424  multiplications.  Which  \\nmethod yields the best asymptotic running time when  used in a divide-and-conquer  \\nmatrix-multiplication  algorithm?  How  does  it compare  with  Strassen’s  algorithm?  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 110}),\n",
       "  '0f7c1d96-ed3c-4d97-895b-e57d5be3d904': Document(page_content='90  Chapter  4 Divide-and-Conquer  \\n4.2-5  \\nShow how to multiply the complex numbers a C bi and c C di  using only three \\nmultiplications of real numbers. The algorithm shou ld take a, b, c , and d as input \\nand produce the real component ac  \\ue003 bd  and the imaginary component ad  C bc  \\nseparately. \\n4.2-6  \\nSuppose that you have a ‚.n  ˛ /-time  algorithm  for  squaring  n \\ue005 n matrices, where \\n˛ \\ue004 2. Show how to use that algorithm to multiply two di fferent n \\ue005 n matrices in \\n‚.n  ˛ / time. \\n4.3  The  substitution  method  for  solving  recurrences  \\nNow that you have seen how recurrences characterize  the running  times  of divide-  \\nand-conquer  algorithms,  let’s  learn  how  to solve  them.  We  start in this section \\nwith', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 111}),\n",
       "  '1b543b47-8063-42a8-bdb1-4195ce122d9b': Document(page_content='  We  start in this section \\nwith the substitution  method , which is the most general of the four methods in this \\nchapter. The substitution method comprises two step s: \\n1. Guess  the  form  of the  solution  using  symbolic  constants.  \\n2. Use mathematical induction to show that the solu tion works, and  ûnd  the  con-  \\nstants. \\nTo apply the inductive hypothesis, you substitute t he guessed solution  for  the  func-  \\ntion  on  smaller  values4hence  the  name  <substitution  method .= This method is \\npowerful, but you must guess the form of the answer . Although generating a good \\nguess  might  seem  difûcult,  a little  practice  can  quickly  improve your intuition. \\nYou can use the substitution method to establish ei ther an upper or a lower bound \\non  a recurrence.  It’s  usually  best  not  to try  to do  both  at the  same time. That is, \\nrather than trying to prove a ‚-', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 111}),\n",
       "  '540e3fd2-2fcf-4090-a4bc-7478d30892e0': Document(page_content='\\nrather than trying to prove a ‚-bound  directly,  ûrst  prove  an O-bound,  and  then  \\nprove an �-bound.  Together,  they  give  you  a ‚-bound  (Theorem  3.1  on  page  56).  \\nAs  an example  of the  substitution  method,  let’s  determine  an asymptotic upper \\nbound on the recurrence: \\nT.n/  D 2T.bn=2c/ C ‚.n/:  (4.11)  \\nThis  recurrence  is similar  to recurrence  (2.3)  on  page  41  for  merge sort, except \\nfor  the  üoor  function,  which  ensures  that  T.n/  is deûned  over  the  integers.  Let’s  \\nguess  that  the  asymptotic  upper  bound  is the  same4T.n/  D O.n  lg n/4and  use  \\nthe', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 111}),\n",
       "  '1c174064-f59f-464c-aff9-2f5ae872e6f0': Document(page_content=' n/4and  use  \\nthe substitution method to prove it. \\nWe’ll  adopt  the  inductive  hypothesis  that  T.n/  හ cn  lg n for all n \\ue004 n 0 , where \\nwe’ll  choose  the  speciûc  constants  c > 0  and n 0 > 0  later, after we see what ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 111}),\n",
       "  '02b8033d-75f0-4bc6-b4ea-54605ea52857': Document(page_content='4.3  The  substitution  method  for  solving  recurrences  91 \\nconstraints they need to obey. If we can establish this inductive hypothesis, we can \\nconclude that T.n/  D O.n  lg n/. It would be dangerous to use T.n/  D O.n  lg n/ \\nas the inductive hypothesis because the constants m atter, as we’ll  see  in a moment  \\nin our discussion of pitfalls. \\nAssume by induction that this bound holds for all n umbers at least as big as n 0 \\nand less than n. In particular, therefore, if n \\ue004 2n  0 , it holds for bn=2c, yielding \\nT.bn=2c/ හ c bn=2c lg.bn=2c/. Substituting  into  recurrence  (4.11)4hence  the  \\nname  <substitution=  method4yields  \\nT.n/  හ 2.c  bn=2c lg.bn=2', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 112}),\n",
       "  '2b701416-b255-4087-8b84-5829ef7ad56a': Document(page_content='n=2c lg.bn=2c// C ‚.n/  \\nහ 2.c.n=2/  lg.n=2//  C ‚.n/  \\nD cn  lg.n=2/  C ‚.n/  \\nD cn  lg n \\ue003 cn  lg 2 C ‚.n/  \\nD cn  lg n \\ue003 cn  C ‚.n/  \\nහ cn  lg n;  \\nwhere the last step holds if we constrain the const ants n 0 and c to be sufûciently  \\nlarge that for n \\ue004 2n  0 , the quantity cn  dominates the anonymous function hidden \\nby the ‚.n/  term. \\nWe’ve  shown  that  the  inductive  hypothesis  holds  for  the  inductive case, but we \\nalso need to prove that the inductive hypothesis ho lds for the base cases of the \\ninduction, that is, that T.n/  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 112}),\n",
       "  '92a7cb47-56cd-4324-a35e-e24e565fe539': Document(page_content=' that is, that T.n/  හ cn  lg n when n 0 හ n<2n  0 . As long as n 0 >1  (a \\nnew constraint on n 0 ), we have lg n >0 , which implies that n lg n >0. So  let’s  \\npick n 0 D 2. Since  the  base  case  of recurrence  (4.11)  is not  stated  expli citly, by our \\nconvention, T.n/  is algorithmic, which means that T.2/  and T.3/  are constant (as \\nthey  should  be if they  describe  the  worst-case  running  time  of any real program on \\ninputs of size 2 or 3). Picking c D max fT.2/;T.3/ g yields T.2/  හ c<.2  lg 2/c  \\nand T.3/  හ c<.3  lg 3/c  , establishing the inductive hypothesis for the bas e cases. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 112}),\n",
       "  '0971ed7f-b785-4577-9f11-18f703070a52': Document(page_content=' inductive hypothesis for the bas e cases. \\nThus, we have T.n/  හ cn  lg n for all n \\ue004 2, which implies that the solution to \\nrecurrence  (4.11)  is T.n/  D O.n  lg n/. \\nIn the algorithms literature, people rarely carry o ut their substitution proofs to \\nthis level of detail, especially in their treatment  of base cases. The reason is that for \\nmost  algorithmic  divide-and-conquer  recurrences,  the  base cases are all handled in \\npretty much the same way. You ground the induction on a range of values from a \\nconvenient positive constant n 0 up to some constant n 0 \\n0 >n  0 such that for n \\ue004 n 0 \\n0 , \\nthe  recurrence  always  bottoms  out  in a constant-sized  base  case between n 0 and n 0 \\n0 . \\n(This example used n 0 \\n0 D 2n  0 .) Then,  it’s  usually  apparent,  without  spelling  out ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 112}),\n",
       "  '1452ae3f-32ad-479e-9bf2-3761c5f13b02': Document(page_content='  apparent,  without  spelling  out  \\nthe details, that with a suitably large choice of t he leading constant (such as c for \\nthis example), the inductive hypothesis can be made  to hold for all the values in the \\nrange from n 0 to n 0 \\n0 . ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 112}),\n",
       "  '885dcff9-bf56-44ab-89d8-59d264e24361': Document(page_content='92  Chapter  4 Divide-and-Conquer  \\nMaking  a good  guess  \\nUnfortunately, there is no general way to correctly  guess the tightest asymptotic \\nsolution to an arbitrary recurrence. Making a good guess takes experience and, \\noccasionally, creativity. Fortunately, learning som e recurrence-solving  heuristics,  \\nas well as playing around with recurrences to gain experience, can help you become \\na good  guesser.  You  can  also  use  recursion  trees,  which  we’ll  see  in Section  4.4,  to \\nhelp generate good guesses. \\nIf a recurrence  is similar  to one  you’ve  seen  before,  then  guessing a similar \\nsolution is reasonable. As an example, consider the  recurrence \\nT.n/  D 2T.n=2  C 17/  C ‚.n/;  \\ndeûned  on  the  reals.  This  recurrence  looks  somewhat  like  the  merge-sort  recur-  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 113}),\n",
       "  '49bd960a-1509-488e-a9a4-c7af110a3073': Document(page_content='  merge-sort  recur-  \\nrence  (2.3),  but  it’s  more  complicated  because  of the  added  <17= in the argument \\nto T on  the  right-hand  side.  Intuitively,  however,  this  additional  term  shouldn’t  \\nsubstantially affect the solution to the recurrence . When n is large, the relative \\ndifference between n=2  and n=2  C 17  is not that large: both cut n nearly in half. \\nConsequently, it makes sense to guess that T.n/  D O.n  lg n/, which you can verify \\nis correct  using  the  substitution  method  (see  Exercise  4.3-1).  \\nAnother way to make a good guess is to determine lo ose upper and lower bounds \\non the recurrence and then reduce your range of unc ertainty. For example, you \\nmight start with a lower bound of T.n/  D �.n/  for  recurrence  (4.11),  since  the ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 113}),\n",
       "  '674d3126-5836-4e5c-ba35-ff7513ebd272': Document(page_content=' (4.11),  since  the  \\nrecurrence includes the term ‚.n/ , and you can prove an initial upper bound of \\nT.n/  D O.n  2 /. Then split your time between trying to lower the upper bound and \\ntrying to raise the lower bound until you converge on the correct, asymptotically \\ntight solution, which in this case is T.n/  D ‚.n  lg n/. \\nA trick  of the  trade:  subtracting  a low-order  term  \\nSometimes, you might correctly guess a tight asympt otic bound on the solution \\nof a recurrence, but somehow the math fails to work  out in the induction proof. \\nThe problem frequently turns out to be that the ind uctive assumption is not strong \\nenough. The trick to resolving this problem is to r evise your guess by subtracting  \\na lower-order  term  when  you  hit  such  a snag.  The  math  then  often goes through. \\nConsider the recurrence \\nT.n/  D 2T.n=2', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 113}),\n",
       "  '6c764325-a960-4f99-8dac-ae5e1e47b971': Document(page_content='n/  D 2T.n=2/  C ‚.1/  (4.12)  \\ndeûned  on  the  reals.  Let’s  guess  that  the  solution  is T.n/  D O.n/  and try to show \\nthat T.n/  හ cn  for n \\ue004 n 0 , where we choose the constants c;n  0 > 0  suitably. \\nSubstituting our guess into the recurrence, we obta in \\nT.n/  හ 2.c.n=2//  C ‚.1/  \\nD cn  C ‚.1/;  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 113}),\n",
       "  '1f5116a9-acec-4249-b3fa-e2bb432df4aa': Document(page_content='4.3  The  substitution  method  for  solving  recurrences  93 \\nwhich, unfortunately, does not imply that T.n/  හ cn  for any  choice of c . We might \\nbe tempted to try a larger guess, say T.n/  D O.n  2 /. Although this larger guess \\nworks, it provides only a loose upper bound. It tur ns out that our original guess of \\nT.n/  D O.n/  is correct and tight. In order to show that it is c orrect, however, we \\nmust strengthen our inductive hypothesis. \\nIntuitively, our guess is nearly right: we are off only by ‚.1/, a lower-order  \\nterm. Nevertheless, mathematical induction requires  us to prove the exact  form of \\nthe  inductive  hypothesis.  Let’s  try  our  trick  of subtracting  a lower-order  term  from  \\nour previous guess: T.n/  හ cn  \\ue003 d , where d \\ue004 0 is a constant. We now have ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 114}),\n",
       "  '1e623043-86a2-4f6c-aef9-699969dfccb4': Document(page_content='� 0 is a constant. We now have \\nT.n/  හ 2.c.n=2/  \\ue003 d/  C ‚.1/  \\nD cn  \\ue003 2d  C ‚.1/  \\nහ cn  \\ue003 d \\ue003 .d \\ue003 ‚.1//  \\nහ cn  \\ue003 d \\nas long as we choose d to be larger  than  the  anonymous  upper-bound  constant  \\nhidden by the ‚-notation.  Subtracting  a lower-order  term  works!  Of  course , we \\nmust not forget to handle the base case, which is t o choose the constant c large \\nenough that cn  \\ue003 d dominates the implicit base cases. \\nYou  might  ûnd  the  idea  of subtracting  a lower-order  term  to be counterintuitive. \\nAfter  all,  if the  math  doesn’t  work  out,  shouldn’t  you  increase  your  guess?', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 114}),\n",
       "  '7780231c-d564-4d04-a027-10b465c9c839': Document(page_content='t  you  increase  your  guess?  Not  \\nnecessarily! When the recurrence contains more than  one recursive invocation \\n(recurrence  (4.12)  contains  two),  if you  add  a lower-order  term to the guess, then \\nyou end up adding it once for each of the recursive  invocations. Doing so takes \\nyou  even  further  away  from  the  inductive  hypothesis.  On  the  other hand, if you \\nsubtract  a lower-order  term  from  the  guess,  then  you  get  to subtract it once for each \\nof the recursive invocations. In the above example,  we subtracted the constant d \\ntwice  because  the  coefûcient  of T.n=2/  is 2. We ended up with the inequality \\nT.n/  හ cn  \\ue003 d \\ue003 .d \\ue003 ‚.1// , and we readily found a suitable value for d . \\nAvoiding  pitfalls  \\nAvoid using asymptotic notation in the inductive', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 114}),\n",
       "  'e319da00-cdb7-4bcf-91c2-c12c69161cb1': Document(page_content=' using asymptotic notation in the inductive hy pothesis for the substitution \\nmethod  because  it’s  error  prone.  For  example,  for  recurrence  (4.11),  we  can  falsely  \\n<prove= that T.n/  D O.n/  if we unwisely adopt T.n/  D O.n/  as our inductive \\nhypothesis: \\nT.n/  හ 2 \\ue001 O.bn=2c/ C ‚.n/  \\nD 2 \\ue001 O.n/  C ‚.n/  \\nD O.n/:  Ń  wrong! ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 114}),\n",
       "  '2497f10f-63d1-40f0-a1ba-9c1f16caabdb': Document(page_content='94  Chapter  4 Divide-and-Conquer  \\nThe problem with this reasoning is that the constan t hidden by the O-notation  \\nchanges. We can expose the fallacy by repeating the  <proof= using an explicit \\nconstant. For the inductive hypothesis, assume that  T.n/  හ cn  for all n \\ue004 n 0 , \\nwhere c;n  0 >0  are  constants.  Repeating  the  ûrst  two  steps  in the  inequalit y chain \\nyields \\nT.n/  හ 2.c  bn=2c/ C ‚.n/  \\nහ cn  C ‚.n/:  \\nNow, indeed cnC‚.n/  D O.n/ , but the constant hidden by the O-notation  must  be \\nlarger than c because the anonymous function hidden by the ‚.n/  is asymptotically \\npositive. We cannot take the third step to conclude  that cn  C ‚.n/  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 115}),\n",
       "  '04665447-7f34-4318-8722-9aa69171b370': Document(page_content='n  C ‚.n/  හ cn, thus \\nexposing the fallacy. \\nWhen using the substitution method, or more general ly mathematical induction, \\nyou must be careful that the constants hidden by an y asymptotic notation are the \\nsame  constants  throughout  the  proof.  Consequently,  it’s  best to avoid asymptotic \\nnotation in your inductive hypothesis and to name c onstants explicitly. \\nHere’s  another  fallacious  use  of the  substitution  method  to show that the solution \\nto recurrence  (4.11)  is T.n/  D O.n/ . We guess T.n/  හ cn  and then argue \\nT.n/  හ 2.c  bn=2c/ C ‚.n/  \\nහ cn  C ‚.n/  \\nD O.n/;  Ń  wrong! \\nsince c is a positive constant. The mistake stems from the difference between our \\ngoal4to  prove  that  T', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 115}),\n",
       "  'a3d4e863-f01f-41f8-8f00-652cda805c92': Document(page_content='\\ngoal4to  prove  that  T.n/  D O.n/4and  our  inductive  hypothesis4to  prove  that  \\nT.n/  හ cn. When using the substitution method, or in any ind uctive proof, you \\nmust prove the exact  statement of the inductive hypothesis. In this case , we must \\nexplicitly prove that T.n/  හ cn  to show that T.n/  D O.n/ . \\nExercises  \\n4.3-1  \\nUse the substitution method to show that each of th e following recurrences  deûned  \\non  the  reals  has  the  asymptotic  solution  speciûed:  \\na. T.n/  D T.n  \\ue003 1/ C n has solution T.n/  D O.n  2 /. \\nb. T.n/  D T.n=2/  C ‚.1/  has solution T.n/  D O.lg n/. \\nc', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 115}),\n",
       "  '05d39f9a-3553-41d7-b5b1-03d30a016667': Document(page_content=' D O.lg n/. \\nc. T.n/  D 2T.n=2/  C n has solution T.n/  D ‚.n  lg n/. \\nd. T.n/  D 2T.n=2  C 17/  C n has solution T.n/  D O.n  lg n/. \\ne. T.n/  D 2T.n=3/  C ‚.n/  has solution T.n/  D ‚.n/ . \\nf. T.n/  D 4T.n=2/  C ‚.n/  has solution T.n/  D ‚.n  2 /. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 115}),\n",
       "  'cc131e63-5a1f-40c6-af49-0e443b832c41': Document(page_content='4.4  The  recursion-tree  method  for  solving  recurrences  95 \\n4.3-2  \\nThe solution to the recurrence T.n/  D 4T.n=2/  C n turns out to be T.n/  D ‚.n  2 /. \\nShow that a substitution proof with the assumption T.n/  හ cn  2 fails. Then show \\nhow  to subtract  a lower-order  term  to make  a substitution  proof work. \\n4.3-3  \\nThe recurrence T.n/  D 2T.n  \\ue003 1/ C 1 has the solution T.n/  D O.2  n /. Show that a \\nsubstitution proof fails with the assumption T.n/  හ c2  n , where c>0  is constant. \\nThen  show  how  to subtract  a lower-order  term  to make  a substit ution proof work. \\n4.4  The  recursion-tree  method  for  solving  recurrences  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 116}),\n",
       "  '9e9e4500-9bb1-4af0-ad54-ef310cb63e3f': Document(page_content='  for  solving  recurrences  \\nAlthough you can use the substitution method to pro ve that a solution  to a recur-  \\nrence is correct, you might have trouble coming up with a good guess. Drawing \\nout  a recursion  tree,  as we  did  in our  analysis  of the  merge-sort  recurrence  in Sec-  \\ntion  2.3.2,  can  help.  In a recursion  tree, each node represents the cost of a single \\nsubproblem somewhere in the set of recursive functi on invocations. You typically \\nsum  the  costs  within  each  level  of the  tree  to obtain  the  per-level costs, and then you \\nsum  all  the  per-level  costs  to determine  the  total  cost  of all  levels of the recursion. \\nSometimes, however, adding up the total cost takes more creativity. \\nA recursion tree is best used to generate intuition  for a good guess, which you \\ncan then verify by the substitution method. If you are meticulous when drawing out \\na recursion tree', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 116}),\n",
       "  '1306a71a-55b8-4fed-a09b-1d37b8d1916e': Document(page_content=' meticulous when drawing out \\na recursion tree and summing the costs, however, yo u can use a recursion tree as a \\ndirect proof of a solution to a recurrence. But if you use it only to generate a good \\nguess, you can often tolerate a small amount of <sl oppiness,= which can simplify \\nthe math. When you verify your guess with the subst itution method later on, your \\nmath should be precise. This section demonstrates h ow you can use recursion trees \\nto solve recurrences, generate good guesses, and ga in intuition for recurrences. \\nAn  illustrative  example  \\nLet’s  see  how  a recursion  tree  can  provide  a good  guess  for  an upper-bound  solution  \\nto the recurrence \\nT.n/  D 3T.n=4/  C ‚.n  2 /: (4.13)  \\nFigure  4.1  shows  how  to derive  the  recursion  tree  for  T.n/  D 3T.n=4/  C cn  2 ,', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 116}),\n",
       "  'e798666c-a5fe-4346-a41b-68bd64a34eec': Document(page_content='=4/  C cn  2 , \\nwhere the constant c >0  is the  upper-bound  constant  in the  ‚.n  2 / term. Part (a) \\nof the  ûgure  shows  T.n/, which  part  (b)  expands  into  an equivalent  tree  represent-  \\ning the recurrence. The cn  2 term at the root represents the cost at the top lev el \\nof recursion, and the three subtrees of the root re present the costs incurred by the ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 116}),\n",
       "  'b4a08d43-7c50-453c-bd2f-fe534d0edbbc': Document(page_content='96  Chapter  4 Divide-and-Conquer  \\n… \\n… \\n(d) (c) (b) (a) T.n/  cn  2 cn  2 \\ncn  2 T \\ue002 n \\n4 Í \\nT \\ue002 n \\n4 Í \\nT \\ue002 n \\n4 Í \\nT \\ue002 n \\n16  Í \\nT \\ue002 n \\n16  Í \\nT \\ue002 n \\n16  Í \\nT \\ue002 n \\n16  Í \\nT \\ue002 n \\n16  Í \\nT \\ue002 n \\n16  Í \\nT \\ue002 n \\n16  Í \\nT \\ue002 n \\n16  Í \\nT \\ue002 n \\n16  Í \\ncn  2 c \\ue002 n \\n4 Í 2 \\nc \\ue002 n \\n4 Í 2 \\nc \\ue002 n \\n4 Í 2 \\nc \\ue002 n \\n4 Í 2', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 117}),\n",
       "  '309e95cf-3adc-4324-a805-1ed40f2be970': Document(page_content=' \\ue002 n \\n4 Í 2 \\nc \\ue002 n \\n4 Í 2 \\nc \\ue002 n \\n4 Í 2 \\nc \\ue002 n \\n16  Í 2 \\nc \\ue002 n \\n16  Í 2 \\nc \\ue002 n \\n16  Í 2 \\nc \\ue002 n \\n16  Í 2 \\nc \\ue002 n \\n16  Í 2 \\nc \\ue002 n \\n16  Í 2 \\nc \\ue002 n \\n16  Í 2 \\nc \\ue002 n \\n16  Í 2 \\nc \\ue002 n \\n16  Í 2 3 \\n16  cn  2 \\nÎ 3 \\n16  Ï 2 \\ncn  2 log 4 n \\n3 log 4 n D n log 4 3 ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/ ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 117}),\n",
       "  '3ca91b90-b3ab-4b68-8ace-a6035e6df70a': Document(page_content='.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.n  log 4 3 / \\nTotal: O.n  2 / \\nFigure  4.1  Constructing a recursion tree for the recurrence T.n/  D 3T.n=4/  C cn  2 . Part (a)  \\nshows T.n/ , which progressively expands in (b)–(d)  to form the recursion tree. The fully expanded \\ntree in (d)  has height log 4 n. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 117}),\n",
       "  'fc666e7c-0bc1-4285-8dc8-88685acaa8da': Document(page_content='4.4  The  recursion-tree  method  for  solving  recurrences  97 \\nsubproblems of size n=4. Part (c) shows this process carried one step furt her by \\nexpanding each node with cost T.n=4/  from part (b). The cost for each of the three \\nchildren of the root is c.n=4/  2 . We continue expanding each node in the tree by \\nbreaking it into its constituent parts as determine d by the recurrence. \\nBecause subproblem sizes decrease by a factor of 4 every time we go down one \\nlevel, the recursion must eventually bottom out in a base case where n<n  0 . By \\nconvention, the base case is T.n/  D ‚.1/  for n < n  0 , where n 0 > 0  is any \\nthreshold  constant  sufûciently  large  that  the  recurrence  is well  deûned.  For  the  \\npurpose  of intuition,  however,  let’s  simplify  the  math  a little.  Let’s', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 118}),\n",
       "  'bd8f0819-1bde-45a8-9ed2-47cfecd24ee5': Document(page_content=' math  a little.  Let’s  assume  that  n \\nis an exact power of 4 and that the base case is T.1/  D ‚.1/ . As it turns out, these \\nassumptions  don’t  affect  the  asymptotic  solution.  \\nWhat’s  the  height  of the  recursion  tree?  The  subproblem  size  for a node at \\ndepth i is n=4  i . As we descend the tree from the root, the subprob lem size hits \\nn D 1 when n=4  i D 1 or, equivalently, when i D log 4 n. Thus, the tree has \\ninternal nodes at depths 0;1;2;:::;  log 4 n \\ue003 1 and leaves at depth log 4 n. \\nPart  (d)  of Figure  4.1  shows  the  cost  at each  level  of the  tree.  Each level has \\nthree times as many nodes as the level above, and s o the number of nodes at \\ndepth i is 3 i . Because sub', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 118}),\n",
       "  '2bc8781b-4b5d-46a2-b189-f1ae6a1f32a5': Document(page_content=' \\ndepth i is 3 i . Because subproblem sizes reduce by a factor of 4 for each level \\nfurther from the root, each internal node at depth i D 0;1;2;:::;  log 4 n \\ue003 1 has a \\ncost of c.n=4  i / 2 . Multiplying, we see that the total cost of all no des at a given \\ndepth i is 3 i c.n=4  i / 2 D .3=16/  i cn  2 . The bottom level, at depth log 4 n, con-  \\ntains 3 log 4 n D n log 4 3 leaves  (using  equation  (3.21)  on  page  66).  Each  leaf  con-  \\ntributes ‚.1/ , leading to a total leaf cost of ‚.n  log 4 3 /. \\nNow we add up the costs over all levels to determin e the cost for the entire tree: \\nT.n/  D cn  2 C 3 \\n16  cn  2 C Î 3 \\n16  Ï 2 \\ncn  2 C �', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 118}),\n",
       "  'a7cf6796-d4f5-4ce9-a5f7-6a3d2c1d9210': Document(page_content='� 2 \\ncn  2 C \\ue001\\ue001\\ue001 C  Î 3 \\n16  Ï log 4 n \\ncn  2 C ‚.n  log 4 3 / \\nD log 4 n X  \\ni D0 Î 3 \\n16  Ï i \\ncn  2 C ‚.n  log 4 3 / \\n< 1  X  \\ni D0 Î 3 \\n16  Ï i \\ncn  2 C ‚.n  log 4 3 / \\nD 1 \\n1 \\ue003 .3=16/  cn  2 C ‚.n  log 4 3 / (by  equation  (A.7)  on  page  1142)  \\nD 16  \\n13  cn  2 C ‚.n  log 4 3 / \\nD O.n  2 / (‚.n  log 4 3 / D O.n  0:8  / D O.n  2 /) . \\nWe’ve  derived  the  guess  of T.n/  D O.n  2 / for', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 118}),\n",
       "  '1ddf6967-bef6-4397-8bd1-8294ce4b5f43': Document(page_content='/  D O.n  2 / for  the  original  recurrence.  In this  exam-  \\nple,  the  coefûcients  of cn  2 form  a decreasing  geometric  series.  By  equation  (A.7),  \\nthe  sum  of these  coefûcients  is bounded  from  above  by  the  constant 16=13 . Since ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 118}),\n",
       "  '1c56ed21-f0de-4a50-a4a6-3099923ca1f4': Document(page_content='98  Chapter  4 Divide-and-Conquer  \\nthe  root’s  contribution  to the  total  cost  is cn  2 , the cost of the root dominates the \\ntotal cost of the tree. \\nIn fact, if O.n  2 / is indeed  an upper  bound  for  the  recurrence  (as  we’ll  verify  in \\na moment),  then  it must  be a tight  bound.  Why?  The  ûrst  recursi ve call contributes \\na cost of ‚.n  2 /, and so �.n  2 / must be a lower bound for the recurrence. \\nLet’s  now  use  the  substitution  method  to verify  that  our  guess is correct, namely, \\nthat T.n/  D O.n  2 / is an upper bound for the recurrence T.n/  D 3T.n=4/ C‚.n  2 /. \\nWe want to show that T.n/  හ dn  2 for some constant d > 0 .', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 119}),\n",
       "  'e5208a36-f61b-4ae0-9ae7-c7af33f9826a': Document(page_content='n  2 for some constant d > 0 . Using the same \\nconstant c>0  as before, we have \\nT.n/  හ 3T.n=4/  C cn  2 \\nහ 3d.n=4/  2 C cn  2 \\nD 3 \\n16  dn  2 C cn  2 \\nහ dn  2 ; \\nwhere the last step holds if we choose d \\ue004 .16=13/c  . \\nFor the base case of the induction, let n 0 > 0  be a sufûciently  large  threshold  \\nconstant  that  the  recurrence  is well  deûned  when  T.n/  D ‚.1/  for n < n  0 . We \\ncan pick d large enough that d dominates the constant hidden by the ‚, in which \\ncase dn  2 \\ue004 d \\ue004 T.n/  for 1 හ n<n  0 , completing the proof of the base case. \\nThe substitution proof we just saw involves two', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 119}),\n",
       "  '8bc26b9c-7594-4d9d-8d23-3998ea8a99c7': Document(page_content=' \\nThe substitution proof we just saw involves two nam ed constants, c and d . We \\nnamed c and  used  it to stand  for  the  upper-bound  constant  hidden  and  guaranteed to \\nexist by the ‚-notation.  We  cannot  pick  c arbitrarily4it’s  given  to us4although,  \\nfor any such c , any constant c 0 \\ue004 c also  sufûces.  We  also  named  d , but we were \\nfree  to choose  any  value  for  it that  ût our  needs.  In this  examp le, the value of d \\nhappened to depend on the value of c , which  is ûne,  since  d is constant if c is \\nconstant. \\nAn  irregular  example  \\nLet’s  ûnd  an asymptotic  upper  bound  for  another,  more  irregular,  example.  Fig-  \\nure  4.2  shows  the  recursion  tree  for  the  recurrence  \\nT', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 119}),\n",
       "  '8469f88a-3a11-4de4-94cc-589a68a5b66c': Document(page_content=' for  the  recurrence  \\nT.n/  D T.n=3/  C T.2n=3/  C ‚.n/:  (4.14)  \\nThis  recursion  tree  is unbalanced,  with  different  root-to- leaf paths having different \\nlengths.  Going  left  at any  node  produces  a subproblem  of one-third the size, and \\ngoing  right  produces  a subproblem  of two-thirds  the  size.  Let n 0 >0  be the implicit \\nthreshold constant such that T.n/  D ‚.1/  for 0<n<n  0 , and let c represent the \\nupper-bound  constant  hidden  by  the  ‚.n/  term for n \\ue004 n 0 . There are actually two \\nn 0 constants  here4one  for  the  threshold  in the  recurrence,  and  the other for the \\nthreshold in the ‚-notation,  so we’ll  let  n 0 be the larger of the two constants.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 119}),\n",
       "  '83a6fee2-5819-462b-a3ab-1a402d137c4e': Document(page_content=' n 0 be the larger of the two constants. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 119}),\n",
       "  '5d6b97eb-ded0-4a90-9dae-05d894916991': Document(page_content='4.4  The  recursion-tree  method  for  solving  recurrences  99 … \\n… cn  \\ncn  cn  cn  \\nc \\ue002 n \\n3 Í \\nc Î 2n  \\n3 Ï \\nc \\ue002 n \\n9 Í \\nc Î 2n  \\n9 Ï \\nc Î 2n  \\n9 Ï \\nc Î 4n  \\n9 Ï \\n\\ue00a \\nlog 3=2  .n=n  0 / Ú \\nC 1 \\nTotal: O.n  lg n/ ‚.1/  ‚.1/  ‚.1/  ‚.1/  \\n‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  \\n‚.1/  \\n‚.n/  \\nFigure  4.2  A recursion tree for the recurrence T.n/  D T.n=3', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 120}),\n",
       "  'f2f55d1f-665f-46d0-8a04-9341aafa6967': Document(page_content='.n/  D T.n=3/  C T.2n=3/  C cn. \\nThe height of the tree runs down the right edge of the tree, corresponding  to sub-  \\nproblems of sizes n;.2=3/n;.4=9/n;:::;‚.1/  with costs bounded by cn;c.2n=3/;  \\nc.4n=9/;:::;‚.1/ , respectively. We hit the rightmost leaf when .2=3/  h n<n  0 හ \\n.2=3/  h\\ue0021 n, which happens when h D blog 3=2  .n=n  0 /cC  1 since,  applying  the  üoor  \\nbounds  in equation  (3.2)  on  page  64  with  x D log 3=2  .n=n  0 /, we have .2=3/  h n D \\n.2=3/  bxcC1 n<.2=3/  x n D .', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 120}),\n",
       "  '5c4d4ca9-8944-4c60-abc7-68898f8be0e8': Document(page_content='.2=3/  x n D .n 0 =n/n  D n 0 and .2=3/  h\\ue0021 n D .2=3/  bxc n>.2=3/  x n \\nD .n 0 =n/n  D n 0 . Thus, the height of the tree is h D ‚.lg n/. \\nWe’re  now  in a position  to understand  the  upper  bound.  Let’s  postpone dealing \\nwith the leaves for a moment. Summing the costs of internal nodes across each \\nlevel, we have at most cn  per level times the ‚.lg n/ tree height for a total cost of \\nO.n  lg n/ for all internal nodes. \\nIt remains to deal with the leaves of the recursion  tree, which represent base \\ncases, each costing ‚.1/. How  many  leaves  are  there?  It’s  tempting  to upper-  \\nbound their number by the number of leaves in a com plete binary tree of height \\nh D blog 3=2  .n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 120}),\n",
       "  'ef6cc81a-5bfa-4075-8602-e812a6fda0aa': Document(page_content='\\nh D blog 3=2  .n=n  0 /c C  1, since  the  recursion  tree  is contained  within  such  a com-  \\nplete binary tree. But this approach turns out to g ive us a poor bound. The \\ncomplete binary tree has 1 node at the root, 2 nodes at depth 1, and  gener-  \\nally 2 k nodes at depth k. Since the height is h D b log 3=2  nc C  1, there are ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 120}),\n",
       "  '93636606-b4d0-42e4-a7dc-b6ef0b06bedc': Document(page_content='100  Chapter  4 Divide-and-Conquer  \\n2 h D 2 blog 3=2  ncC1 හ 2n  log 3=2  2 leaves in the complete binary tree, which is an \\nupper bound on the number of leaves in the recursio n tree. Because the cost of \\neach leaf is ‚.1/ , this analysis says that the total cost of all lea ves in the recursion \\ntree is O.n  log 3=2  2 / D O.n  1:71  /, which is an asymptotically greater bound than the  \\nO.n  lg n/ cost  of all  internal  nodes.  In fact,  as we’re  about  to see,  this bound is \\nnot tight. The cost of all leaves in the recursion tree is O.n/4asymptotically  less \\nthan O.n  lg n/. In other words, the cost of the internal nodes do minates the cost of \\nthe leaves, not vice versa. \\nRather than analyzing the leaves, we could quit rig ht now', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 121}),\n",
       "  '6d51708a-f865-425d-81c9-766f8573fffe': Document(page_content=' the leaves, we could quit rig ht now and prove  by  substi-  \\ntution that T.n/  D ‚.n  lg n/. This  approach  works  (see  Exercise  4.4-3),  but  it’s  \\ninstructive to understand how many leaves this recu rsion tree has. You may see \\nrecurrences for which the cost of leaves dominates the cost of internal nodes, and \\nthen  you’ll  be in better  shape  if you’ve  had  some  experience  analyzing the number \\nof leaves. \\nTo  ûgure  out  how  many  leaves  there  really  are,  let’s  write  a recurrence L.n/  for \\nthe number of leaves in the recursion tree for T.n/ . Since all the leaves in T.n/  \\nbelong either to the left subtree or the right subt ree of the root, we have \\nL.n/  D ( \\n1 if n<n  0 ; \\nL.n=3/ ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 121}),\n",
       "  '42c246ed-9bf9-401f-ac34-29fb7070814f': Document(page_content=' ; \\nL.n=3/  C L.2n=3/  if n \\ue004 n 0 : (4.15)  \\nThis  recurrence  is similar  to recurrence  (4.14),  but  it’s  missing the ‚.n/  term, and \\nit contains an explicit base case. Because this rec urrence omits the ‚.n/  term, it \\nis much  easier  to solve.  Let’s  apply  the  substitution  method  to show that it has \\nsolution L.n/  D O.n/ . Using the inductive hypothesis L.n/  හ dn  for some \\nconstant d > 0 , and assuming that the inductive hypothesis holds for all values \\nless than n, we have \\nL.n/  D L.n=3/  C L.2n=3/  \\nහ dn=3  C 2.dn/=3  \\nහ dn;  \\nwhich holds for any d >0 . We can now choose d large enough to', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 121}),\n",
       "  '0b1ec52e-35ff-4b85-aed5-b5aa4ae5a6a7': Document(page_content='0 . We can now choose d large enough to handle the base \\ncase L.n/  D 1 for 0 < n < n  0 , for which d D 1 sufûces,  thereby  completing  \\nthe substitution method for the upper bound on leav es. (Exercise  4.4-2  asks  you  to \\nprove that L.n/  D ‚.n/ .) \\nReturning  to recurrence  (4.14)  for  T.n/ , it now becomes apparent that the total \\ncost of leaves over all levels must be L.n/  \\ue001 ‚.1/  D ‚.n/ . Since we have derived \\nthe bound of O.n  lg n/ on the cost of the internal nodes, it follows that the solution \\nto recurrence  (4.14)  is T.n/  D O.n  lg n/ C ‚.n/  D O.n  lg n/. (Exercise  4.4-3  \\nasks you to prove that T.n/  D ‚.n  lg', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 121}),\n",
       "  '720018f7-3138-486a-9ad1-28c8eebf9452': Document(page_content='/  D ‚.n  lg n/.) \\nIt’s  wise  to verify  any  bound  obtained  with  a recursion  tree  by  using  the  sub-  \\nstitution  method,  especially  if you’ve  made  simplifying  assumptions. But another ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 121}),\n",
       "  'f819e012-7f36-4ced-a895-f08d34be4c33': Document(page_content='4.5  The  master  method  for  solving  recurrences  101 \\nstrategy  altogether  is to use  more-powerful  mathematics,  typically in the form of \\nthe master method in the next section (which unfort unately doesn’t  apply  to recur-  \\nrence  (4.14))  or the  Akra-Bazzi  method  (which  does,  but  requires calculus). Even \\nif you use a powerful method, a recursion tree can improve you r intuition  for  what’s  \\ngoing on beneath the heavy math. \\nExercises  \\n4.4-1  \\nFor each of the following recurrences, sketch its r ecursion tree, and guess a good \\nasymptotic upper bound on its solution. Then use th e substitution method to verify \\nyour answer. \\na. T.n/  D T.n=2/  C n 3 . \\nb. T.n/  D 4T.n=3/  C n. \\nc. T.n/  D 4T.n=', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 122}),\n",
       "  '7b8f665b-5ddd-427a-9c91-2ba570a98da6': Document(page_content='.n/  D 4T.n=2/  C n. \\nd. T.n/  D 3T.n  \\ue003 1/ C 1. \\n4.4-2  \\nUse  the  substitution  method  to prove  that  recurrence  (4.15)  has the asymptotic \\nlower bound L.n/  D �.n/ . Conclude that L.n/  D ‚.n/ . \\n4.4-3  \\nUse  the  substitution  method  to prove  that  recurrence  (4.14)  has the solution T.n/  D \\n�.n  lg n/. Conclude that T.n/  D ‚.n  lg n/. \\n4.4-4  \\nUse a recursion tree to justify a good guess for th e solution to the recurrence \\nT.n/  D T.˛n/ CT..1\\ue003˛/n/C‚.n/ , where ˛ is a constant in the range 0<˛<1 . \\n4.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 122}),\n",
       "  '8c83d507-84e2-4101-9baf-e7573d105675': Document(page_content='<˛<1 . \\n4.5  The  master  method  for  solving  recurrences  \\nThe master method provides a <cookbook= method for solving algorithmic  recur-  \\nrences of the form \\nT.n/  D aT.n=b/  C f.n/;  (4.16)  \\nwhere a>0  and b>1  are constants. We call f.n/  a driving  function , and we call \\na recurrence of this general form a master  recurrence . To use the master method, \\nyou  need  to memorize  three  cases,  but  then  you’ll  be able  to solve many master \\nrecurrences quite easily. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 122}),\n",
       "  '9d173b0f-3d66-4018-9b79-3e9cf05426e8': Document(page_content='102  Chapter  4 Divide-and-Conquer  \\nA master  recurrence  describes  the  running  time  of a divide-and-conquer  algo-  \\nrithm that divides a problem of size n into a subproblems, each of size n=b  < n. \\nThe algorithm solves the a subproblems recursively, each in T.n=b/  time. The \\ndriving function f.n/  encompasses  the  cost  of dividing  the  problem  before  the  re-  \\ncursion, as well as the cost of combining the resul ts of the recursive solutions to \\nsubproblems. For example, the recurrence arising fr om Strassen’s  algorithm  is a \\nmaster recurrence with a D 7, b D 2, and driving function f.n/  D ‚.n  2 /. \\nAs we have mentioned, in solving a recurrence that describes the running time \\nof an algorithm,  one  technicality  that  we’d  often  prefer  to ignore is the requirement \\nthat the input size n be an', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 123}),\n",
       "  '32b287e0-d979-4d85-b321-7a252f33f183': Document(page_content=' requirement \\nthat the input size n be an integer. For example, we saw that the running  time \\nof merge  sort  can  be described  by  recurrence  (2.3),  T.n/  D 2T.n=2/  C ‚.n/ , \\non  page  41.  But  if n is an odd  number,  we  really  don’t  have  two  problems  of \\nexactly half the size. Rather, to ensure that the p roblem sizes are integers, we round \\none subproblem down to size bn=2c and the other up to size dn=2e, so the true \\nrecurrence is T.n/  D T.dn=2e C  T.bn=2c/ C ‚.n/. But  this  üoors-and-ceilings  \\nrecurrence is longer to write and messier to deal w ith than recurrence  (2.3),  which  \\nis deûned  on  the  reals.  We’d  rather  not  worry  about  �', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 123}),\n",
       "  '4e35ec8f-e1c7-491e-93b3-fda94045a698': Document(page_content='  rather  not  worry  about  üoors  and  ceilings,  if we  don’t  \\nhave to, especially since the two recurrences have the same ‚.n  lg n/ solution. \\nThe master method allows you to state a master recu rrence without  üoors  and  \\nceilings and implicitly infer them. No matter how t he arguments are rounded up \\nor down to the nearest integer, the asymptotic boun ds that it provides remain the \\nsame.  Moreover,  as we’ll  see  in Section  4.6,  if you  deûne  your  master recurrence \\non  the  reals,  without  implicit  üoors  and  ceilings,  the  asymptotic  bounds  still  don’t  \\nchange.  Thus  you  can  ignore  üoors  and  ceilings  for  master  recurrences.  Section  4.7  \\ngives  sufûcient  conditions  for  ignoring  üoors  and  ceilings  in', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 123}),\n",
       "  'e1073af6-7381-49b1-8163-04a1a05bb46b': Document(page_content=' üoors  and  ceilings  in more  general  divide-  \\nand-conquer  recurrences.  \\nThe  master  theorem  \\nThe master method depends upon the following theore m. \\nTheorem  4.1  (Master  theorem)  \\nLet a > 0  and b > 1  be constants, and let f.n/  be a driving function that is \\ndeûned  and  nonnegative  on  all  sufûciently  large  reals.  Deûn e the recurrence T.n/  \\non n 2 N by \\nT.n/  D aT.n=b/  C f.n/;  (4.17)  \\nwhere aT.n=b/  actually means a 0 T.bn=bc/ C a 00 T.dn=be/ for some constants \\na 0 \\ue004 0 and a 00 \\ue004 0 satisfying a D a 0 C a 00 . Then the asymptotic behavior of T.n/  \\ncan be characterized as follows: ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 123}),\n",
       "  'e9579eeb-3c8a-4889-b9f9-f10d6ce86672': Document(page_content='4.5  The  master  method  for  solving  recurrences  103 \\n1. If there  exists  a constant  � > 0  such that f.n/  D O.n  log b a\\ue002\\ue001 /, then T.n/  D \\n‚.n  log b a /. \\n2. If there exists a constant k \\ue004 0 such that f.n/  D ‚.n  log b a lg k n/, then T.n/  D \\n‚.n  log b a lg kC1 n/. \\n3. If there  exists  a constant  �>0  such that f.n/  D �.n  log b aC\\ue001 /, and if f.n/  addi-  \\ntionally  satisûes  the  regularity  condition  af.n=b/  හ cf.n/  for some constant \\nc<1  and  all  sufûciently  large  n, then T.n/  D ‚.f.n// . \\nBefore ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 124}),\n",
       "  'f2f7ace2-bdca-4b1e-8994-6ddfc34ee553': Document(page_content='.f.n// . \\nBefore  applying  the  master  theorem  to some  examples,  let’s  spend  a few  mo-  \\nments to understand broadly what it says. The funct ion n log b a is called the water-  \\nshed  function . In each of the three cases, we compare the drivin g function f.n/  to \\nthe watershed function n log b a . Intuitively,  if the  watershed  function  grows  asymp-  \\ntotically  faster  than  the  driving  function,  then  case  1 applies. Case 2 applies if the \\ntwo  functions  grow  at nearly  the  same  asymptotic  rate.  Case  3 is the <opposite= of \\ncase  1, where  the  driving  function  grows  asymptotically  faster than the watershed \\nfunction. But the technical details matter. \\nIn case  1, not  only  must  the  watershed  function  grow  asymptot ically faster than \\nthe driving function, it must grow polynomially', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 124}),\n",
       "  'bb8a0f52-5e8b-4d4f-b132-6157932dd9bb': Document(page_content=' driving function, it must grow polynomially  faster.  That  is, the  watershed  func-  \\ntion n log b a must be asymptotically larger than the driving func tion f.n/  by at least \\na factor of ‚.n  \\ue001 / for some constant �>0 . The master theorem then says that the \\nsolution is T.n/  D ‚.n  log b a /. In this case, if we look at the recursion tree fo r the \\nrecurrence, the cost per level grows at least geome trically from root to leaves, and \\nthe total cost of leaves dominates the total cost o f the internal nodes. \\nIn case 2, the watershed and driving functions grow  at nearly the  same  asymp-  \\ntotic  rate.  But  more  speciûcally,  the  driving  function  grows  faster  than  the  wa-  \\ntershed function by a factor of ‚.lg k n/, where k \\ue004 0. The master theorem \\nsays that we tack on an extra lg n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 124}),\n",
       "  'a398dfb6-bed0-4f0c-8e14-81ac3d34c4f4': Document(page_content='ays that we tack on an extra lg n factor to f.n/ , yielding the solution T.n/  D \\n‚.n  log b a lg kC1 n/. In this case, each level of the recursion tree co sts approxi- \\nmately  the  same4‚.n  log b a lg k n/4and  there  are  ‚.lg n/ levels. In practice, the \\nmost common situation for case 2 occurs when k D 0, in which  case  the  water-  \\nshed and driving functions have the same asymptotic  growth, and the solution is \\nT.n/  D ‚.n  log b a lg n/. \\nCase  3 mirrors  case  1. Not  only  must  the  driving  function  grow  asymptotically \\nfaster than the watershed function, it must grow polynomially  faster. That is, the \\ndriving function f.n/  must be asymptotically larger than the watershed fu nction \\nn log b a by at least a factor of ‚.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 124}),\n",
       "  'd42f09e1-0e1f-4e39-a011-b2961d470175': Document(page_content=' a by at least a factor of ‚.n  \\ue001 / for some constant �>0 . Moreover, the driving \\nfunction must satisfy the regularity condition that  af.n=b/  හ cf.n/. This  condi-  \\ntion  is satisûed  by  most  of the  polynomially  bounded  functions  that  you’re  likely  \\nto encounter  when  applying  case  3. The  regularity  condition  might  not  be satisûed  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 124}),\n",
       "  '909df85e-d39f-49d9-9f99-db29b72c0d16': Document(page_content='104  Chapter  4 Divide-and-Conquer  \\nif the driving function grows slowly in local areas , yet relatively quickly overall. \\n(Exercise  4.5-5  gives  an example  of such  a function.)  For  case  3, the  master  theo-  \\nrem says that the solution is T.n/  D ‚.f.n// . If we look at the recursion tree, the \\ncost per level drops at least geometrically from th e root to the leaves, and the root \\ncost dominates the cost of all other nodes. \\nIt’s  worth  looking  again  at the  requirement  that  there  be polynomial separation \\nbetween the watershed function and the driving func tion for either  case  1 or case  3 \\nto apply.  The  separation  doesn’t  need  to be much,  but  it must  be there, and it must \\ngrow polynomially. For example, for the recurrence T.n/  D 4T.n=2/  C n 1:99  \\n(admittedly  not', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 125}),\n",
       "  '25c136d5-f866-4696-875a-880ab36ef9f5': Document(page_content=':99  \\n(admittedly  not  a recurrence  you’re  likely  to see  when  analy zing an algorithm), the \\nwatershed function is n log b a D n 2 . Hence the driving function f.n/  D n 1:99  is \\npolynomially smaller by a factor of n 0:01  . Thus  case  1 applies  with  � D 0:01. \\nUsing  the  master  method  \\nTo use the master method, you determine which case (if any) of the master theorem \\napplies and write down the answer. \\nAs  a ûrst  example,  consider  the  recurrence  T.n/  D 9T.n=3/  C n. For this \\nrecurrence, we have a D 9 and b D 3, which implies that n log b a D n log 3 9 D ‚.n  2 ). \\nSince f.n/  D n D O.n  2\\ue002\\ue001 / for any constant � හ 1, we  can  apply  case  1 of the  \\nmaster theorem', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 125}),\n",
       "  'd165c9de-eaf9-44f2-a182-b62b213068f4': Document(page_content=' case  1 of the  \\nmaster theorem to conclude that the solution is T.n/  D ‚.n  2 /. \\nNow consider the recurrence T.n/  D T.2n=3/  C 1, which has a D 1 and \\nb D 3=2, which means that the watershed function is n log b a D n log 3=2  1 D n 0 D 1. \\nCase 2 applies since f.n/  D 1 D ‚.n  log b a lg 0 n/ D ‚.1/ . The solution to the \\nrecurrence is T.n/  D ‚.lg n/. \\nFor the recurrence T.n/  D 3T.n=4/  C n lg n, we have a D 3 and b D 4, which \\nmeans that n log b a D n log 4 3 D O.n  0:793  /. Since f.n/  D n lg n D �.n  log 4 3C\\ue001 /, \\nwhere � can be as large as approximately 0:2, case  3 applies  as long', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 125}),\n",
       "  '366a9bb7-ef3b-48ad-9cd4-37027ca6f1a2': Document(page_content=':2, case  3 applies  as long  as the  regularity  \\ncondition holds for f.n/. It does,  because  for  sufûciently  large  n, we have that \\naf.n=b/  D 3.n=4/  lg.n=4/  හ .3=4/n  lg n D cf.n/  for c D 3=4. By  case  3, the  \\nsolution to the recurrence is T.n/  D ‚.n  lg n/. \\nNext,  let’s  look  at the  recurrence  T.n/  D 2T.n=2/  C n lg n, where we have \\na D 2, b D 2, and n log b a D n log 2 2 D n. Case 2 applies since f.n/  D n lg n D \\n‚.n  log b a lg 1 n/. We conclude that the solution is T.n/  D ‚.n  lg 2 n/. \\nWe can use the master method to', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 125}),\n",
       "  '30858af7-dc9d-46c5-a95b-daa335b92fc9': Document(page_content='/. \\nWe can use the master method to solve the recurrenc es we saw in Sections  2.3.2,  \\n4.1,  and  4.2.  \\nRecurrence  (2.3),  T.n/  D 2T.n=2/  C ‚.n/, on  page  41,  characterizes  the  run-  \\nning time of merge sort. Since a D 2 and b D 2, the watershed function is \\nn log b a D n log 2 2 D n. Case 2 applies because f.n/  D ‚.n/ , and the solution is \\nT.n/  D ‚.n  lg n/. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 125}),\n",
       "  '150ad71f-5abf-4bf4-92f6-5c5f3c26d573': Document(page_content='4.5  The  master  method  for  solving  recurrences  105 \\nRecurrence  (4.9),  T.n/  D 8T.n=2/  C ‚.1/, on  page  84,  describes  the  running  \\ntime of the simple recursive algorithm for matrix m ultiplication. We have a D 8 \\nand b D 2, which means that the watershed function is n log b a D n log 2 8 D n 3 . \\nSince n 3 is polynomially larger than the driving function f.n/  D ‚.1/4indeed,  \\nwe have f.n/  D O.n  3\\ue002\\ue001 / for any positive � < 34case  1 applies.  We  conclude  \\nthat T.n/  D ‚.n  3 /. \\nFinally,  recurrence  (4.10),  T.n/  D 7T.n=2/  C ‚.n  2 /, on  page  87,  arose  from  \\nthe  analysis  of Strassen’s ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 126}),\n",
       "  '4c99e8e4-9666-4697-aaf5-49c688aac3c3': Document(page_content=' analysis  of Strassen’s  algorithm  for  matrix  multiplica tion. For this recurrence, \\nwe have a D 7 and b D 2, and the watershed function is n log b a D n lg 7 . Observing  \\nthat lg 7 D 2:807355::: , we can let � D 0:8  and bound the driving function \\nf.n/  D ‚.n  2 / D O.n  lg 7\\ue002\\ue001 /. Case  1 applies  with  solution  T.n/  D ‚.n  lg 7 /. \\nWhen  the  master  method  doesn’t  apply  \\nThere  are  situations  where  you  can’t  use  the  master  theorem.  For example, it can \\nbe that the watershed function and the driving func tion cannot be asymptotically \\ncompared. We might have that f.n/  \\ue007  n log b a for  an inûnite  number  of values  \\nof n but also that f.n/  \\ue008  n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 126}),\n",
       "  '5fa1cc88-4cf1-4208-bfe3-bcc9552e2c62': Document(page_content=' f.n/  \\ue008  n log b a for  an inûnite  number  of different  values  of n. \\nAs a practical matter, however, most of the driving  functions that arise in the study \\nof algorithms can be meaningfully compared with the  watershed function. If you \\nencounter  a master  recurrence  for  which  that’s  not  the  case,  you’ll  have  to resort  to \\nsubstitution or other methods. \\nEven when the relative growths of the driving and w atershed functions can be \\ncompared, the master theorem does not cover all the  possibilities. There is a gap \\nbetween  cases  1 and  2 when  f.n/  D o.n  log b a /, yet the watershed function does \\nnot grow polynomially faster than the driving funct ion. Similarly, there is a gap \\nbetween  cases  2 and  3 when  f.n/  D !.n  log b a / and the driving function grows \\nmore than polylogarithmically faster than the water shed function,', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 126}),\n",
       "  'b5e419b9-7beb-4279-a53a-c0d15bc1c297': Document(page_content='ithmically faster than the water shed function, but it does not \\ngrow polynomially faster. If the driving function f alls into one of these gaps, or if \\nthe  regularity  condition  in case  3 fails  to hold,  you’ll  need  to use something other \\nthan the master method to solve the recurrence. \\nAs an example of a driving function falling into a gap, consider the recurrence \\nT.n/  D 2T.n=2/  C n=  lg n. Since a D 2 and b D 2, the watershed function \\nis n log b a D n log 2 2 D n 1 D n. The driving function is n=  lg n D o.n/, which \\nmeans that it grows asymptotically more slowly than  the watershed function n. \\nBut n=  lg n grows only logarithmically  slower than n, not polynomially  slower. \\nMore  precisely,  equation  (3.24)  on  page  67  says  that  lg n D o.n  \\ue001 / for', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 126}),\n",
       "  '685b5880-3cbc-4ec2-af11-6dc805725b0a': Document(page_content=' D o.n  \\ue001 / for any constant \\n�>0 , which means that 1=  lg n D !.n  \\ue002\\ue001 / and n=  lg n D !.n  1\\ue002\\ue001 / D !.n  log b a\\ue002\\ue001 /. \\nThus no constant � >0  exists such that n=  lg n D O.n  log b a\\ue002\\ue001 /, which is required \\nfor  case  1 to apply.  Case  2 fails  to apply  as well,  since  n=  lg n D ‚.n  log b a lg k n/, \\nwhere k D \\ue0031, but k must be nonnegative for case 2 to apply. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 126}),\n",
       "  'fe75a7d9-fbd6-4e13-91ad-f1a0a57912cd': Document(page_content='106  Chapter  4 Divide-and-Conquer  \\nTo solve this kind of recurrence, you must use anot her method, such  as the  sub-  \\nstitution  method  (Section  4.3)  or the  Akra-Bazzi  method  (Section  4.7).  (Exer-  \\ncise  4.6-3  asks  you  to show  that  the  answer  is ‚.n  lg lg n/.) Although the master \\ntheorem  doesn’t  handle  this  particular  recurrence,  it does  handle the overwhelming \\nmajority of recurrences that tend to arise in pract ice. \\nExercises  \\n4.5-1  \\nUse the master method to give tight asymptotic boun ds for the following  recur-  \\nrences. \\na. T.n/  D 2T.n=4/  C 1. \\nb. T.n/  D 2T.n=4/  C p n. \\nc. T.n/  D 2T.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 127}),\n",
       "  '320c757f-1537-49bb-9244-0eb1d5223bd1': Document(page_content='. T.n/  D 2T.n=4/  C p n lg 2 n. \\nd. T.n/  D 2T.n=4/  C n. \\ne. T.n/  D 2T.n=4/  C n 2 . \\n4.5-2  \\nProfessor  Caesar  wants  to develop  a matrix-multiplication  algorithm  that  is asymp-  \\ntotically  faster  than  Strassen’s  algorithm.  His  algorithm  will  use  the  divide-and-  \\nconquer method, dividing each matrix into n=4  \\ue005 n=4  submatrices, and the divide \\nand combine steps together will take ‚.n  2 / time.  Suppose  that  the  professor’s  al-  \\ngorithm creates a recursive subproblems of size n=4. What is the largest integer \\nvalue of a for which his algorithm could possibly run asymptot ically faster than \\nStrassen’s?  \\n4.5-3  \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 127}),\n",
       "  'a889da76-0865-4a32-9304-0de5c13b7d4c': Document(page_content=' \\n4.5-3  \\nUse  the  master  method  to show  that  the  solution  to the  binary- search recurrence \\nT.n/  D T.n=2/  C ‚.1/  is T.n/  D ‚.lg n/. (See  Exercise  2.3-6  for  a description  \\nof binary search.) \\n4.5-4  \\nConsider the function f.n/  D lg n. Argue that although f.n=2/  < f.n/ , the \\nregularity condition af.n=b/  හ cf.n/  with a D 1 and b D 2 does not hold for \\nany constant c <1 . Argue further that for any � >0, the  condition  in case  3 that  \\nf.n/  D �.n  log b aC\\ue001 / does not hold. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 127}),\n",
       "  'adbe85b1-0183-408c-9586-855af49a575c': Document(page_content='4.6  Proof  of the  continuous  master  theorem  107 \\n4.5-5  \\nShow that for suitable constants a, b, and � , the function f.n/  D 2 dlg ne satisûes  all  \\nthe  conditions  in case  3 of the  master  theorem  except  the  regularity condition. \\n? 4.6  Proof  of the  continuous  master  theorem  \\nProving  the  master  theorem  (Theorem  4.1)  in its  full  general ity, especially dealing \\nwith  the  knotty  technical  issue  of üoors  and  ceilings,  is beyond the scope of this \\nbook. This section, however, states and proves a va riant of the master theorem, \\ncalled the continuous  master  theorem  1 in which  the  master  recurrence  (4.17)  is \\ndeûned  over  sufûciently  large  positive  real  numbers.  The  proof of this version, \\nuncomplicated  by  üoors  and ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 128}),\n",
       "  '24c394a2-0d2f-4bc7-81ee-47e40e3cc651': Document(page_content='  by  üoors  and  ceilings,  contains  the  main  ideas needed to understand \\nhow  master  recurrences  behave.  Section  4.7  discusses  üoors  and  ceilings  in divide-  \\nand-conquer  recurrences  at greater  length,  presenting  sufûcient  conditions  for  them  \\nnot to affect the asymptotic solutions. \\nOf  course,  since  you  need  not  understand  the  proof  of the  mast er theorem in \\norder to apply the master method, you may choose to  skip this section. But if you \\nwish  to study  more-advanced  algorithms  beyond  the  scope  of this textbook, you \\nmay appreciate a better understanding of the underl ying mathematics, which the \\nproof of the continuous master theorem provides. \\nAlthough we usually assume that recurrences are alg orithmic and  don’t  require  \\nan explicit statement of a base case, we must be mu ch more careful for proofs that \\njustify the practice. The lemm', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 128}),\n",
       "  '64ca24ca-0ee5-410b-8224-7b0c95348a07': Document(page_content=' \\njustify the practice. The lemmas and theorem in thi s section explicitly state the base \\ncases, because the inductive proofs require mathema tical grounding. It is common \\nin the world of mathematics to be extraordinarily c areful proving theorems that \\njustify acting more casually in practice. \\nThe proof of the continuous master theorem involves  two lemmas.  Lemma  4.2  \\nuses  a slightly  simpliûed  master  recurrence  with  a threshol d constant of n 0 D 1, \\nrather than the more general n 0 >0  threshold constant implied by the unstated base \\ncase. The lemma employs a recursion tree to reduce the solution  of the  simpliûed  \\nmaster  recurrence  to that  of evaluating  a summation.  Lemma  4.3  then  provides  \\nasymptotic bounds for the summation, mirroring the three cases  of the  master  the-  \\norem. Finally, the continuous master theorem itself  (Theorem  4.4)  gives  asymp-  \\nt', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 128}),\n",
       "  'bd24da1c-6840-437b-aa77-107123aaa307': Document(page_content=' gives  asymp-  \\ntotic bounds for master recurrences, while generali zing to an arbitrary threshold \\nconstant n 0 >0  as implied by the unstated base case. \\n1 This terminology does not mean that either T.n/  or f.n/  need be continuous, only that the domain \\nof T.n/  is the real numbers, as opposed to integers. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 128}),\n",
       "  '73126f9f-bc23-441f-84fa-e0ea37a41737': Document(page_content='108  Chapter  4 Divide-and-Conquer  \\nSome  of the  proofs  use  the  properties  described  in Problem  3-5  on  pages  72373  \\nto combine and simplify complicated asymptotic expr essions. Although  Prob-  \\nlem  3-5  addresses  only  ‚-notation,  the  properties  enumerated  there  can  be ex-  \\ntended to O-notation  and  �-notation  as well.  \\nHere’s  the  ûrst  lemma.  \\nLemma  4.2  \\nLet a > 0  and b > 1  be constants, and let f.n/  be a function  deûned  over  real  \\nnumbers n \\ue004 1. Then the recurrence \\nT.n/  D ( \\n‚.1/  if 0 හ n<1;  \\naT.n=b/  C f.n/  if n \\ue004 1 \\nhas solution \\nT.n/  D ‚', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 129}),\n",
       "  '9aab8afa-5161-4b24-aecd-e12074fc5028': Document(page_content=' \\nT.n/  D ‚.n  log b a / C blog b nc X  \\nj D0 a j f.n=b  j /: (4.18)  \\nProof  Consider  the  recursion  tree  in Figure  4.3.  Let’s  look  ûrst  at its  inter-  \\nnal nodes. The root of the tree has cost f.n/ , and it has a children, each with \\ncost f.n=b/ . (It is convenient to think of a as being  an integer,  especially  when  vi-  \\nsualizing the recursion tree, but the mathematics d oes not require it.) Each of these \\nchildren has a children, making a 2 nodes at depth 2, and each of the a children \\nhas cost f.n=b  2 /. In general, there are a j nodes at depth j , and each node has \\ncost f.n=b  j /. \\nNow,  let’s  move  on  to understanding  the  leaves.  The  tree  grows ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 129}),\n",
       "  'f502b3de-76d4-485a-b457-be24ec410e41': Document(page_content='  leaves.  The  tree  grows  downward  un-  \\ntil n=b  j becomes less than 1. Thus, the tree has height blog b nc C  1, because \\nn=b  blog b nc \\ue004 n=b  log b n D 1 and n=b  blog b ncC1 < n=b  log b n D 1. Since, as we \\nhave observed, the number of nodes at depth j is a j and all the leaves are at \\ndepth blog b nc C  1, the tree contains a blog b ncC1 leaves.  Using  the  identity  (3.21)  \\non  page  66,  we  have  a blog b ncC1 හ a log b nC1 D an  log b a D O.n  log b a /, since a is \\nconstant, and a blog b ncC1 \\ue004 a log b n D n log b a D �.n  log b a /. Consequently, the total \\nnumber of leaves is ‚.n  log b a /4asymptot', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 129}),\n",
       "  'a9b8f744-bb78-4726-8f67-40c0ec68854b': Document(page_content='  log b a /4asymptotically,  the  watershed  function.  \\nWe  are  now  in a position  to derive  equation  (4.18)  by  summing  the costs of \\nthe  nodes  at each  depth  in the  tree,  as shown  in the  ûgure.  The  ûrst term in the \\nequation is the total costs of the leaves. Since ea ch leaf is at depth blog b nc C  1 \\nand n=b  blog b ncC1 < 1 , the base case of the recurrence gives the cost of  a \\nleaf: T.n=b  blog b ncC1 / D ‚.1/ . Hence the cost of all ‚.n  log b a / leaves is \\n‚.n  log b a / \\ue001 ‚.1/  D ‚.n  log b a / by  Problem  3-5(d).  The  second  term  in equa-  \\ntion  (4.18)  is the  cost  of the  internal', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 129}),\n",
       "  '34dbe1d8-2b8e-45fb-819b-c6ba38821df6': Document(page_content='  is the  cost  of the  internal  nodes,  which,  in the  underlying  divide-and-  \\nconquer algorithm, represents the costs of dividing  problems into subproblems and ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 129}),\n",
       "  '65f71b58-35f1-47c3-9dd0-76b9e38143ec': Document(page_content='4.6  Proof  of the  continuous  master  theorem  109 \\n… … \\n… \\n… … … … \\n… … … … \\n… … … \\n… f.n/  f.n/  \\na a a a \\na a a a \\na a a a a \\nf.n=b/  f.n=b/  f.n=b/  \\nf.n=b  2 / f.n=b  2 / f.n=b  2 / f.n=b  2 / f.n=b  2 / f.n=b  2 / f.n=b  2 / f.n=b  2 / f.n=b  2 / af.n=b/  \\na 2 f.n=b  2 / blog b nc C  1 \\na blog b ncC1 ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 130}),\n",
       "  '80335134-f98d-48c1-9947-a801136ffe02': Document(page_content=' ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.1/  ‚.n  log b a / \\nTotal: ‚.n  log b a / C blog b nc X  \\nj D0 a j f.n=b  j / \\nFigure  4.3  The recursion tree generated by T.n/  D aT.n=b/  C f.n/ . The tree is a complete a-ary  \\ntree with a blog b ncC1 leaves and height blog b nc C  1. The cost of the nodes at each depth is shown \\nat the  right,  and  their  sum  is given  in equation  (4.18).  \\nthen recombining the subproblems. Since the cost fo r all the internal nodes at \\ndepth j is a j f.n=b  j /, the total cost of all internal nodes is \\nblog b nc X  \\nj D0 a j f.n=b  j /: \\nAs  we’', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 130}),\n",
       "  '6832f01e-bffb-41ca-9178-6d0e2d346a36': Document(page_content=' j /: \\nAs  we’ll  see,  the  three  cases  of the  master  theorem  depend  on  the distribution of \\nthe total cost across levels of the recursion tree:  \\nCase  1: The costs increase geometrically from the root to t he leaves, growing by \\na constant factor with each level. \\nCase  2: The costs depend on the value of k in the theorem. With k D 0, the costs \\nare equal for each level; with k D 1, the costs grow linearly from the root to \\nthe leaves; with k D 2, the growth is quadratic; and in general, the cost s grow \\npolynomially in k. \\nCase  3: The costs decrease geometrically from the root to t he leaves, shrinking \\nby a constant factor with each level. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 130}),\n",
       "  '33c7109d-beef-43fa-9496-e4681806047c': Document(page_content='110  Chapter  4 Divide-and-Conquer  \\nThe  summation  in equation  (4.18)  describes  the  cost  of the  dividing  and  com-  \\nbining  steps  in the  underlying  divide-and-conquer  algorithm.  The  next  lemma  pro-  \\nvides  asymptotic  bounds  on  the  summation’s  growth.  \\nLemma  4.3  \\nLet a > 0  and b > 1  be constants, and let f.n/  be a function  deûned  over  real  \\nnumbers n \\ue004 1. Then the asymptotic behavior of the function \\ng.n/  D blog b nc X  \\nj D0 a j f.n=b  j /; (4.19)  \\ndeûned  for  n \\ue004 1, can be characterized as follows: \\n1. If there  exists  a constant  � > 0  such that f.n/  D O.n  log b a\\ue002\\ue001', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 131}),\n",
       "  '2a3bcf26-dfed-4c14-b42f-b70b7a5d8db2': Document(page_content='.n  log b a\\ue002\\ue001 /, then g.n/  D \\nO.n  log b a /. \\n2. If there exists a constant k \\ue004 0 such that f.n/  D ‚.n  log b a lg k n/, then g.n/  D \\n‚.n  log b a lg kC1 n/. \\n3. If there  exists  a constant  c in the range 0<c <1  such that 0 < af.n=b/  හ \\ncf.n/  for all n \\ue004 1, then g.n/  D ‚.f.n// . \\nProof  For  case  1, we  have  f.n/  D O.n  log b a\\ue002\\ue001 /, which implies that f.n=b  j / D \\nO..n=b  j / log b a\\ue002\\ue001 /. Substituting  into  equation  (4.19)  yields  \\ng.n/  D blog b nc X  \\nj D0 a', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 131}),\n",
       "  '0ab538c8-aacb-4435-ad51-975f5d153ac4': Document(page_content=' nc X  \\nj D0 a j O Î \\ue002 n \\nb j Í log b a\\ue002\\ue001 Ï \\nD O \\ue001 blog b nc X  \\nj D0 a j \\ue002 n \\nb j Í log b a\\ue002\\ue001 ! \\n(by  Problem  3-5(c),  repeatedly)  \\nD O \\ue001 \\nn log b a\\ue002\\ue001 blog b nc X  \\nj D0 Î ab  \\ue001 \\nb log b a Ï j ! \\nD O \\ue001 \\nn log b a\\ue002\\ue001 blog b nc X  \\nj D0 .b \\ue001 / j ! \\n(by  equation  (3.17)  on  page  66)  \\nD O Î \\nn log b a\\ue002\\ue001 Î b \\ue001.blog b ncC1/  \\ue003 1 \\nb \\ue001 \\ue003 1 ÏÏ  \\n(by  equation  (A.6)  on  page ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 131}),\n",
       "  '8b713fc5-1f98-4b74-8d9d-a91f180b8a53': Document(page_content=' (A.6)  on  page  1142)  , \\nthe last series being geometric. Since b and � are constants, the b \\ue001 \\ue003 1 denom-  \\ninator  doesn’t  affect  the  asymptotic  growth  of g.n/ , and neither does the \\ue0031 in ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 131}),\n",
       "  '030f2bc0-dc0e-4b31-a3d3-ff8348eb7409': Document(page_content='4.6  Proof  of the  continuous  master  theorem  111 \\nthe numerator. Since b \\ue001.blog b ncC1/  හ .b log b nC1 / \\ue001 D b \\ue001 n \\ue001 D O.n  \\ue001 /, we obtain \\ng.n/  D O.n  log b a\\ue002\\ue001 \\ue001 O.n  \\ue001 // D O.n  log b a /, thereby  proving  case  1. \\nCase 2 assumes that f.n/  D ‚.n  log b a lg k n/, from which we can conclude that \\nf.n=b  j / D ‚..n=b  j / log b a lg k .n=b  j //. Substituting  into  equation  (4.19)  and  re-  \\npeatedly  applying  Problem  3-5(c)  yields  \\ng.n/  D ‚ \\ue001 blog b nc X  \\nj D0 a j \\ue002 n \\nb', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 132}),\n",
       "  '43b9bc3d-1484-46c7-ac22-308aabbee3c4': Document(page_content='0 a j \\ue002 n \\nb j Í log b a \\nlg k \\ue002 n \\nb j Í ! \\nD ‚ \\ue001 \\nn log b a blog b nc X  \\nj D0 a j \\nb j log b a lg k \\ue002 n \\nb j Í ! \\nD ‚ \\ue001 \\nn log b a blog b nc X  \\nj D0 lg k \\ue002 n \\nb j Í ! \\nD ‚ \\ue001 \\nn log b a blog b nc X  \\nj D0 Î log b .n=b  j / \\nlog b 2 Ï k ! \\n(by  equation  (3.19)  on  page  66)  \\nD ‚ \\ue001 \\nn log b a blog b nc X  \\nj D0 Î log b n \\ue003 j \\nlog b 2 Ï k ! \\n(by  equations  (3.17),  (3.18),  \\nand  (3.20)) ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 132}),\n",
       "  '4012cc94-5354-4d60-92ca-c2f037c8c4e5': Document(page_content=' \\nand  (3.20))  \\nD ‚ \\ue001 \\nn log b a \\nlog k \\nb 2 blog b nc X  \\nj D0 .log b n \\ue003 j/  k ! \\nD ‚ \\ue001 \\nn log b a blog b nc X  \\nj D0 .log b n \\ue003 j/  k ! \\n(b>1  and k are constants) . \\nThe summation within the ‚-notation  can  be bounded  from  above  as follows:  \\nblog b nc X  \\nj D0 .log b n \\ue003 j/  k හ blog b nc X  \\nj D0 .blog b nc C  1 \\ue003 j/  k \\nD blog b ncC1 X  \\nj D1 j k (reindexing4pages  114331144)  \\nD O..blog b nc C  1/ kC1 / (by  Exercise  A.1-5  on  page  1144)  \\nD O.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 132}),\n",
       "  'd473760f-3226-4558-81be-454542426145': Document(page_content='  1144)  \\nD O.log kC1 \\nb n/ (by  Exercise  3.3-3  on  page  70)  . \\nExercise  4.6-1  asks  you  to show  that  the  summation  can  simila rly be bounded from \\nbelow by �.log kC1 \\nb n/. Since  we  have  tight  upper  and  lower  bounds,  the  summa-  \\ntion is ‚.log kC1 \\nb n/, from which we can conclude that g.n/  D ‚ ã \\nn log b a log kC1 \\nb n ä \\n, \\nthereby completing the proof of case 2. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 132}),\n",
       "  '36ee66a0-7f56-42b5-ae47-3bcdceb761c6': Document(page_content='112  Chapter  4 Divide-and-Conquer  \\nFor  case  3, observe  that  f.n/  appears  in the  deûnition  (4.19)  of g.n/  (when \\nj D 0) and that all terms of g.n/  are positive. Therefore, we must have g.n/  D \\n�.f  .n// , and it only remains to prove that g.n/  D O.f.n// . Performing j itera-  \\ntions of the inequality af.n=b/  හ cf.n/  yields a j f.n=b  j / හ c j f.n/. Substitut-  \\ning  into  equation  (4.19),  we  obtain  \\ng.n/  D blog b nc X  \\nj D0 a j f.n=b  j / \\nහ blog b nc X  \\nj D0 c j f.n/  \\nහ f.n/  1  X  \\nj D', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 133}),\n",
       "  '8b472731-5540-4bee-8b51-10412a094a10': Document(page_content='/  1  X  \\nj D0 c j \\nD f.n/  Î 1 \\n1 \\ue003 c Ï \\n(by  equation  (A.7)  on  page  1142  since  jc j <1) \\nD O.f.n//:  \\nThus, we can conclude that g.n/  D ‚.f.n//. With  case  3 proved,  the  entire  proof  \\nof the lemma is complete. \\nWe can now state and prove the continuous master th eorem. \\nTheorem  4.4  (Continuous  master  theorem)  \\nLet a>0  and b>1  be constants, and let f.n/  be a driving  function  that  is deûned  \\nand  nonnegative  on  all  sufûciently  large  reals.  Deûne  the  algorithmic recurrence \\nT.n/  on the positive real numbers by \\nT.n/  D aT.n=b/  C f.n/:  \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 133}),\n",
       "  '2ba7b23c-78a1-4827-80ee-8d55f161b72e': Document(page_content='  C f.n/:  \\nThen the asymptotic behavior of T.n/  can be characterized as follows: \\n1. If there  exists  a constant  � > 0  such that f.n/  D O.n  log b a\\ue002\\ue001 /, then T.n/  D \\n‚.n  log b a /. \\n2. If there exists a constant k \\ue004 0 such that f.n/  D ‚.n  log b a lg k n/, then T.n/  D \\n‚.n  log b a lg kC1 n/. \\n3. If there  exists  a constant  �>0  such that f.n/  D �.n  log b aC\\ue001 /, and if f.n/  ad-  \\nditionally  satisûes  the  regularity  condition  af.n=b/  හ cf.n/  for some constant \\nc<1  and  all  sufûciently  large  n, then T.n/  D ‚.', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 133}),\n",
       "  'b935c7cf-8ca9-433a-a971-551f634cfbe7': Document(page_content=' then T.n/  D ‚.f.n// . \\nProof  The  idea  is to bound  the  summation  (4.18)  from  Lemma  4.2  by  applying \\nLemma  4.3.  But  we  must  account  for  Lemma  4.2  using  a base  case  for 0<n<1 , ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 133}),\n",
       "  '2be61a63-0f8c-41f3-bd5d-d33f7ea713b3': Document(page_content='4.6  Proof  of the  continuous  master  theorem  113 \\nwhereas this theorem uses an implicit base case for  0<n<n  0 , where n 0 >0  is \\nan arbitrary threshold constant. Since the recurren ce is algorithmic, we can assume \\nthat f.n/  is deûned  for  n \\ue004 n 0 . \\nFor n>0, let  us deûne  two  auxiliary  functions  T 0 .n/  D T.n  0 n/ and f 0 .n/  D \\nf.n  0 n/. We have \\nT 0 .n/  D T.n  0 n/ \\nD ( \\n‚.1/  if n 0 n<n  0 ; \\naT.n  0 n=b/  C f.n  0 n/ if n 0 n \\ue004 n 0 \\nD ( \\n‚.1/  if n<1;  \\naT  0 .n=b/  C f 0 .n/  if n \\ue004 1:  (4.20)', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 134}),\n",
       "  '825936ea-6415-4565-805c-f5b412a137ca': Document(page_content='\\ue004 1:  (4.20)  \\nWe have obtained a recurrence for T 0 .n/  that  satisûes  the  conditions  of Lemma  4.2,  \\nand by that lemma, the solution is \\nT 0 .n/  D ‚.n  log b a / C blog b nc X  \\nj D0 a j f 0 .n=b  j /: (4.21)  \\nTo solve T 0 .n/, we  ûrst  need  to bound  f 0 .n/. Let’s  examine  the  individual  cases  \\nin the theorem. \\nThe  condition  for  case  1 is f.n/  D O.n  log b a\\ue002\\ue001 / for some constant � >0 . We \\nhave \\nf 0 .n/  D f.n  0 n/ \\nD O..n  0 n/ log b a\\ue002\\ue001 / \\nD O.n  log b a\\ue002\\ue001 /; \\nsince a, b, n 0 , and � are all constant', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 134}),\n",
       "  'cf9af004-d8c4-4737-8c51-e6f42dc6f70e': Document(page_content=' b, n 0 , and � are all constant. The function f 0 .n/  satisûes  the  conditions  of \\ncase  1 of Lemma  4.3,  and  the  summation  in equation  (4.18)  of Lemma  4.2  evaluates  \\nto O.n  log b a /. Because a, b and n 0 are all constants, we have \\nT.n/  D T 0 .n=n  0 / \\nD ‚..n=n  0 / log b a / C O..n=n  0 / log b a / \\nD ‚.n  log b a / C O.n  log b a / \\nD ‚.n  log b a / (by  Problem  3-5(b))  , \\nthereby  completing  case  1 of the  theorem.  \\nThe condition for case 2 is f.n/  D ‚.n  log b a lg k n/ for some constant k \\ue004 0. \\nWe have \\nf 0 .n/  D f.n  0', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 134}),\n",
       "  '7dd50daa-0677-49ae-b62a-9fe99bd6e1f7': Document(page_content=' .n/  D f.n  0 n/ \\nD ‚..n  0 n/ log b a lg k .n 0 n//  \\nD ‚.n  log b a lg k n/ (by eliminating the constant terms) . ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 134}),\n",
       "  'a7090997-f123-4f51-a624-e84235650a9a': Document(page_content='114  Chapter  4 Divide-and-Conquer  \\nSimilar  to the  proof  of case  1, the  function  f 0 .n/  satisûes  the  conditions  of case  2 \\nof Lemma  4.3.  The  summation  in equation  (4.18)  of Lemma  4.2  is therefore \\n‚.n  log b a lg kC1 n/, which implies that \\nT.n/  D T 0 .n=n  0 / \\nD ‚..n=n  0 / log b a / C ‚..n=n  0 / log b a lg kC1 .n=n  0 // \\nD ‚.n  log b a / C ‚.n  log b a lg kC1 n/ \\nD ‚.n  log b a lg kC1 n/ (by  Problem  3-5(c))  , \\nwhich proves case 2 of the theorem. \\nFinally,  the  condition  for  case  3 is f.n/  D �.n ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 135}),\n",
       "  '617a6a28-5794-40ca-a4fb-16fee0c96edf': Document(page_content=' f.n/  D �.n  log b aC\\ue001 / for some constant �>0  \\nand f.n/  additionally  satisûes  the  regularity  condition  af.n=b/  හ cf.n/  for all \\nn \\ue004 n 0 and some constants c < 1  and n 0 > 1. The  ûrst  part  of case  3 is like  \\ncase  1: \\nf 0 .n/  D f.n  0 n/ \\nD �..n  0 n/ log b aC\\ue001 / \\nD �.n  log b aC\\ue001 /: \\nUsing  the  deûnition  of f 0 .n/  and the fact that n 0 n \\ue004 n 0 for all n \\ue004 1, we have for \\nn \\ue004 1 that \\naf  0 .n=b/  D af.n  0 n=b/  \\nහ cf.n  0 n/ \\nD cf  0 .n/:  \\nThus f 0 .n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 135}),\n",
       "  '46d5b12e-40aa-4d19-8d25-b1d302ee109a': Document(page_content='/:  \\nThus f 0 .n/  satisûes  the  requirements  for  case  3 of Lemma  4.3,  and  the  summation \\nin equation  (4.18)  of Lemma  4.2  evaluates  to ‚.f  0 .n//, yielding \\nT.n/  D T 0 .n=n  0 / \\nD ‚..n=n  0 / log b a / C ‚.f  0 .n=n  0 // \\nD ‚.f  0 .n=n  0 // \\nD ‚.f.n//;  \\nwhich  completes  the  proof  of case  3 of the  theorem  and  thus  the  whole theorem. \\nExercises  \\n4.6-1  \\nShow that P  blog b nc \\nj D0 .log b n \\ue003 j/  k D �.log kC1 \\nb n/. \\n? 4.6-2  \\nShow  that  case  3 of the  master  theorem  is over', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 135}),\n",
       "  '422c99ee-cf0c-48fc-8b10-a3968a2f9a2c': Document(page_content=' 3 of the  master  theorem  is overstated  (which  is also  why  case  3 \\nof Lemma  4.3  does  not  require  that  f.n/  D �.n  log b aC\\ue001 /) in the sense that the ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 135}),\n",
       "  '0d614a28-3f47-47ca-9c36-1c85eafec937': Document(page_content='4.7  Akra-Bazzi  recurrences  115 \\nregularity condition af.n=b/  හ cf.n/  for some constant c <1  implies that there \\nexists a constant �>0  such that f.n/  D �.n  log b aC\\ue001 /. \\n? 4.6-3  \\nFor f.n/  D ‚.n  log b a = lg n/, prove  that  the  summation  in equation  (4.19)  has  solu-  \\ntion g.n/  D ‚.n  log b a lg lg n/. Conclude that a master recurrence T.n/  using f.n/  \\nas its driving function has solution T.n/  D ‚.n  log b a lg lg n/. \\n? 4.7  Akra-Bazzi  recurrences  \\nThis section provides an overview of two advanced t opics related  to divide-and-  \\nconquer  recurrences.  The  ûrst', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 136}),\n",
       "  '8710cae0-bc73-48e9-9bfd-f9d62e2248bc': Document(page_content='urrences.  The  ûrst  deals  with  technicalities  arising from the use of \\nüoors  and  ceilings,  and  the  second  discusses  the  Akra-Bazzi  method,  which  in-  \\nvolves  a little  calculus,  for  solving  complicated  divide-and-conquer  recurrences.  \\nIn particular,  we’ll  look  at the  class  of algorithmic  divide-and-conquer  recur-  \\nrences  originally  studied  by  M.  Akra  and  L. Bazzi  [13].  These  Akra-Bazzi  recur-  \\nrences take the form \\nT.n/  D f.n/  C k X  \\ni D1 a i T.n=b  i /; (4.22)  \\nwhere k is a positive integer; all the constants a 1 ;a  2 ;:::;a  k 2 R are  strictly  posi-  \\ntive; all the constants b 1 ;b  2', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 136}),\n",
       "  'b2152422-5487-476d-a3e2-0a6e0c2681b2': Document(page_content='; all the constants b 1 ;b  2 ;:::;b  k 2 R are strictly greater than 1; and the driving \\nfunction f.n/  is deûned  on  sufûciently  large  nonnegative  reals  and  is itself  non-  \\nnegative. \\nAkra-Bazzi  recurrences  generalize  the  class  of recurrence s addressed by the \\nmaster theorem. Whereas master recurrences characte rize the running times of \\ndivide-and-conquer  algorithms  that  break  a problem  into  equal-sized  subproblems  \\n(modulo  üoors  and  ceilings),  Akra-Bazzi  recurrences  can  describe the running time \\nof divide-and-conquer  algorithms  that  break  a problem  into  different-sized  sub-  \\nproblems. The master theorem, however, allows you t o ignore üoors  and  ceilings,  \\nbut  the  Akra-Bazzi  method  for  solving  Akra-Bazzi ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 136}),\n",
       "  'df8d7fe0-2169-4d3d-9623-cf1a1e24699b': Document(page_content='  solving  Akra-Bazzi  recurrenc es needs an additional \\nrequirement  to deal  with  üoors  and  ceilings.  \\nBut  before  diving  into  the  Akra-Bazzi  method  itself,  let’s  understand  the  lim-  \\nitations  involved  in ignoring  üoors  and  ceilings  in Akra-Ba zzi recurrences. As \\nyou’re  aware,  algorithms  generally  deal  with  integer-sized  inputs.  The  mathemat-  \\nics for recurrences is often easier with real numbe rs, however, than with integers, \\nwhere  we  must  cope  with  üoors  and  ceilings  to ensure  that  terms  are  well  deûned.  \\nThe  difference  may  not  seem  to be much4especially  because  that’s  often  the  truth  \\nwith  recurrences4but  to be mathematically  correct,  we  must  be careful with our ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 136}),\n",
       "  'b48c9080-7846-4603-a486-a92b0dcd1e6d': Document(page_content='116  Chapter  4 Divide-and-Conquer  \\nassumptions. Since our end goal is to understand al gorithms and not the vagaries \\nof mathematical  corner  cases,  we’d  like  to be casual  yet  rigorous. How can we \\ntreat  üoors  and  ceilings  casually  while  still  ensuring  rigor?  \\nFrom  a mathematical  point  of view,  the  difûculty  in dealing  with  üoors  and  \\nceilings is that some driving functions can be real ly, really weird.  So  it’s  not  okay  in \\ngeneral  to ignore  üoors  and  ceilings  in Akra-Bazzi  recurren ces. Fortunately, most \\nof the driving functions we encounter in the study of algorithms behave nicely, and \\nüoors  and  ceilings  don’t  make  a difference.  \\nThe  polynomial-growth  condition  \\nIf the driving function f.n/  in equation  (4.22)  is well', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 137}),\n",
       "  'd22f409f-7322-42a4-83d4-9b53c0538650': Document(page_content=' equation  (4.22)  is well  behaved  in the  following  \\nsense,  it’s  okay  to drop  üoors  and  ceilings.  \\nA function f.n/  deûned  on  all  sufûciently  large  positive  reals  satisûes  the  \\npolynomial-growth  condition  if there exists a constant y n>0  such that the \\nfollowing holds: for every constant � \\ue004 1, there exists a constant d > 1  \\n(depending on �) such that f.n/=d  හ f.\\ue001n/  හ df.n/  for all 1 හ \\ue001 හ � \\nand n \\ue004 y  n. \\nThis  deûnition  may  be one  of the  hardest  in this  textbook  to get your head around. \\nTo  a ûrst  order,  it says  that  f.n/  satisûes  the  property  that  f.‚', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 137}),\n",
       "  '52ab1dda-e094-44de-b56b-23d009e6a83f': Document(page_content=' the  property  that  f.‚.n//  D ‚.f.n// , \\nalthough  the  polynomial-growth  condition  is actually  somewhat  stronger  (see  Ex-  \\nercise  4.7-4).  The  deûnition  also  implies  that  f.n/  is asymptotically positive (see \\nExercise  4.7-3).  \\nExamples  of functions  that  satisfy  the  polynomial-growth  condition include any \\nfunction of the form f.n/  D ‚.n  ˛ lg ˇ n lg lg \\ue002 n/, where ˛, ˇ, and � are constants. \\nMost of the polynomially bounded functions used in this book satisfy the condition. \\nExponentials  and  superexponentials  do  not  (see  Exercise  4.7-2,  for  example),  and  \\nthere also exist polynomially bounded functions tha t do not. \\nFloors  and  ceilings  in <nice', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 137}),\n",
       "  '2220529f-c35b-4466-b889-736bdbfabb43': Document(page_content='Floors  and  ceilings  in <nice=  recurrences  \\nWhen  the  driving  function  in an Akra-Bazzi  recurrence  satisûes  the  polynomial-  \\ngrowth  condition,  üoors  and  ceilings  don’t  change  the  asymp totic behavior of the \\nsolution. The following theorem, which is presented  without proof, formalizes this \\nnotion. \\nTheorem  4.5  \\nLet T.n/  be a function  deûned  on  the  nonnegative  reals  that  satisûes  recur-  \\nrence  (4.22),  where  f.n/  satisûes  the  polynomial-growth  condition.  Let  T 0 .n/  be \\nanother  function  deûned  on  the  natural  numbers  also  satisfying  recurrence  (4.22),  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 137}),\n",
       "  'd0992c2b-cd84-4d88-8172-dfc61e353fd3': Document(page_content='4.7  Akra-Bazzi  recurrences  117 \\nexcept that each T.n=b  i / is replaced either with T.dn=b  i e/ or with T.bn=b  i c/. \\nThen we have T 0 .n/  D ‚.T.n// . \\nFloors and ceilings represent a minor perturbation to the arguments  in the  re-  \\ncursion.  By  inequality  (3.2)  on  page  64,  they  perturb  an argument by at most 1. \\nBut much larger perturbations are tolerable. As lon g as the driving function f.n/  \\nin recurrence  (4.22)  satisûes  the  polynomial-growth  condition,  it turns  out  that  re-  \\nplacing any term T.n=b  i / with T.n=b  i C h i .n//, where jh i .n/j D  O.n=  lg 1C\\ue001 n/ \\nfor some constant � > 0  and ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 138}),\n",
       "  '52632d83-5c29-48b6-afbb-8e8e81f80d2b': Document(page_content='\\nfor some constant � > 0  and  sufûciently  large  n, leaves the asymptotic solution \\nunaffected.  Thus,  the  divide  step  in a divide-and-conquer  algorithm  can  be moder-  \\nately  coarse  without  affecting  the  solution  to its  running- time recurrence. \\nThe  Akra-Bazzi  method  \\nThe  Akra-Bazzi  method,  not  surprisingly,  was  developed  to solve  Akra-Bazzi  re-  \\ncurrences  (4.22),  which  by  dint  of Theorem  4.5,  applies  in the  presence  of üoors  \\nand ceilings or even larger perturbations, as just discussed. The method involves \\nûrst  determining  the  unique  real  number  p such that P  k \\ni D1 a i =b  p \\ni D 1. Such a p \\nalways exists, because when p ! \\ue0031 , the sum goes to 1; it decreases as p in', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 138}),\n",
       "  '723089b4-1b97-45a1-b125-0b83dd805689': Document(page_content=' sum goes to 1; it decreases as p in-  \\ncreases; and when p ! 1 , it goes to 0. The  Akra-Bazzi  method  then  gives  the  \\nsolution to the recurrence as \\nT.n/  D ‚ Î \\nn p Î \\n1 C Z n \\n1 f.x/  \\nx pC1 dx  ÏÏ  \\n: (4.23)  \\nAs an example, consider the recurrence \\nT.n/  D T.n=5/  C T.7n=10/  C n:  (4.24)  \\nWe’ll  see  the  similar  recurrence  (9.1)  on  page  240  when  we  study an algorithm for \\nselecting the i th smallest element from a set of n numbers. This recurrence has the \\nform  of equation  (4.22),  where  a 1 D a 2 D 1, b 1 D 5, b 2 D 10=7 , and f.n/  D n. \\nTo', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 138}),\n",
       "  'd0629714-2a06-4139-afb6-8eb91fb9a629': Document(page_content='.n/  D n. \\nTo  solve  it, the  Akra-Bazzi  method  says  that  we  should  determ ine the unique p \\nsatisfying \\nÎ 1 \\n5 Ï p \\nC Î 7 \\n10  Ï p \\nD 1:  \\nSolving for p is kind  of messy4it  turns  out  that  p D 0:83978:::4but  we  can  \\nsolve the recurrence without actually knowing the e xact value for p. Observe  that  \\n.1=5/  0 C .7=10/  0 D 2 and .1=5/  1 C .7=10/  1 D 9=10 , and thus p lies in the \\nrange 0 < p < 1 . That  turns  out  to be sufûcient  for  the  Akra-Bazzi  method  \\nto give  us the  solution.  We’ll  use  the  fact  from  calculus  that  if k ¤ \\ue0031, then R ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 138}),\n",
       "  '886453fa-f625-4ff6-bdd3-aa358a496909': Document(page_content=' ¤ \\ue0031, then R \\nx k dx  D x kC1 =.k  C 1/, which  we’ll  apply  with  k D \\ue003p ¤ \\ue0031. The  Akra-Bazzi  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 138}),\n",
       "  '60f95e25-7415-42b5-81cd-c06586c4a4bc': Document(page_content='118  Chapter  4 Divide-and-Conquer  \\nsolution  (4.23)  gives  us \\nT.n/  D ‚ Î \\nn p Î \\n1 C Z n \\n1 f.x/  \\nx pC1 dx  ÏÏ  \\nD ‚ Î \\nn p Î \\n1 C Z n \\n1 x \\ue002p dx  ÏÏ  \\nD ‚ Î \\nn p Î \\n1 C Ð x 1\\ue002p \\n1 \\ue003 p \\ue00b n \\n1 ÏÏ  \\nD ‚ Î \\nn p Î \\n1 C Î n 1\\ue002p \\n1 \\ue003 p \\ue003 1 \\n1 \\ue003 p ÏÏÏ  \\nD ‚ ã \\nn p \\ue001 ‚.n  1\\ue002p / ä \\n(because 1 \\ue003 p is a positive constant) \\nD ‚.n/  (by  Problem  3-5(d', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 139}),\n",
       "  '65e64cb1-26d0-42a4-b36b-a39fc3400090': Document(page_content=' (by  Problem  3-5(d))  . \\nAlthough  the  Akra-Bazzi  method  is more  general  than  the  mast er theorem, it \\nrequires calculus and sometimes a bit more reasonin g. You also must ensure that \\nyour  driving  function  satisûes  the  polynomial-growth  condition  if you  want  to ig-  \\nnore  üoors  and  ceilings,  although  that’s  rarely  a problem.  When it applies, the \\nmaster method is much simpler to use, but only when  subproblem sizes are more \\nor less equal. They are both good tools for your al gorithmic toolkit. \\nExercises  \\n? 4.7-1  \\nConsider  an Akra-Bazzi  recurrence  T.n/  on  the  reals  as given  in recurrence  (4.22),  \\nand  deûne  T 0 .n/  as \\nT 0 .n/  D cf.n/  C k X  \\ni D', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 139}),\n",
       "  '265d3ca7-f0ee-4b77-9e6f-52d4da1a77d4': Document(page_content='/  C k X  \\ni D1 a i T 0 .n=b  i /; \\nwhere c>0  is constant. Prove that whatever the implicit initi al conditions for T.n/  \\nmight be, there exist initial conditions for T 0 .n/  such that T 0 .n/  D cT.n/  for \\nall n>0 . Conclude that we can drop the asymptotics on a dr iving function in any \\nAkra-Bazzi  recurrence  without  affecting  its  asymptotic  solution. \\n4.7-2  \\nShow that f.n/  D n 2 satisûes  the  polynomial-growth  condition  but  that  f.n/  D 2 n \\ndoes not. \\n4.7-3  \\nLet f.n/  be a function  that  satisûes  the  polynomial-growth  conditio n. Prove that \\nf.n/  is asymptotically positive, that is, there exists a  constant n 0 \\ue004 0 such', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 139}),\n",
       "  '78d8b4aa-ff44-4fed-b925-a8f3c74dc252': Document(page_content=' a  constant n 0 \\ue004 0 such that \\nf.n/  \\ue004 0 for all n \\ue004 n 0 . ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 139}),\n",
       "  '4e1246d1-ca09-4548-afcb-dfc5854746bc': Document(page_content='Problems for Chapter 4 119 \\n? 4.7-4  \\nGive  an example  of a function  f.n/  that  does  not  satisfy  the  polynomial-growth  \\ncondition but for which f.‚.n//  D ‚.f.n// . \\n4.7-5  \\nUse  the  Akra-Bazzi  method  to solve  the  following  recurrence s. \\na. T.n/  D T.n=2/  C T.n=3/  C T.n=6/  C n lg n. \\nb. T.n/  D 3T.n=3/  C 8T.n=4/  C n 2 = lg n. \\nc. T.n/  D .2=3/T.n=3/  C .1=3/T.2n=3/  C lg n. \\nd. T.n/  D .1=3/T.n=3/  C 1=n. \\ne. T.n/  D', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 140}),\n",
       "  'be648e5e-0b9b-4ea6-82ad-8f51cd2a331c': Document(page_content=' \\ne. T.n/  D 3T.n=3/  C 3T.2n=3/  C n 2 . \\n? 4.7-6  \\nUse  the  Akra-Bazzi  method  to prove  the  continuous  master  theorem. \\nProblems  \\n4-1  Recurrence  examples  \\nGive  asymptotically  tight  upper  and  lower  bounds  for  T.n/  in each of the following \\nalgorithmic  recurrences.  Justify  your  answers.  \\na. T.n/  D 2T.n=2/  C n 3 . \\nb. T.n/  D T.8n=11/  C n. \\nc. T.n/  D 16T.n=4/  C n 2 . \\nd. T.n/  D 4T.n=2/  C n 2 lg n. \\ne. T.n/  D 8T.n=3/  C n 2 . \\nf. T.n/  D', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 140}),\n",
       "  '432887aa-7d9f-4db9-9a5a-5a9e72b0f8eb': Document(page_content=' \\nf. T.n/  D 7T.n=2/  C n 2 lg n. \\ng. T.n/  D 2T.n=4/  C p n. \\nh. T.n/  D T.n  \\ue003 2/ C n 2 . ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 140}),\n",
       "  '327b42fa-ba8c-4283-a907-061b19f786a4': Document(page_content='120  Chapter  4 Divide-and-Conquer  \\n4-2  Parameter-passing  costs  \\nThroughout this book, we assume that parameter pass ing during procedure calls \\ntakes constant time, even if an N -element  array  is being  passed.  This  assumption  \\nis valid in most systems because a pointer to the a rray is passed, not the array itself. \\nThis  problem  examines  the  implications  of three  parameter- passing strategies: \\n1. Arrays  are  passed  by  pointer.  Time  D ‚.1/ . \\n2. Arrays are passed by copying. Time D ‚.N/ , where N is the size of the array. \\n3. Arrays  are  passed  by  copying  only  the  subrange  that  might  be accessed by the \\ncalled procedure. Time D ‚.n/  if the subarray contains n elements. \\nConsider the following three algorithms: \\na. The  recursive  binary-search  algorithm  for  ûnding  a number  in a sorted array \\n(see  Exercise', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 141}),\n",
       "  'b7b2cd23-bbde-4e84-b48c-a5b330596105': Document(page_content=' in a sorted array \\n(see  Exercise  2.3-6).  \\nb. The MERGE-SORT  procedure  from  Section  2.3.1.  \\nc. The M ATRIX-MULTIPLY-RECURSIVE procedure  from  Section  4.1.  \\nGive  nine  recurrences  T a1  .N;n/;T  a2  .N;n/;:::;T  c3  .N;n/  for  the  worst-case  run-  \\nning times of each of the three algorithms above wh en arrays and matrices are \\npassed  using  each  of the  three  parameter-passing  strategies  above.  Solve  your  re-  \\ncurrences, giving tight asymptotic bounds. \\n4-3  Solving  recurrences  with  a change  of variables  \\nSometimes, a little algebraic manipulation can make  an unknown  recurrence  simi-  \\nlar  to one  you  have  seen  before.  Let’s  solve', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 141}),\n",
       "  '6168c953-9d43-4e66-a786-36ed1e28c75c': Document(page_content='  before.  Let’s  solve  the  recurrence  \\nT.n/  D 2T  ãp n ä C ‚.lg n/ (4.25)  \\nby  using  the  change-of-variables  method.  \\na. Deûne  m D lg n and S.m/  D T.2  m /. Rewrite  recurrence  (4.25)  in terms  of m \\nand S.m/ . \\nb. Solve your recurrence for S.m/ . \\nc. Use your solution for S.m/  to conclude that T.n/  D ‚.lg n lg lg n/. \\nd. Sketch  the  recursion  tree  for  recurrence  (4.25),  and  use  it to explain intuitively \\nwhy the solution is T.n/  D ‚.lg n lg lg n/. \\nSolve the following recurrences by changing variabl es: ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 141}),\n",
       "  '09c42dd0-c40b-4e08-8551-031281ae0457': Document(page_content='Problems for Chapter 4 121 \\ne. T.n/  D 2T.  p n/ C ‚.1/ . \\nf. T.n/  D 3T.  3 p n/ C ‚.n/ . \\n4-4  More  recurrence  examples  \\nGive  asymptotically  tight  upper  and  lower  bounds  for  T.n/  in each of the following \\nrecurrences.  Justify  your  answers.  \\na. T.n/  D 5T.n=3/  C n lg n. \\nb. T.n/  D 3T.n=3/  C n=  lg n. \\nc. T.n/  D 8T.n=2/  C n 3 p n. \\nd. T.n/  D 2T.n=2  \\ue003 2/ C n=2. \\ne. T.n/  D 2T.n=2/  C n=  lg n. \\nf. T.n/  D T.n=', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 142}),\n",
       "  '2ad4c69b-56f4-4ffd-a64c-3b3694eb455b': Document(page_content=' T.n/  D T.n=2/  C T.n=4/  C T.n=8/  C n. \\ng. T.n/  D T.n  \\ue003 1/ C 1=n. \\nh. T.n/  D T.n  \\ue003 1/ C lg n. \\ni. T.n/  D T.n  \\ue003 2/ C 1=  lg n. \\nj. T.n/  D p nT.  p n/ C n. \\n4-5  Fibonacci  numbers  \\nThis problem develops properties of the Fibonacci n umbers, which  are  deûned  \\nby  recurrence  (3.31)  on  page  69.  We’ll  explore  the  technique  of generating  func-  \\ntions  to solve  the  Fibonacci  recurrence.  Deûne  the  generating  function  (or formal  \\npower  series ) F as \\nF .´/  D 1  X  \\ni D0', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 142}),\n",
       "  '469a1495-26b8-44e0-9eac-b00ed45a5834': Document(page_content=' D 1  X  \\ni D0 F i ´ i \\nD 0 C ´ C ´ 2 C 2´  3 C 3´  4 C 5´  5 C 8´  6 C 13´  7 C 21´  8 C \\ue001 \\ue001 \\ue001  ; \\nwhere F i is the i th Fibonacci number. \\na. Show that F .´/  D ´ C ´F .´/  C ´ 2 F .´/. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 142}),\n",
       "  'dde0895a-646f-40a3-a086-ed3dfcdc5b4d': Document(page_content='122  Chapter  4 Divide-and-Conquer  \\nb. Show that \\nF .´/  D ´ \\n1 \\ue003 ´ \\ue003 ´ 2 \\nD ´ \\n.1 \\ue003 �´/.1  \\ue003 y �´/  \\nD 1 p \\n5 Î 1 \\n1 \\ue003 �´  \\ue003 1 \\n1 \\ue003 y �´  Ï \\n; \\nwhere � is the golden ratio, and y � is its  conjugate  (see  page  69).  \\nc. Show that \\nF .´/  D 1  X  \\ni D0 1 p \\n5 .�  i \\ue003 y � i /´ i : \\nYou  may  use  without  proof  the  generating-function  version  of equation  (A.7)  on  \\npage  1142,  P  1  \\nkD0 x k D 1=.1  \\ue003 x/. Because this equation involves a generating \\nfunction, x is a formal  variable,  not ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 143}),\n",
       "  '4316b415-3b07-4691-8b99-93937435406a': Document(page_content=' x is a formal  variable,  not  a real-valued  variable,  so that  you  don’t  \\nhave to worry about convergence of the summation or  about the requirement in \\nequation  (A.7)  that  jx j <1, which  doesn’t  make  sense  here.  \\nd. Use part (c) to prove that F i D � i = p \\n5 for i >0 , rounded to the nearest integer. \\n(Hint: Observe  that  ˇ ˇ y � ˇ ˇ <1.) \\ne. Prove that F i C2 \\ue004 � i for i \\ue004 0. \\n4-6  Chip  testing  \\nProfessor Diogenes has n supposedly  identical  integrated-circuit  chips  that  in prin-  \\nciple  are  capable  of testing  each  other.  The  professor’s  test jig accommodates two \\nchips at a time. When the jig is loaded, each chip tests the other and reports whether \\nit is good or', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 143}),\n",
       "  'e5d01efb-e6b6-48ca-854c-9cacc1d691eb': Document(page_content=' other and reports whether \\nit is good or bad. A good chip always reports accur ately whether the other chip is \\ngood or bad, but the professor cannot trust the ans wer of a bad chip. Thus, the four \\npossible outcomes of a test are as follows: \\nChip A says Chip B says Conclusion \\nB is good A is good both are good, or both are bad \\nB is good A is bad at least one is bad \\nB is bad A is good at least one is bad \\nB is bad A is bad at least one is bad \\na. Show that if at least n=2  chips  are  bad,  the  professor  cannot  necessarily  deter-  \\nmine which chips are good using any strategy based on this kind of pairwise \\ntest. Assume that the bad chips can conspire to foo l the professor. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 143}),\n",
       "  '58a748ce-378a-468d-8916-611cb38b00f5': Document(page_content='Problems for Chapter 4 123 \\nNow you will design an algorithm to identify which chips are good and which are \\nbad, assuming that more than n=2  of the chips are good. First, you will determine \\nhow to identify one good chip. \\nb. Show that bn=2c pairwise  tests  are  sufûcient  to reduce  the  problem  to one  of \\nnearly half the size. That is, show how to use bn=2c pairwise tests to obtain a \\nset with at most dn=2e chips that still has the property that more than ha lf of \\nthe chips are good. \\nc. Show how to apply the solution to part (b) recursiv ely to identify one good \\nchip.  Give  and  solve  the  recurrence  that  describes  the  numbe r of tests needed \\nto identify one good chip. \\nYou have now determined how to identify one good ch ip. \\nd. Show how to identify all the good chips with an add itional ‚.n/  pairwise tests. \\n4-7  Monge ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 144}),\n",
       "  '0eb42144-766e-48a5-82bd-16291a734916': Document(page_content='. \\n4-7  Monge  arrays  \\nAn m \\ue005 n array A of real numbers is a Monge  array  if for all i , j , k, and l such \\nthat 1 හ i <k  හ m and 1 හ j <l  හ n, we have \\nAŒi; j �  C AŒk; l�  හ AŒi; l�  C AŒk; j � :  \\nIn other words, whenever we pick two rows and two c olumns of a Monge array and \\nconsider the four elements at the intersections of the rows and the columns, the sum \\nof the  upper-left  and  lower-right  elements  is less  than  or equal to the sum of the \\nlower-left  and  upper-right  elements.  For  example,  the  following array is Monge: \\n10  17  13  28  23  \\n17  22  16  29  23  \\n24  28  22  34  24  \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 144}),\n",
       "  'd5cc8073-ff3b-4751-8635-4910b693f610': Document(page_content=' 28  22  34  24  \\n11  13  6 17  7 \\n45  44  32  37  23  \\n36  33  19  21  6 \\n75  66  51  53  34  \\na. Prove that an array is Monge if and only if for all  i D 1;2;:::;m  \\ue003 1 and \\nj D 1;2;:::;n  \\ue003 1, we have \\nAŒi; j �  C AŒi  C 1;j  C 1� හ AŒi;j  C 1� C AŒi  C 1; j � :  \\n(Hint: For the <if= part, use induction separately on rows  and columns.) \\nb. The following array is not Monge. Change one elemen t in order to make it \\nMonge. ( Hint: Use part (a).) ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 144}),\n",
       "  'ed091ddb-bfcb-4602-8dcc-1ae2f003eef1': Document(page_content='124  Chapter  4 Divide-and-Conquer  \\n37  23  22  32  \\n21  6 7 10  \\n53  34  30  31  \\n32  13  9 6 \\n43  21  15  8 \\nc. Let f.i/  be the index of the column containing the leftmost minimum element \\nof row i . Prove that f.1/  හ f.2/  හ \\ue001 \\ue001 \\ue001 හ  f.m/  for any m \\ue005 n Monge array. \\nd. Here  is a description  of a divide-and-conquer  algorithm  that  computes  the  left-  \\nmost minimum element in each row of an m \\ue005 n Monge array A: \\nConstruct a submatrix A 0 of A consisting  of the  even-numbered  rows  of A. \\nRecursively determine the leftmost minimum for each  row of A 0 . Then \\ncompute  the  leftmost  minimum  in the  odd-numbered  rows  of A. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 145}),\n",
       "  '71cd5f04-88f4-41c4-bbb6-cea11adcad13': Document(page_content=' odd-numbered  rows  of A. \\nExplain  how  to compute  the  leftmost  minimum  in the  odd-numbe red rows of A \\n(given  that  the  leftmost  minimum  of the  even-numbered  rows  is known) in \\nO.m  C n/ time. \\ne. Write the recurrence for the running time of the al gorithm in part (d). Show \\nthat its solution is O.m  C n log m/. \\nChapter  notes  \\nDivide-and-conquer  as a technique  for  designing  algorithm s dates back at least to \\n1962  in an article  by  Karatsuba  and  Ofman  [242],  but  it might  have been used \\nwell  before  then.  According  to Heideman,  Johnson,  and  Burrus  [211],  C. F. Gauss  \\ndevised  the  ûrst  fast  Fourier  transform  algorithm  in 1805,  and  Gauss’s  formulation  \\nbreaks the problem into smaller', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 145}),\n",
       "  '0d647c0b-b53d-4806-bc2a-0bda4f387c42': Document(page_content='  formulation  \\nbreaks the problem into smaller subproblems whose s olutions are combined. \\nStrassen’s  algorithm  [424]  caused  much  excitement  when  it appeared  in 1969.  \\nBefore then, few imagined the possibility of an alg orithm asymptotically faster than \\nthe basic M ATRIX-MULTIPLY procedure. Shortly thereafter, S. Winograd reduced \\nthe  number  of submatrix  additions  from  18  to 15  while  still  using seven submatrix \\nmultiplications. This improvement, which Winograd a pparently never published \\n(and which is frequently miscited in the literature ), may enhance the practicality \\nof the method, but it does not affect its asymptoti c performance.  Probert  [368]  \\ndescribed  Winograd’s  algorithm  and  showed  that  with  seven  multiplications,  15  \\nadditions is the minimum possible. \\nStrassen’s  ‚.n  lg 7 / D O.n  2:81  / bound', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 145}),\n",
       "  '725b5859-f4ff-48f8-9674-603b9c2ed547': Document(page_content=' O.n  2:81  / bound  for  matrix  multiplication  held  until  1987,  \\nwhen  Coppersmith  and  Winograd  [103]  made  a signiûcant  advan ce, improving the ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 145}),\n",
       "  '0155cf4d-ac67-4766-a1e1-897a0635cb74': Document(page_content='Notes for Chapter 4 125 \\nbound to O.n  2:376  / time with a mathematically sophisticated but wildly  impracti- \\ncal algorithm based on tensor products. It took app roximately 25  years  before  the  \\nasymptotic  upper  bound  was  again  improved.  In 2012  Vassilevska  Williams  [445]  \\nimproved it to O.n  2:37287  /, and  two  years  later  Le  Gall  [278]  achieved  O.n  2:37286  /, \\nboth of them using mathematically fascinating but i mpractical algorithms. The best \\nlower bound to date is just the obvious �.n  2 / bound  (obvious  because  any  algo-  \\nrithm  for  matrix  multiplication  must  ûll  in the  n 2 elements of the product matrix). \\nThe performance of M ATRIX-MULTIPLY-RECURSIVE can  be improved  in prac-  \\ntice by coarsening the leaves of the recursion. It also', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 146}),\n",
       "  '800c5961-212d-439f-80d0-615a338a41aa': Document(page_content='ening the leaves of the recursion. It also exhibits  better  cache  behav-  \\nior than M ATRIX-MULTIPLY , although M ATRIX-MULTIPLY can be improved by \\n<tiling.=  Leiserson  et al.  [293]  conducted  a performance-engineering  study  of ma-  \\ntrix multiplication in which a parallel and vectori zed divide-and-conquer  algorithm  \\nachieved  the  highest  performance.  Strassen’s  algorithm  can be practical for large \\ndense matrices, although large matrices tend to be sparse, and sparse methods can \\nbe much  faster.  When  using  limited-precision  üoating-point  values,  Strassen’s  al-  \\ngorithm produces larger numerical errors than the ‚.n  3 / algorithms do, although \\nHigham  [215]  demonstrated  that  Strassen’s  algorithm  is amply accurate for some \\napplications. \\nRecurrences  were  studied  as early  as 1202', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 146}),\n",
       "  '2a0c167c-1a90-47c5-9c41-d9dbc4d68d23': Document(page_content=' were  studied  as early  as 1202  by  Leonardo  Bonacci  [66],  also  \\nknown as Fibonacci, for whom the Fibonacci numbers are named, although Indian \\nmathematicians had discovered Fibonacci numbers cen turies before. The French \\nmathematician  De  Moivre  [108]  introduced  the  method  of gene rating functions \\nwith  which  he studied  Fibonacci  numbers  (see  Problem  4-5).  Knuth  [259]  and  \\nLiu  [302]  are  good  resources  for  learning  the  method  of gener ating functions. \\nAho,  Hopcroft,  and  Ullman  [5,  6] offered  one  of the  ûrst  gener al methods for \\nsolving  recurrences  arising  from  the  analysis  of divide-and-conquer  algorithms.  \\nThe master method was adapted from Bentley, Haken, and Saxe [52].  The  Akra-  \\nBazzi  method  is due  (', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 146}),\n",
       "  '7774c8d8-7133-4525-b25f-0d1863b3268d': Document(page_content='Bazzi  method  is due  (unsurprisingly)  to Akra  and  Bazzi  [13].  Divide-and-conquer  \\nrecurrences have been studied by many researchers, including  Campbell  [79],  Gra-  \\nham,  Knuth,  and  Patashnik  [199],  Kuszmaul  and  Leiserson  [274],  Leighton  [287],  \\nPurdom  and  Brown  [371],  Roura  [389],  Verma  [447],  and  Yap  [462]. \\nThe  issue  of üoors  and  ceilings  in divide-and-conquer  recur rences, including a \\ntheorem  similar  to Theorem  4.5,  was  studied  by  Leighton  [287].  Leighton  pro-  \\nposed  a version  of the  polynomial-growth  condition.  Campbell  [79]  removed  sev-  \\neral  limitations  in Leighton’s  statement  of it and  showed  that  there', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 146}),\n",
       "  '471f4c9b-bcf4-40de-8a62-99c56245eb48': Document(page_content='  of it and  showed  that  there  were  polyno-  \\nmially  bounded  functions  that  do  not  satisfy  Leighton’s  condition. Campbell also \\ncarefully studied many other technical issues, incl uding the well-deûnedness  of \\ndivide-and-conquer  recurrences.  Kuszmaul  and  Leiserson  [274]  provided  a proof  \\nof Theorem  4.5  that  does  not  involve  calculus  or other  higher  math.  Both  Camp-  \\nbell and Leighton explored the perturbations of arg uments beyond  simple  üoors  \\nand ceilings. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 146}),\n",
       "  '73a33525-5e35-4e8e-9796-82896c4b0842': Document(page_content='5 Probabilistic  Analysis  and  Randomized  \\nAlgorithms  \\nThis chapter introduces probabilistic analysis and randomized algorithms. If you \\nare unfamiliar with the basics of probability theor y, you should read Sections \\nC.13C.4  of Appendix  C, which  review  this  material.  We’ll  revisit probabilistic \\nanalysis and randomized algorithms several times th roughout this book. \\n5.1  The  hiring  problem  \\nSuppose  that  you  need  to hire  a new  ofûce  assistant.  Your  previous attempts at \\nhiring have been unsuccessful, and you decide to us e an employment agency. The \\nemployment agency sends you one candidate each day.  You interview that person \\nand then decide either to hire that person or not. You must pay the employment \\nagency a small fee to interview an applicant. To ac tually hire an applicant is more \\ncostly,  however,  since  you  must  ûre  your  current  ofûce  assis tant and also pay a \\nsubstantial hiring fee to the employment agency. Yo u', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 147}),\n",
       "  '1d954c58-3eb5-4b5d-a303-3e93f1ce5b8d': Document(page_content='stantial hiring fee to the employment agency. Yo u are committed to having, at \\nall times, the best possible person for the job. Th erefore, you decide that, after \\ninterviewing each applicant, if that applicant is b etter qualiûed  than  the  current  \\nofûce  assistant,  you  will  ûre  the  current  ofûce  assistant  and hire the new applicant. \\nYou are willing to pay the resulting price of this strategy, but you wish to estimate \\nwhat that price will be. \\nThe procedure H IRE-ASSISTANT on the facing page expresses this strategy for \\nhiring  in pseudocode.  The  candidates  for  the  ofûce  assistan t job are numbered 1 \\nthrough n and interviewed in that order. The procedure assume s that after  inter-  \\nviewing candidate i , you can determine whether candidate i is the best candidate \\nyou have seen so far. It starts by creating a dummy  candidate, numbered 0, who is \\nless  qualiûed  than  each  of the  other  candidates.  \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 147}),\n",
       "  'd78db288-6cf3-4e7e-bf4a-f7dc1f64f9f0': Document(page_content=' of the  other  candidates.  \\nThe cost model for this problem differs from the mo del described in Chapter 2. \\nWe focus not on the running time of H IRE-ASSISTANT , but instead on the fees paid \\nfor  interviewing  and  hiring.  On  the  surface,  analyzing  the  cost of this algorithm ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 147}),\n",
       "  'c0e57a2e-1326-4999-99c3-9bf78cd83e63': Document(page_content='5.1 The hiring problem 127 \\nHIRE-ASSISTANT .n/  \\n1 best D 0 / / candidate  0 is a least-qualiûed  dummy  candidate  \\n2 for  i D 1 to n \\n3 interview candidate i \\n4 if candidate i is better than candidate best \\n5 best D i \\n6 hire candidate i \\nmay seem very different from analyzing the running time of, say, merge sort. The \\nanalytical techniques used, however, are identical whether we are analyzing cost \\nor running time. In either case, we are counting th e number of times certain basic \\noperations are executed. \\nInterviewing has a low cost, say c i , whereas hiring is expensive, costing c h . Let-  \\nting m be the number of people hired, the total cost assoc iated with this algorithm \\nis O.c  i n C c h m/. No matter how many people you hire, you always in terview n \\ncandidates and thus always incur the cost c i n associated with interviewing. We \\ntherefore concentrate on analyzing c h m, the hiring cost. This quantity depends on', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 148}),\n",
       "  'b4a8f825-acb1-4732-8276-6187bf956c54': Document(page_content=' m, the hiring cost. This quantity depends on \\nthe order in which you interview candidates. \\nThis scenario serves as a model for a common comput ational paradigm.  Al-  \\ngorithms  often  need  to ûnd  the  maximum  or minimum  value  in a sequence by \\nexamining each element of the sequence and maintain ing a current <winner.= The \\nhiring problem models how often a procedure updates  its notion of which element \\nis currently winning. \\nWorst-case  analysis  \\nIn the worst case, you actually hire every candidat e that you interview.  This  situa-  \\ntion occurs if the candidates come in strictly incr easing order of quality, in which \\ncase you hire n times, for a total hiring cost of O.c  h n/. \\nOf  course,  the  candidates  do  not  always  come  in increasing  order of quality. In \\nfact, you have no idea about the order in which the y arrive, nor do you have any \\ncontrol over this order. Therefore, it is natural t o ask what we expect to happen in \\n', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 148}),\n",
       "  'd1e7430a-dd20-4a4c-b90c-dd13ce275977': Document(page_content=' o ask what we expect to happen in \\na typical or average case. \\nProbabilistic  analysis  \\nProbabilistic  analysis  is the use of probability in the analysis of proble ms. Most \\ncommonly, we use probabilistic analysis to analyze the running  time  of an algo-  \\nrithm. Sometimes we use it to analyze other quantit ies, such as the hiring cost in ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 148}),\n",
       "  '8adebbd4-ec66-421f-984c-23ccde3ff82d': Document(page_content='128  Chapter  5 Probabilistic  Analysis  and  Randomized  Algori thms \\nprocedure H IRE-ASSISTANT . In order to perform a probabilistic analysis, we must \\nuse knowledge of, or make assumptions about, the di stribution of the inputs. Then \\nwe  analyze  our  algorithm,  computing  an average-case  runnin g time, where we take \\nthe average, or expected value, over the distributi on of the possible inputs. When \\nreporting such a running time, we refer to it as th e average-case  running  time. \\nYou must be careful in deciding on the distribution  of inputs. For some problems, \\nyou may reasonably assume something about the set o f all possible inputs, and \\nthen you can use probabilistic analysis as a techni que for designing  an efûcient  \\nalgorithm and as a means for gaining insight into a  problem. For other problems, \\nyou cannot characterize a reasonable input distribu tion, and in these cases you \\ncannot use probabilistic analysis. \\nFor the hiring problem, we can assume that the appl', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 149}),\n",
       "  '7f80f235-62d9-485e-af84-da07431ed100': Document(page_content=' the hiring problem, we can assume that the appl icants come in a random \\norder.  What  does  that  mean  for  this  problem?  We  assume  that  you can compare \\nany  two  candidates  and  decide  which  one  is better  qualiûed,  which is to say that \\nthere is a total order on the candidates. (See Sect ion B.2 for the  deûnition  of a total  \\norder.) Thus, you can rank each candidate with a un ique number from 1 through n, \\nusing rank.i/  to denote the rank of applicant i , and adopt the convention that a \\nhigher  rank  corresponds  to a better  qualiûed  applicant.  The  ordered list hrank.1/;  \\nrank.2/;:::;  rank.n/i is a permutation of the list h1;2;:::;n i. Saying that the \\napplicants come in a random order is equivalent to saying that this list of ranks is \\nequally likely to be any one of the nŠ permutations of the numbers', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 149}),\n",
       "  '16604c44-b199-4a64-b3b2-b7f7a061248c': Document(page_content=' of the nŠ permutations of the numbers 1 through n. \\nAlternatively, we say that the ranks form a uniform  random  permutation , that is, \\neach of the possible nŠ permutations appears with equal probability. \\nSection  5.2  contains  a probabilistic  analysis  of the  hiring  problem. \\nRandomized  algorithms  \\nIn order to use probabilistic analysis, you need to  know something  about  the  dis-  \\ntribution of the inputs. In many cases, you know li ttle about the  input  distribu-  \\ntion. Even if you do know something about the distr ibution, you might not be able \\nto model this knowledge computationally. Yet, proba bility and randomness often \\nserve as tools for algorithm design and analysis, b y making part of the algorithm \\nbehave randomly. \\nIn the hiring problem, it may seem as if the candid ates are being presented to \\nyou in a random order, but you have no way of knowi ng whether they really are. \\nThus, in order to develop a randomized algorithm fo r the', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 149}),\n",
       "  '49041716-d50d-47bb-b0f1-130c7795fa54': Document(page_content=' in order to develop a randomized algorithm fo r the hiring problem, you need \\ngreater  control  over  the  order  in which  you’ll  interview  the  candidates. We will, \\ntherefore, change the model slightly. The employmen t agency sends you a list of \\nthe n candidates  in advance.  On  each  day,  you  choose,  randomly,  which  candi-  \\ndate to interview. Although you know nothing about the candidates (besides their \\nnames),  we  have  made  a signiûcant  change.  Instead  of accepti ng the order given ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 149}),\n",
       "  '69b99a1b-38c5-467f-ad6d-b3a23a54924d': Document(page_content='5.1 The hiring problem 129 \\nto you  by  the  employment  agency  and  hoping  that  it’s  random,  you have instead \\ngained control of the process and enforced a random  order. \\nMore generally, we call an algorithm randomized  if its behavior is determined \\nnot only by its input but also by values produced b y a random-number  generator . \\nWe  assume  that  we  have  at our  disposal  a random-number  genera tor RANDOM . \\nA call to R ANDOM.a;b/  returns an integer between a and b, inclusive, with each \\nsuch integer being equally likely. For example, R ANDOM.0;1/  produces 0 with \\nprobability 1=2, and it produces 1 with probability 1=2. A call to R ANDOM.3;7/  \\nreturns any one of 3, 4, 5, 6, or 7, each with probability 1=5. Each integer returned \\nby RANDOM  is independent of the integers returned on previous  calls. You may \\nimagine R ANDOM  as rolling a .', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 150}),\n",
       "  '99572c82-6c62-4617-89cd-b3e4d873d2f3': Document(page_content='imagine R ANDOM  as rolling a .b \\ue003 a C 1/-sided  die  to obtain  its  output.  (In  prac-  \\ntice, most programming environments offer a pseudorandom-number  generator : \\na deterministic algorithm returning numbers that <l ook= statistically random.) \\nWhen analyzing the running time of a randomized alg orithm, we take  the  expec-  \\ntation of the running time over the distribution of  values returned by the random \\nnumber generator. We distinguish these algorithms f rom those in which the input \\nis random by referring to the running time of a ran domized algorithm as an ex-  \\npected  running  time. In general,  we  discuss  the  average-case  running  time  when  \\nthe probability distribution is over the inputs to the algorithm, and we discuss the \\nexpected running time when the algorithm itself mak es random choices. \\nExercises  \\n5.1-1  \\nShow that the assumption that you are always able t o determine which candidate is \\nbest,  in line  4 of procedure  HI', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 150}),\n",
       "  'b6303265-debf-4fbd-bb58-b53457ac9d77': Document(page_content=',  in line  4 of procedure  HIRE-ASSISTANT , implies that you know a total order \\non the ranks of the candidates. \\n? 5.1-2  \\nDescribe an implementation of the procedure R ANDOM.a;b/  that makes calls only \\nto RANDOM.0;1/ . What is the expected running time of your procedu re, as a \\nfunction of a and b? \\n? 5.1-3  \\nYou wish to implement a program that outputs 0 with probability 1=2  and 1 with \\nprobability 1=2. At your disposal is a procedure B IASED-RANDOM  that outputs \\neither 0 or 1, but it outputs 1 with some probability p and 0 with probability 1 \\ue003 p, \\nwhere 0 < p < 1 . You do not know what p is. Give  an algorithm  that  uses  \\nBIASED-RANDOM  as a subroutine, and returns an unbiased answer, re turning 0 \\nwith probability 1=2  and 1 with probability 1=2. What is the expected running \\ntime of your algorithm as a function of', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 150}),\n",
       "  '5943fd0c-5937-4eab-b41f-2585181b86c4': Document(page_content=' \\ntime of your algorithm as a function of p? ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 150}),\n",
       "  '6414eaae-61c8-4685-a704-efc165b0c4c1': Document(page_content='130  Chapter  5 Probabilistic  Analysis  and  Randomized  Algori thms \\n5.2  Indicator  random  variables  \\nIn order to analyze many algorithms, including the hiring problem, we use indicator \\nrandom variables. Indicator random variables provid e a convenient method for \\nconverting  between  probabilities  and  expectations.  Given  a sample space S and an \\nevent A, the indicator  random  variable  I fAg associated with event A is deûned  as \\nI fAg D  ( \\n1 if A occurs ; \\n0 if A does not occur : (5.1)  \\nAs a simple example, let us determine the expected number of heads obtained \\nwhen  üipping  a fair  coin.  The  sample  space  for  a single  coin  üip is S D fH;T  g, \\nwith Pr fH g D  Pr fT g D  1=2. We  can  then  deûne  an indicator  random  vari-  \\nable X H  , associated with the coin coming up heads, which i s the', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 151}),\n",
       "  '2ee468dd-21a7-4cff-a6cf-fa897823ad64': Document(page_content=' the coin coming up heads, which i s the event H . This \\nvariable  counts  the  number  of heads  obtained  in this  üip,  and  it is 1 if the coin \\ncomes up heads and 0 otherwise. We write \\nX H  D I fH g \\nD ( \\n1 if H occurs ; \\n0 if T occurs : \\nThe  expected  number  of heads  obtained  in one  üip  of the  coin  is simply  the  ex-  \\npected value of our indicator variable X H  : \\nE ŒX  H  � D E ŒI fH g� \\nD 1 \\ue001 Pr fH g C  0 \\ue001 Pr fT g \\nD 1 \\ue001 .1=2/  C 0 \\ue001 .1=2/  \\nD 1=2:  \\nThus  the  expected  number  of heads  obtained  by  one  üip  of a fair  coin is 1=2. As \\nthe following lemma shows, the expected value of an  indicator random variable \\nassociated with', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 151}),\n",
       "  '19eeceec-9515-4a52-9de1-f1090eb3046f': Document(page_content=' of an  indicator random variable \\nassociated with an event A is equal to the probability that A occurs. \\nLemma  5.1  \\nGiven  a sample  space  S and an event A in the sample space S , let X A D I fAg. \\nThen E ŒX  A � D Pr fAg. \\nProof  By  the  deûnition  of an indicator  random  variable  from  equation  (5.1)  and  \\nthe  deûnition  of expected  value,  we  have  \\nE ŒX  A � D E ŒI fAg� \\nD 1 \\ue001 Pr fAg C  0 \\ue001 Pr ˚ \\nA \\ue009 \\nD Pr fAg ; ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 151}),\n",
       "  'de307dfc-3660-4f05-bb4a-ea331b227696': Document(page_content='5.2  Indicator  random  variables  131 \\nwhere A denotes S \\ue003 A, the complement of A. \\nAlthough indicator random variables may seem cumber some for an applica-  \\ntion  such  as counting  the  expected  number  of heads  on  a üip  of a single coin, \\nthey are useful for analyzing situations that perfo rm repeated random trials. In \\nAppendix C, for example, indicator random variables  provide a simple way to \\ndetermine the expected number of heads in n coin  üips.  One  option  is to con-  \\nsider separately the probability of obtaining 0 heads, 1 head, 2 heads,  etc.  to ar-  \\nrive  at the  result  of equation  (C.41)  on  page  1199.  Alternati  vely, we can employ \\nthe  simpler  method  proposed  in equation  (C.42),  which  uses  indicator random \\nvariables implicitly. Making this argument more exp licit, let X i be the indicator \\nrandom variable associated with the event in which the i', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 152}),\n",
       "  '853b2bb4-8ac0-42d8-91b4-1162610a09d0': Document(page_content='random variable associated with the event in which the i th üip  comes  up  heads:  \\nX i D I fthe i th üip  results  in the  event  H g. Let X be the  random  variable  denot-  \\ning the total number of heads in the n coin  üips,  so that  \\nX D n X  \\ni D1 X i : \\nIn order to compute the expected number of heads, t ake the expectation of both \\nsides of the above equation to obtain \\nE ŒX�  D E \" n X  \\ni D1 X i # \\n: (5.2)  \\nBy  Lemma  5.1,  the  expectation  of each  of the  random  variables  is E ŒX  i � D 1=2  for \\ni D 1;2;:::;n . Then we can compute the sum of the expectations: P  n \\ni D1 E ŒX  i � D \\nn=2. But  equation  (5.2)  calls  for  the ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 152}),\n",
       "  '30b5a31c-e37f-404e-b26a-4ca362bdfe2d': Document(page_content='.2)  calls  for  the  expectation  of the  sum,  not  the  sum  of the  ex-  \\npectations.  How  can  we  resolve  this  conundrum?  Linearity  of expectation,  equa-  \\ntion  (C.24)  on  page  1192,  to the  rescue:  the  expectation  of the  sum  always  equals  \\nthe  sum  of the  expectations . Linearity of expectation applies even when there is \\ndependence among the random variables. Combining in dicator random variables \\nwith linearity of expectation gives us a powerful t echnique to compute expected \\nvalues when multiple events occur. We now can compu te the expected number of \\nheads: \\nE ŒX�  D E \" n X  \\ni D1 X i # \\nD n X  \\ni D1 E ŒX  i � \\nD n X  \\ni D1 1=2  \\nD n=2:  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 152}),\n",
       "  '26ca0f96-ee10-4089-b42c-3ccfe447038d': Document(page_content='132  Chapter  5 Probabilistic  Analysis  and  Randomized  Algori thms \\nThus,  compared  with  the  method  used  in equation  (C.41),  indicator  random  vari-  \\nables greatly simplify the calculation. We use indi cator random  variables  through-  \\nout this book. \\nAnalysis  of the  hiring  problem  using  indicator  random  variables  \\nReturning to the hiring problem, we now wish to com pute the expected number of \\ntimes  that  you  hire  a new  ofûce  assistant.  In order  to use  a probabilistic analysis, \\nlet’s  assume  that  the  candidates  arrive  in a random  order,  as discussed  in Sec-  \\ntion  5.1.  (We’ll  see  in Section  5.3  how  to remove  this  assumpt ion.) Let X be the \\nrandom variable whose value equals the number of ti mes you hire a new  ofûce  as-  \\nsistant.  We  could  then ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 153}),\n",
       "  '3f886341-be9d-40de-9edc-fd402edfe211': Document(page_content='sistant.  We  could  then  apply  the  deûnition  of expected  value  from  equation  (C.23)  \\non  page  1192  to obtain  \\nE ŒX�  D n X  \\nxD1 x Pr fX D x g ; \\nbut  this  calculation  would  be cumbersome.  Instead,  let’s  simplify the calculation \\nby using indicator random variables. \\nTo use indicator random variables, instead of compu ting E ŒX�  by  deûning  just  \\none  variable  denoting  the  number  of times  you  hire  a new  ofûce  assistant, think \\nof the  process  of hiring  as repeated  random  trials  and  deûne  n variables indicating \\nwhether each particular candidate is hired. In part icular, let X i be the indicator \\nrandom variable associated with the event in which the i th candidate is hired. Thus, \\nX i D I fcandidate i is hired g \\nD ( \\n1 if candidate i is hired ; \\n0', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 153}),\n",
       "  '7b3b0a66-416d-4e03-bf00-185bd49deede': Document(page_content='1 if candidate i is hired ; \\n0 if candidate i is not hired ; \\nand \\nX D X 1 C X 2 C \\ue001 \\ue001 \\ue001 C  X n : (5.3)  \\nLemma  5.1  gives  \\nE ŒX  i � D Pr fcandidate i is hired g ; \\nand  we  must  therefore  compute  the  probability  that  lines  536  of H IRE-ASSISTANT \\nare executed. \\nCandidate i is hired,  in line  6, exactly  when  candidate  i is better than each of \\ncandidates 1 through i \\ue003 1. Because we have assumed that the candidates arriv e in \\na random  order,  the  ûrst  i candidates have appeared in a random order. Any one  of \\nthese  ûrst  i candidates  is equally  likely  to be the  best  qualiûed  so far.  Candidate i \\nhas a probability of 1=i  of being  better  qualiûed  than  candidates  1 through i ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 153}),\n",
       "  'a9d79555-a15a-4f25-835c-237eba4aeaf9': Document(page_content='ed  than  candidates  1 through i \\ue003 1 \\nand thus a probability of 1=i  of being  hired.  By  Lemma  5.1,  we  conclude  that  ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 153}),\n",
       "  '59a97fce-b1e3-4a3d-b841-875b3f1423a8': Document(page_content='5.2  Indicator  random  variables  133 \\nE ŒX  i � D 1=i:  (5.4)  \\nNow we can compute E ŒX�: \\nE ŒX�  D E \" n X  \\ni D1 X i # \\n(by  equation  (5.3))  (5.5)  \\nD n X  \\ni D1 E ŒX  i � (by  equation  (C.24),  linearity  of expectation)  \\nD n X  \\ni D1 1 \\ni (by  equation  (5.4))  \\nD ln n C O.1/  (by  equation  (A.9),  the  harmonic  series)  . (5.6)  \\nEven though you interview n people, you actually hire only approximately ln n of \\nthem, on average. We summarize this result in the f ollowing lemma. \\nLemma  5.2  \\nAssuming that the candidates are presented in a ran dom order, algorithm H IRE- \\nASSIST', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 154}),\n",
       "  '0fe1720d-b072-477a-b6fc-124fd1e37767': Document(page_content=', algorithm H IRE- \\nASSISTANT has  an average-case  total  hiring  cost  of O.c  h ln n/. \\nProof  The  bound  follows  immediately  from  our  deûnition  of the  hiring cost \\nand  equation  (5.6),  which  shows  that  the  expected  number  of hires  is approxi-  \\nmately ln n. \\nThe  average-case  hiring  cost  is a signiûcant  improvement  over  the  worst-case  \\nhiring cost of O.c  h n/. \\nExercises  \\n5.2-1  \\nIn H IRE-ASSISTANT, assuming  that  the  candidates  are  presented  in a random  or-  \\nder,  what  is the  probability  that  you  hire  exactly  one  time?  What is the probability \\nthat you hire exactly n times?  \\n5.2-2  \\nIn H IRE-ASSISTANT, assuming  that  the  candidates  are  presented', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 154}),\n",
       "  '4b9df9bc-3391-4732-be25-2acb7d6f2026': Document(page_content='  that  the  candidates  are  presented  in a random  or-  \\nder,  what  is the  probability  that  you  hire  exactly  twice?  \\n5.2-3  \\nUse indicator random variables to compute the expec ted value of the sum of n dice. ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 154}),\n",
       "  '7864ca66-87a4-403f-a221-ad6a4adc4a39': Document(page_content='134  Chapter  5 Probabilistic  Analysis  and  Randomized  Algori thms \\n5.2-4  \\nThis exercise asks you to (partly) verify that line arity of expectation holds even \\nif the random variables are not independent. Consid er two 6-sided  dice  that  are  \\nrolled  independently.  What  is the  expected  value  of the  sum?  Now consider the \\ncase  where  the  ûrst  die  is rolled  normally  and  then  the  second  die is set equal to the \\nvalue  shown  on  the  ûrst  die.  What  is the  expected  value  of the  sum?  Now  consider  \\nthe  case  where  the  ûrst  die  is rolled  normally  and  the  second  die is set equal to 7 \\nminus  the  value  of the  ûrst  die.  What  is the  expected  value  of the  sum?  \\n5.2-5  \\nUse indicator random variables to solve the followi', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 155}),\n",
       "  '4feb9cb8-5fa3-4ef5-b4a2-2c242515f004': Document(page_content='\\nUse indicator random variables to solve the followi ng problem, which is known as \\nthe hat-check  problem . Each of n customers  gives  a hat  to a hat-check  person  at a \\nrestaurant.  The  hat-check  person  gives  the  hats  back  to the  customers in a random \\norder. What is the expected number of customers who  get back their  own  hat?  \\n5.2-6  \\nLet AŒ1  W n� be an array of n distinct numbers. If i <j  and AŒi� > AŒj � , then the \\npair .i;j/  is called an inversion  of A. (See  Problem  2-4  on  page  47  for  more  on  \\ninversions.) Suppose that the elements of A form a uniform random permutation \\nof h1;2;:::;n i. Use indicator random variables to compute the exp ected number \\nof inversions. \\n5.3  Randomized  algorithms  \\nIn the previous section, we showed how knowing a di', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 155}),\n",
       "  'e026fff4-a767-4068-a874-79892905c2b5': Document(page_content=' the previous section, we showed how knowing a di stribution on the inputs can \\nhelp  us to analyze  the  average-case  behavior  of an algorithm . What if you do \\nnot  know  the  distribution?  Then  you  cannot  perform  an average-case  analysis.  \\nAs  mentioned  in Section  5.1,  however,  you  might  be able  to use  a randomized \\nalgorithm. \\nFor a problem such as the hiring problem, in which it is helpful to assume that \\nall permutations of the input are equally likely, a  probabilistic analysis can guide \\nus when developing a randomized algorithm. Instead of assuming  a distribution \\nof inputs, we impose a distribution. In particular, before running the a lgorithm, \\nlet’s  randomly  permute  the  candidates  in order  to enforce  the property that every \\npermutation  is equally  likely.  Although  we  have  modiûed  the  algorithm, we still \\nexpect  to hire  a new  ofûce  assistant  approximately  ln n times. But now we', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 155}),\n",
       "  '44552ad2-5570-47c0-8ded-55674b1f7b4a': Document(page_content=' approximately  ln n times. But now we expect \\nthis to be the case for any  input, rather than for inputs drawn from a particul ar \\ndistribution. \\nLet us further explore the distinction between prob abilistic analysis  and  ran-  \\ndomized  algorithms.  In Section  5.2,  we  claimed  that,  assumi ng that the candidates ', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 155}),\n",
       "  '34dc3ed3-5db5-4c00-87e9-48d7f74d5af1': Document(page_content='5.3  Randomized  algorithms  135 \\narrive in a random order, the expected number of ti mes you hire a new  ofûce  as-  \\nsistant is about ln n. This algorithm is deterministic: for any particul ar input, the \\nnumber  of times  a new  ofûce  assistant  is hired  is always  the  same. Furthermore, \\nthe  number  of times  you  hire  a new  ofûce  assistant  differs  for  different inputs, \\nand it depends on the ranks of the various candidat es. Since this number depends \\nonly on the ranks of the candidates, to represent a  particular input, we can just \\nlist, in order, the ranks hrank.1/;  rank.2/;:::;  rank.n/i of the  candidates.  Given  \\nthe rank list A 1 D h1;2;3;4;5;6;7;8;9;10 i, a new  ofûce  assistant  is always  \\nhired 10  times, since each successive candidate is better th an the previous one', metadata={'source': 'data/web_data//introduction to algo4 new.pdf', 'page': 156}),\n",
       "  ...}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = len(db.docstore.__dict__['_dict'])\n",
    "print(length)\n",
    "\n",
    "db.docstore.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
