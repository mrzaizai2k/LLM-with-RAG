{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mrzaizai2k/code_Bao/LLM-with-RAG/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(file_path)\n",
    "except:\n",
    "    file_path = os.path.abspath('')\n",
    "    os.chdir(os.path.dirname(file_path))\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "                                                  PyMuPDFLoader,\n",
    "                                                  NewsURLLoader,\n",
    "                                                  YoutubeLoader,\n",
    "                                                  )\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter, NLTKTextSplitter, SpacyTextSplitter\n",
    "import pymupdf\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "import shutil\n",
    "from transformers import pipeline\n",
    "from src.utils import *\n",
    "from src.data_processing import MathLatexRecovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = take_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = config_parser(data_config_path = 'config/data_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"data/web_data/introduction to algo4 new.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "ori_text = loader.load()\n",
    "ori_text, duplicate_count = remove_duplicate_documents(ori_text[1060:1067])\n",
    "print(len(ori_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Problems for Chapter 33 \\n1039 \\nwhere \\x03 \\n.0/ is some value between x \\n.0/ and x \\n\\ue003 . If the approximation in equa- \\ntion (33.34) holds for x \\n.0/ , it also holds for any point closer to x \\n\\ue003 . \\nb. Assume that the function f has exactly one point x \\n\\ue003 for which f .x \\n\\ue003 / D 0. Let \\n\\x05 \\n.t/ D \\nˇ \\nˇ \\nx \\n.t/ \\ue003 x \\n\\ue003 ˇ \\nˇ \\n. Using the Taylor expansion in equation (33.34), show that \\n\\x05 \\n.t C1/ D \\nˇ \\nˇ \\nf 00 .\\x03 \\n.t/ / \\nˇ \\nˇ \\n2 jf 0 .\\x03 \\n.t/ /j \\x05 \\n.t/ ; \\nwhere \\x03 \\n.t/ is some value between x \\n.t/ and x \\n\\ue003 . \\nc. If \\nˇ \\nˇ \\nf 00 .\\x03 \\n.t/ / \\nˇ \\nˇ \\n2 jf 0 .\\x03 \\n.t/ /j හ c \\nfor some constant c and \\x05 \\n.0/ < 1, then we say that the function f has quadratic \\nconvergence, since the error decreases quadratically. Assuming that f has qua- \\ndratic convergence, how many iterations are needed to ûnd a root of f .x/ to an \\naccuracy of ı \\n? Your answer should include ı \\n. \\nd. Suppose you wish to ûnd a root of the function f .x/ D .x \\ue003 3/ \\n2 , which is also \\nthe minimizer, and you start at x \\n.0/ D 3:5. Compare the number of iterations \\nneeded by gradient descent to ûnd the minimizer and Newton’s method to ûnd \\nthe root. \\n33-2 Hedge \\nAnother variant in the multiplicative-weights framework is known as HEDGE. It \\ndiffers from WEIGHTED MAJORITY in two ways. First, HEDGE makes the pre- \\ndiction randomly4in iteration t \\n, it assigns a probability p \\n.t/ \\ni \\nD w \\n.t/ \\ni =Z \\n.t/ to ex- \\npert E \\ni , where Z \\n.t/ D P \\nn \\ni D1 w \\n.t/ \\ni . It then chooses an expert E \\ni \\n0 according to this \\nprobability distribution and predicts according to E \\ni \\n0 . Second, the update rule is \\ndifferent. If an expert makes a mistake, line 16 updates that expert’s weight by the \\nrule w \\n.t C1/ \\ni \\nD w \\n.t/ \\ni e \\n\\ue002\\ue001 , for some 0 < \\x05 < 1. Show that the expected number of \\nmistakes made by HEDGE, running for T rounds, is at most m \\n\\ue003 C .ln n/=\\x05 C \\x05T . \\n33-3 Nonoptimality of Lloyd’s procedure in one dimension \\nGive an example to show that even in one dimension, Lloyd’s procedure for ûnding \\nclusters does not always return an optimum result. That is, Lloyd’s procedure may \\nterminate and return as a result a set C of clusters that does not minimize f .S; C/, \\neven when S is a set of points on a line. \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1060, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1040 \\nChapter 33 Machine-Learning Algorithms \\n33-4 Stochastic gradient descent \\nConsider the problem described in Section 33.3 of ûtting a line f .x/ D ax C b \\nto a given set of point/value pairs S D f.x \\n1 ; y \\n1 /; :::; .x \\nT ; y \\nT /g by optimizing the \\nchoice of the parameters a and b using gradient descent to ûnd a best least-squares \\nût. Here we consider the case where x is a real-valued variable, rather than a vector. \\nSuppose that you are not given the point/value pairs in S all at once, but only \\none at a time in an online manner. Furthermore, the points are given in random \\norder. That is, you know that there are n points, but in iteration t you are given \\nonly .x \\ni ; y \\ni / where i is independently and randomly chosen from f1; : : : ; T g. \\nYou can use gradient descent to compute an estimate to the function. As each \\npoint .x \\ni ; y \\ni / is considered, you can update the current values of a and b by taking \\nthe derivative with respect to a and b of the term of the objective function depend- \\ning on .x \\ni ; y \\ni /. Doing so gives you a stochastic estimate of the gradient, and you \\ncan then take a small step in the opposite direction. \\nGive pseudcode to implement this variant of gradient descent. What would the \\nexpected value of the error be as a function of T , L, and R? (Hint: Replicate the \\nanalysis of GRADIENT-DESCENT in Section 33.3 for this variant.) \\nThis procedure and its variants are known as stochastic gradient descent. \\nChapter notes \\nFor a general introduction to artiûcial intelligence, we recommend Russell and \\nNorvig [391]. For a general introduction to machine learning, we recommend \\nMurphy [340]. \\nLloyd’s procedure for the k-means problem was ûrst proposed by Lloyd [304] \\nand also later by Forgy [151]. It is sometimes called <Lloyd’ \\ns algorithm= or the \\n<Lloyd-Forgy algorithm.= Although Mahajan et al. [310] showed that ûnding an \\noptimal clustering is NP-hard, even in the plane, Kanungo et al. [241] have shown \\nthat there is an approximation algorithm for the k-means problem with approxima- \\ntion ratio 9 C \\x05 \\n, for any \\x05 > 0. \\nThe multiplicative-weights method is surveyed by Arora, Hazan, and Kale [25]. \\nThe main idea of updating weights based on feedback has been rediscovered many \\ntimes. One early use is in game theory, where Brown deûned <Fictitious Play= [74] \\nand conjectured its convergence to the value of a zero-sum game. The convergence \\nproperties were established by Robinson [382]. \\nIn machine learning, the ûrst use of multiplicative weights was by Littlestone in \\nthe Winnow algorithm [300], which was later extended by Littlestone and Warmuth \\nto the weighted-majority algorithm described in Section 33.2 [301]. This work is \\nclosely connected to the boosting algorithm, originally due to Freund and Shapire \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1061, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Notes for Chapter 33 \\n1041 \\n[159]. The multiplicative-weights idea is also closely related to several more gen- \\neral optimization algorithms, including the perceptron algorithm [328] and algo- \\nrithms for optimization problems such as packing linear programs [177, 359]. \\nThe treatment of gradient descent in this chapter draws heavily on the unpub- \\nlished manuscript of Bansal and Gupta [35]. They emphasize the idea of using \\na potential function and using ideas from amortized analysis to explain gradient \\ndescent. Other presentations and analyses of gradient descent include works by \\nBubeck [75], Boyd and Vanderberghe [69], and Nesterov [343]. \\nGradient descent is known to converge faster when functions obey stronger prop- \\nerties than general convexity. For example, a function f is ˛-strongly convex if \\nf .y \\n/ \\ue004 f .x/ C h.r \\nf /.x/; .y \\ue003 x/i C ˛ ky \\ue003 xk for all x; y 2 R \\nn . In this case, \\nGRADIENT-DESCENT can use a variable step size and return x \\n.T / . The step size \\nat step t becomes \\x03 \\nt D 1=.˛.t C 1//, and the procedure returns a point such that \\nf .x-avg/ \\ue003 f .x \\n\\ue003 / හ L \\n2 =.˛.T C 1//. This convergence is better than that of The- \\norem 33.8 because the number of iterations needed is linear, rather than quadratic, \\nin the desired error parameter \\x05 \\n, and because the performance is independent of the \\ninitial point. \\nAnother case in which gradient descent can be shown to perform better than \\nthe analysis in Section 33.3 suggests is for smooth convex functions. We say that a \\nfunction is ˇ-smooth if f .y \\n/ හ f .x/ \\nCh.r \\nf /.x/;.y \\ue003 x/iC ˇ \\n2 ky \\ue003 xk \\n2 . This in- \\nequality goes in the opposite direction from the one for ˛-strong convexity. Better \\nbounds on gradient descent are possible here as well. \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1062, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='34 \\nNP-Completeness \\nAlmost all the algorithms we have studied thus far have been polynomial-time \\nalgorithms: on inputs of size n, their worst-case running time is O.n \\nk / for some \\nconstant k. You might wonder whether all problems can be solved in polynomial \\ntime. The answer is no. For example, there are problems, such as Turing’s famous \\n<Halting Problem,= that cannot be solved by any computer, no matter how long \\nyou’re willing to wait for an answer. \\n1 There are also problems that can be solved, \\nbut not in O.n \\nk / time for any constant k. Generally, we think of problems that are \\nsolvable by polynomial-time algorithms as being tractable, or <easy,= and problems \\nthat require superpolynomial time as being intractable, or <hard.= \\nThe subject of this chapter, however, is an interesting class of problems, called \\nthe <NP-complete= problems, whose status is unknown. No polynomial-time al- \\ngorithm has yet been discovered for an NP-complete problem, nor has anyone yet \\nbeen able to prove that no polynomial-time algorithm can exist for any one of them. \\nThis so-called P ¤ NP question has been one of the deepest, most perplexing open \\nresearch problems in theoretical computer science since it was ûrst posed in 1971. \\nSeveral NP-complete problems are particularly tantalizing because they seem \\non the surface to be similar to problems that we know how to solve in polynomial \\ntime. In each of the following pairs of problems, one is solvable in polynomial time \\nand the other is NP-complete, but the difference between the problems appears to \\nbe slight: \\nShortest versus longest simple paths: In Chapter 22, we saw that even with neg- \\native edge weights, we can ûnd shortest paths from a single source in a directed \\n1 For the Halting Problem and other unsolvable problems, there are proofs that no algorithm can \\nexist that, for every input, eventually produces the correct answer. A procedure attempting to solve \\nan unsolvable problem might always produce an answer but is sometimes incorrect, or all the answers \\nit produces might be correct but for some inputs it never produces an answer. \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1063, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Chapter 34 NP-Completeness \\n1043 \\ngraph G D .V; E/ in O.VE/ time. Finding a longest simple path between two \\nvertices is difûcult, however. Merely determining whether a graph contains a \\nsimple path with at least a given number of edges is NP-complete. \\nEuler tour versus hamiltonian cycle: An Euler tour of a strongly connected, \\ndirected graph G D .V; E/ is a cycle that traverses each edge of G exactly \\nonce, although it is allowed to visit each vertex more than once. Problem 20-3 \\non page 583 asks you to show how to determine whether a strongly connected, \\ndirected graph has an Euler tour and, if it does, the order of the edges in the Eu- \\nler tour, all in O.E/ time. A hamiltonian cycle of a directed graph G D .V; E/ \\nis a simple cycle that contains each vertex in V . Determining whether a di- \\nrected graph has a hamiltonian cycle is NP-complete. (Later in this chapter, \\nwe’ll prove that determining whether an undirected graph has a hamiltonian \\ncycle is NP-complete.) \\n2-CNF satisﬁability versus 3-CNF satisﬁability: Boolean formulas contain bi- \\nnary variables whose values are 0 or 1; boolean connectives such as ^ (AND), \\n_ (OR), and : (NOT); and parentheses. A boolean formula is satisﬁable if \\nthere exists some assignment of the values 0 and 1 to its variables that causes \\nit to evaluate to 1. We’ll deûne terms more formally later in this chapter, but \\ninformally, a boolean formula is in k-conjunctive normal form, or k-CNF if \\nit is the AND of clauses of ORs of exactly k variables or their negations. For \\nexample, the boolean formula .x \\n1 _ x \\n2 / ^ .:x \\n1 _ x \\n3 / ^ .:x \\n2 _ :x \\n3 / is in \\n2-CNF (with satisfying assignment x \\n1 D 1, x \\n2 D 0, and x \\n3 D 1). Although \\nthere is a polynomial-time algorithm to determine whether a 2-CNF formula \\nis satisûable, we’ll see later in this chapter that determining whether a 3-CNF \\nformula is satisûable is NP-complete. \\nNP-completeness and the classes P and NP \\nThroughout this chapter, we refer to three classes of problems: P, NP, and NPC, the \\nlatter class being the NP-complete problems. We describe them informally here, \\nwith formal deûnitions to appear later on. \\nThe class P consists of those problems that are solvable in polynomial time. \\nMore speciûcally, they are problems that can be solved in O.n \\nk / time for some \\nconstant k, where n is the size of the input to the problem. Most of the problems \\nexamined in previous chapters belong to P. \\nThe class NP consists of those problems that are <veriûable= in polynomial time. \\nWhat do we mean by a problem being veriûable? If you were somehow given \\na <certiûcate= of a solution, then you could verify that the certiûcate is correct \\nin time polynomial in the size of the input to the problem. For example, in the \\nhamiltonian-cycle problem, given a directed graph G D .V; E/, a certiûcate would \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1064, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1044 \\nChapter 34 NP-Completeness \\nbe a sequence hv \\n1 ; v \\n2 ; v \\n3 ; : : : ; v \\njV j i of jV j vertices. You could check in polyno- \\nmial time that the sequence contains each of the jV j vertices exactly once, that \\n.v \\ni ; v \\ni C1 / 2 E for i D 1; 2; 3; : : : ; jV j \\ue003 1, and that .v \\njV j ; v \\n1 / 2 E. As another \\nexample, for 3-CNF satisûability, a certiûcate could be an assignment of values to \\nvariables. You could check in polynomial time that this assignment satisûes the \\nboolean formula. \\nAny problem in P also belongs to NP, since if a problem belongs to P then it \\nis solvable in polynomial time without even being supplied a certiûcate. We’ll \\nformalize this notion later in this chapter, but for now you can believe that P ෂ NP. \\nThe famous open question is whether P is a proper subset of NP. \\nInformally, a problem belongs to the class NPC4and we call it NP-complete \\n4if it belongs to NP and is as <hard= as any problem in NP. We’ll formally de- \\nûne what it means to be as hard as any problem in NP later in this chapter. In the \\nmeantime, we state without proof that if any NP-complete problem can be solved \\nin polynomial time, then every problem in NP has a polynomial-time algorithm. \\nMost theoretical computer scientists believe that the NP-complete problems are \\nintractable, since given the wide range of NP-complete problems that have been \\nstudied to date4without anyone having discovered a polynomial-time solution to \\nany of them4it would be truly astounding if all of them could be solved in poly- \\nnomial time. Yet, given the effort devoted thus far to proving that NP-complete \\nproblems are intractable4without a conclusive outcome4we cannot rule out the \\npossibility that the NP-complete problems could turn out to be solvable in polyno- \\nmial time. \\nTo become a good algorithm designer, you must understand the rudiments of the \\ntheory of NP-completeness. If you can establish a problem as NP-complete, you \\nprovide good evidence for its intractability. As an engineer, you would then do \\nbetter to spend your time developing an approximation algorithm (see Chapter 35) \\nor solving a tractable special case, rather than searching for a fast algorithm that \\nsolves the problem exactly. Moreover, many natural and interesting problems that \\non the surface seem no harder than sorting, graph searching, or network üow are \\nin fact NP-complete. Therefore, you should become familiar with this remarkable \\nclass of problems. \\nOverview of showing problems to be NP-complete \\nThe techniques used to show that a particular problem is NP-complete differ fun- \\ndamentally from the techniques used throughout most of this book to design and \\nanalyze algorithms. If you can demonstrate that a problem is NP-complete, you \\nare making a statement about how hard it is (or at least how hard we think it is), \\nrather than about how easy it is. If you prove a problem NP-complete, you are say- \\ning that searching for efûcient algorithm is likely to be a fruitless endeavor. In this \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1065, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Chapter 34 NP-Completeness \\n1045 \\nway, NP-completeness proofs bear some similarity to the proof in Section 8.1 of an \\nY.n lg n/-time lower bound for any comparison sort algorithm, although the spe- \\nciûc techniques used for showing NP-completeness differ from the decision-tree \\nmethod used in Section 8.1. \\nWe rely on three key concepts in showing a problem to be NP-complete: \\nDecision problems versus optimization problems \\nMany problems of interest are optimization problems, in which each feasible (i.e., \\n<legal=) solution has an associated value, and the goal is to ûnd a feasible solution \\nwith the best value. For example, in a problem that we call SHORTEST-PATH, \\nthe input is an undirected graph G and vertices u and v, and the goal is to ûnd a \\npath from u to v that uses the fewest edges. In other words, SHORTEST-PATH \\nis the single-pair shortest-path problem in an unweighted, undirected graph. NP- \\ncompleteness applies directly not to optimization problems, however, but to deci- \\nsion problems, in which the answer is simply <yes= or <no= (or, more formally, <1= \\nor <0=). \\nAlthough NP-complete problems are conûned to the realm of decision problems, \\nthere is usually a way to cast a given optimization problem as a related decision \\nproblem by imposing a bound on the value to be optimized. For example, a deci- \\nsion problem related to SHORTEST-PATH is PATH: given an undirected graph G, \\nvertices u and v, and an integer k, does a path exist from u to v consisting of at \\nmost k edges? \\nThe relationship between an optimization problem and its related decision prob- \\nlem works in your favor when you try to show that the optimization problem is \\n<hard.= That is because the decision problem is in a sense <easier,= or at least <no \\nharder.= As a speciûc example, you can solve PATH by solving SHORTEST-PATH \\nand then comparing the number of edges in the shortest path found to the value \\nof the decision-problem parameter k. In other words, if an optimization prob- \\nlem is easy, its related decision problem is easy as well. Stated in a way that has \\nmore relevance to NP-completeness, if you can provide evidence that a decision \\nproblem is hard, you also provide evidence that its related optimization problem is \\nhard. Thus, even though it restricts attention to decision problems, the theory of \\nNP-completeness often has implications for optimization problems as well. \\nReductions \\nThe above notion of showing that one problem is no harder or no easier than an- \\nother applies even when both problems are decision problems. Almost every NP- \\ncompleteness proof takes advantage of this idea, as follows. Consider a decision \\nproblem A, which you would like to solve in polynomial time. We call the input \\nto a particular problem an instance of that problem. For example, in PATH, an \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1066, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_text[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [txt.page_content for txt in ori_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"CrissWang/bert-math\", torch_dtype=torch.float16, device = take_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Math', 'score': 0.66357421875}, {'label': 'Not Math', 'score': 1.0}, {'label': 'Not Math', 'score': 1.0}, {'label': 'Not Math', 'score': 1.0}, {'label': 'Not Math', 'score': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(text_list, batch_size=8, truncation=\"only_first\",)\n",
    "print(outputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input, output in zip(text_list, outputs):\n",
    "#     print(f\"output: {output['label']} - Input: {input}\")\n",
    "#     print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "There are 1 / 7 pages has math\n"
     ]
    }
   ],
   "source": [
    "math_indices = [index for index, output in enumerate(outputs) if output['label'].lower() == \"math\"]\n",
    "\n",
    "# Print the list of indices where the label is \"math\"\n",
    "print(math_indices[:5])\n",
    "print(f\"There are {len(math_indices)} / {len(ori_text)} pages has math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_math_doc_image( math_indices, pdf_path, tmp_path=\"data/tmp\"):\n",
    "    doc = pymupdf.open(pdf_path)  # open document\n",
    "    os.makedirs(tmp_path, exist_ok=True)\n",
    "    image_paths = []\n",
    "    for page in doc:  # iterate through the pages\n",
    "        if not (page.number in math_indices):\n",
    "            continue\n",
    "        pix = page.get_pixmap()  # render page to an image\n",
    "        save_path = f\"{tmp_path}/%i.png\" % page.number\n",
    "        pix.save(save_path)  # store image as a PNG\n",
    "        image_paths.append(save_path)\n",
    "    return image_paths\n",
    "\n",
    "tmp_path=\"data/tmp\"\n",
    "image_list = save_math_doc_image(math_indices, pdf_path,tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "nougat_processor = pipeline(\"image-to-text\", model=\"facebook/nougat-base\",  device = take_device(), max_new_tokens=1024 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/tmp/0.png']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = nougat_processor(image_list, batch_size=8,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(math_indices):\n",
    "    for item in outputs[i]:\n",
    "        item['index'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"where \\\\(\\\\gamma^{(0)}\\\\) is some value between \\\\(x^{(0)}\\\\) and \\\\(x^{\\\\star}\\\\). If the approximation in equation (33.34) holds for \\\\(x^{(0)}\\\\), it also holds for any point closer to \\\\(x^{\\\\star}\\\\).\\n\\n_b._ Assume that the function \\\\(f\\\\) has exactly one point \\\\(x^{\\\\star}\\\\) for which \\\\(f(x^{\\\\star})=0\\\\). Let \\\\(\\\\epsilon^{(t)}=\\\\left|x^{(t)}-x^{\\\\star}\\\\right|\\\\). Using the Taylor expansion in equation (33.34), show that\\n\\n\\\\[\\\\epsilon^{(t+1)}=\\\\frac{\\\\left|f^{\\\\prime\\\\prime}(\\\\gamma^{(t)})\\\\right|}{2\\\\left|f^{ \\\\prime}(\\\\gamma^{(t)})\\\\right|}\\\\epsilon^{(t)}\\\\ ,\\\\]\\n\\nwhere \\\\(\\\\gamma^{(t)}\\\\) is some value between \\\\(x^{(t)}\\\\) and \\\\(x^{\\\\star}\\\\).\\n\\n_c._ If\\n\\n\\\\[\\\\frac{\\\\left|f^{\\\\prime\\\\prime}(\\\\gamma^{(t)})\\\\right|}{2\\\\left|f^{\\\\prime}(\\\\gamma^{( t)})\\\\right|}\\\\leq c\\\\]\\n\\n for some constant \\\\(c\\\\) and \\\\(\\\\epsilon^{(0)}<1\\\\), then we say that the function \\\\(f\\\\) has _quadratic convergence_, since the error decreases quadratically. Assuming that \\\\(f\\\\) has quadratic convergence, how many iterations are needed to find a root of \\\\(f(x)\\\\) to an accuracy of \\\\(\\\\delta\\\\)? Your answer should include \\\\(\\\\delta\\\\).\\n\\n_d._ Suppose you wish to find a root of the function \\\\(f(x)=(x-3)^{2}\\\\), which is also the minimizer, and you start at \\\\(x^{(0)}=3.5\\\\). Compare the number of iterations needed by gradient descent to find the minimizer and Newton's method to find the root.\\n\\n_33-2 Hedge_\\n\\nAnother variant in the multiplicative-weights framework is known as Hedge. It differs from Weighted Majority in two ways. First, Hedge makes the prediction randomly--in iteration \\\\(t\\\\), it assigns a probability \\\\(p_{i}^{(t)}=w_{i}^{(t)}/Z^{(t)}\\\\) to expert \\\\(E_{i}\\\\), where \\\\(Z^{(t)}=\\\\sum_{i=1}^{n}w_{i}^{(t)}\\\\). It then chooses an expert \\\\(E_{i^{\\\\prime}}\\\\) according to this probability distribution and predicts according to \\\\(E_{i^{\\\\prime}}\\\\). Second, the update rule is different. If an expert makes a mistake, line 16 updates that expert's weight by the rule \\\\(w_{i}^{(t+1)}=w_{i}^{(t)}e^{-\\\\epsilon}\\\\), for some \\\\(0<\\\\epsilon<1\\\\). Show that the expected number of mistakes made by Hedge, running for \\\\(T\\\\) rounds, is at most \\\\(m^{\\\\star}+(\\\\ln n)/\\\\epsilon+\\\\epsilon T\\\\).\\n\\n_33-3 Nonoptimality of Lloyd's procedure in one dimension_\\n\\nGive an example to show that even in one dimension, Lloyd's procedure for finding clusters does not always return an optimum result. That is, Lloyd's procedure may terminate and return as a result a set \\\\(C\\\\) of clusters that does not minimize \\\\(f(S,C)\\\\), even when \\\\(S\\\\) is a set of points on a line.\",\n",
       "   'index': 0}]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_and_recreate_folder(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(outputs):\n",
    "    idx = int(item[0]['index'])\n",
    "    ori_text[idx].page_content = outputs[i][0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_text = combine_short_doc(ori_text, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"where \\\\(\\\\gamma^{(0)}\\\\) is some value between \\\\(x^{(0)}\\\\) and \\\\(x^{\\\\star}\\\\). If the approximation in equation (33.34) holds for \\\\(x^{(0)}\\\\), it also holds for any point closer to \\\\(x^{\\\\star}\\\\).\\n\\n_b._ Assume that the function \\\\(f\\\\) has exactly one point \\\\(x^{\\\\star}\\\\) for which \\\\(f(x^{\\\\star})=0\\\\). Let \\\\(\\\\epsilon^{(t)}=\\\\left|x^{(t)}-x^{\\\\star}\\\\right|\\\\). Using the Taylor expansion in equation (33.34), show that\\n\\n\\\\[\\\\epsilon^{(t+1)}=\\\\frac{\\\\left|f^{\\\\prime\\\\prime}(\\\\gamma^{(t)})\\\\right|}{2\\\\left|f^{ \\\\prime}(\\\\gamma^{(t)})\\\\right|}\\\\epsilon^{(t)}\\\\ ,\\\\]\\n\\nwhere \\\\(\\\\gamma^{(t)}\\\\) is some value between \\\\(x^{(t)}\\\\) and \\\\(x^{\\\\star}\\\\).\\n\\n_c._ If\\n\\n\\\\[\\\\frac{\\\\left|f^{\\\\prime\\\\prime}(\\\\gamma^{(t)})\\\\right|}{2\\\\left|f^{\\\\prime}(\\\\gamma^{( t)})\\\\right|}\\\\leq c\\\\]\\n\\n for some constant \\\\(c\\\\) and \\\\(\\\\epsilon^{(0)}<1\\\\), then we say that the function \\\\(f\\\\) has _quadratic convergence_, since the error decreases quadratically. Assuming that \\\\(f\\\\) has quadratic convergence, how many iterations are needed to find a root of \\\\(f(x)\\\\) to an accuracy of \\\\(\\\\delta\\\\)? Your answer should include \\\\(\\\\delta\\\\).\\n\\n_d._ Suppose you wish to find a root of the function \\\\(f(x)=(x-3)^{2}\\\\), which is also the minimizer, and you start at \\\\(x^{(0)}=3.5\\\\). Compare the number of iterations needed by gradient descent to find the minimizer and Newton's method to find the root.\\n\\n_33-2 Hedge_\\n\\nAnother variant in the multiplicative-weights framework is known as Hedge. It differs from Weighted Majority in two ways. First, Hedge makes the prediction randomly--in iteration \\\\(t\\\\), it assigns a probability \\\\(p_{i}^{(t)}=w_{i}^{(t)}/Z^{(t)}\\\\) to expert \\\\(E_{i}\\\\), where \\\\(Z^{(t)}=\\\\sum_{i=1}^{n}w_{i}^{(t)}\\\\). It then chooses an expert \\\\(E_{i^{\\\\prime}}\\\\) according to this probability distribution and predicts according to \\\\(E_{i^{\\\\prime}}\\\\). Second, the update rule is different. If an expert makes a mistake, line 16 updates that expert's weight by the rule \\\\(w_{i}^{(t+1)}=w_{i}^{(t)}e^{-\\\\epsilon}\\\\), for some \\\\(0<\\\\epsilon<1\\\\). Show that the expected number of mistakes made by Hedge, running for \\\\(T\\\\) rounds, is at most \\\\(m^{\\\\star}+(\\\\ln n)/\\\\epsilon+\\\\epsilon T\\\\).\\n\\n_33-3 Nonoptimality of Lloyd's procedure in one dimension_\\n\\nGive an example to show that even in one dimension, Lloyd's procedure for finding clusters does not always return an optimum result. That is, Lloyd's procedure may terminate and return as a result a set \\\\(C\\\\) of clusters that does not minimize \\\\(f(S,C)\\\\), even when \\\\(S\\\\) is a set of points on a line.\", metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1060, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1040 \\nChapter 33 Machine-Learning Algorithms \\n33-4 Stochastic gradient descent \\nConsider the problem described in Section 33.3 of ûtting a line f .x/ D ax C b \\nto a given set of point/value pairs S D f.x \\n1 ; y \\n1 /; :::; .x \\nT ; y \\nT /g by optimizing the \\nchoice of the parameters a and b using gradient descent to ûnd a best least-squares \\nût. Here we consider the case where x is a real-valued variable, rather than a vector. \\nSuppose that you are not given the point/value pairs in S all at once, but only \\none at a time in an online manner. Furthermore, the points are given in random \\norder. That is, you know that there are n points, but in iteration t you are given \\nonly .x \\ni ; y \\ni / where i is independently and randomly chosen from f1; : : : ; T g. \\nYou can use gradient descent to compute an estimate to the function. As each \\npoint .x \\ni ; y \\ni / is considered, you can update the current values of a and b by taking \\nthe derivative with respect to a and b of the term of the objective function depend- \\ning on .x \\ni ; y \\ni /. Doing so gives you a stochastic estimate of the gradient, and you \\ncan then take a small step in the opposite direction. \\nGive pseudcode to implement this variant of gradient descent. What would the \\nexpected value of the error be as a function of T , L, and R? (Hint: Replicate the \\nanalysis of GRADIENT-DESCENT in Section 33.3 for this variant.) \\nThis procedure and its variants are known as stochastic gradient descent. \\nChapter notes \\nFor a general introduction to artiûcial intelligence, we recommend Russell and \\nNorvig [391]. For a general introduction to machine learning, we recommend \\nMurphy [340]. \\nLloyd’s procedure for the k-means problem was ûrst proposed by Lloyd [304] \\nand also later by Forgy [151]. It is sometimes called <Lloyd’ \\ns algorithm= or the \\n<Lloyd-Forgy algorithm.= Although Mahajan et al. [310] showed that ûnding an \\noptimal clustering is NP-hard, even in the plane, Kanungo et al. [241] have shown \\nthat there is an approximation algorithm for the k-means problem with approxima- \\ntion ratio 9 C \\x05 \\n, for any \\x05 > 0. \\nThe multiplicative-weights method is surveyed by Arora, Hazan, and Kale [25]. \\nThe main idea of updating weights based on feedback has been rediscovered many \\ntimes. One early use is in game theory, where Brown deûned <Fictitious Play= [74] \\nand conjectured its convergence to the value of a zero-sum game. The convergence \\nproperties were established by Robinson [382]. \\nIn machine learning, the ûrst use of multiplicative weights was by Littlestone in \\nthe Winnow algorithm [300], which was later extended by Littlestone and Warmuth \\nto the weighted-majority algorithm described in Section 33.2 [301]. This work is \\nclosely connected to the boosting algorithm, originally due to Freund and Shapire \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1061, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Notes for Chapter 33 \\n1041 \\n[159]. The multiplicative-weights idea is also closely related to several more gen- \\neral optimization algorithms, including the perceptron algorithm [328] and algo- \\nrithms for optimization problems such as packing linear programs [177, 359]. \\nThe treatment of gradient descent in this chapter draws heavily on the unpub- \\nlished manuscript of Bansal and Gupta [35]. They emphasize the idea of using \\na potential function and using ideas from amortized analysis to explain gradient \\ndescent. Other presentations and analyses of gradient descent include works by \\nBubeck [75], Boyd and Vanderberghe [69], and Nesterov [343]. \\nGradient descent is known to converge faster when functions obey stronger prop- \\nerties than general convexity. For example, a function f is ˛-strongly convex if \\nf .y \\n/ \\ue004 f .x/ C h.r \\nf /.x/; .y \\ue003 x/i C ˛ ky \\ue003 xk for all x; y 2 R \\nn . In this case, \\nGRADIENT-DESCENT can use a variable step size and return x \\n.T / . The step size \\nat step t becomes \\x03 \\nt D 1=.˛.t C 1//, and the procedure returns a point such that \\nf .x-avg/ \\ue003 f .x \\n\\ue003 / හ L \\n2 =.˛.T C 1//. This convergence is better than that of The- \\norem 33.8 because the number of iterations needed is linear, rather than quadratic, \\nin the desired error parameter \\x05 \\n, and because the performance is independent of the \\ninitial point. \\nAnother case in which gradient descent can be shown to perform better than \\nthe analysis in Section 33.3 suggests is for smooth convex functions. We say that a \\nfunction is ˇ-smooth if f .y \\n/ හ f .x/ \\nCh.r \\nf /.x/;.y \\ue003 x/iC ˇ \\n2 ky \\ue003 xk \\n2 . This in- \\nequality goes in the opposite direction from the one for ˛-strong convexity. Better \\nbounds on gradient descent are possible here as well. \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1062, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='34 \\nNP-Completeness \\nAlmost all the algorithms we have studied thus far have been polynomial-time \\nalgorithms: on inputs of size n, their worst-case running time is O.n \\nk / for some \\nconstant k. You might wonder whether all problems can be solved in polynomial \\ntime. The answer is no. For example, there are problems, such as Turing’s famous \\n<Halting Problem,= that cannot be solved by any computer, no matter how long \\nyou’re willing to wait for an answer. \\n1 There are also problems that can be solved, \\nbut not in O.n \\nk / time for any constant k. Generally, we think of problems that are \\nsolvable by polynomial-time algorithms as being tractable, or <easy,= and problems \\nthat require superpolynomial time as being intractable, or <hard.= \\nThe subject of this chapter, however, is an interesting class of problems, called \\nthe <NP-complete= problems, whose status is unknown. No polynomial-time al- \\ngorithm has yet been discovered for an NP-complete problem, nor has anyone yet \\nbeen able to prove that no polynomial-time algorithm can exist for any one of them. \\nThis so-called P ¤ NP question has been one of the deepest, most perplexing open \\nresearch problems in theoretical computer science since it was ûrst posed in 1971. \\nSeveral NP-complete problems are particularly tantalizing because they seem \\non the surface to be similar to problems that we know how to solve in polynomial \\ntime. In each of the following pairs of problems, one is solvable in polynomial time \\nand the other is NP-complete, but the difference between the problems appears to \\nbe slight: \\nShortest versus longest simple paths: In Chapter 22, we saw that even with neg- \\native edge weights, we can ûnd shortest paths from a single source in a directed \\n1 For the Halting Problem and other unsolvable problems, there are proofs that no algorithm can \\nexist that, for every input, eventually produces the correct answer. A procedure attempting to solve \\nan unsolvable problem might always produce an answer but is sometimes incorrect, or all the answers \\nit produces might be correct but for some inputs it never produces an answer. \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1063, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Chapter 34 NP-Completeness \\n1043 \\ngraph G D .V; E/ in O.VE/ time. Finding a longest simple path between two \\nvertices is difûcult, however. Merely determining whether a graph contains a \\nsimple path with at least a given number of edges is NP-complete. \\nEuler tour versus hamiltonian cycle: An Euler tour of a strongly connected, \\ndirected graph G D .V; E/ is a cycle that traverses each edge of G exactly \\nonce, although it is allowed to visit each vertex more than once. Problem 20-3 \\non page 583 asks you to show how to determine whether a strongly connected, \\ndirected graph has an Euler tour and, if it does, the order of the edges in the Eu- \\nler tour, all in O.E/ time. A hamiltonian cycle of a directed graph G D .V; E/ \\nis a simple cycle that contains each vertex in V . Determining whether a di- \\nrected graph has a hamiltonian cycle is NP-complete. (Later in this chapter, \\nwe’ll prove that determining whether an undirected graph has a hamiltonian \\ncycle is NP-complete.) \\n2-CNF satisﬁability versus 3-CNF satisﬁability: Boolean formulas contain bi- \\nnary variables whose values are 0 or 1; boolean connectives such as ^ (AND), \\n_ (OR), and : (NOT); and parentheses. A boolean formula is satisﬁable if \\nthere exists some assignment of the values 0 and 1 to its variables that causes \\nit to evaluate to 1. We’ll deûne terms more formally later in this chapter, but \\ninformally, a boolean formula is in k-conjunctive normal form, or k-CNF if \\nit is the AND of clauses of ORs of exactly k variables or their negations. For \\nexample, the boolean formula .x \\n1 _ x \\n2 / ^ .:x \\n1 _ x \\n3 / ^ .:x \\n2 _ :x \\n3 / is in \\n2-CNF (with satisfying assignment x \\n1 D 1, x \\n2 D 0, and x \\n3 D 1). Although \\nthere is a polynomial-time algorithm to determine whether a 2-CNF formula \\nis satisûable, we’ll see later in this chapter that determining whether a 3-CNF \\nformula is satisûable is NP-complete. \\nNP-completeness and the classes P and NP \\nThroughout this chapter, we refer to three classes of problems: P, NP, and NPC, the \\nlatter class being the NP-complete problems. We describe them informally here, \\nwith formal deûnitions to appear later on. \\nThe class P consists of those problems that are solvable in polynomial time. \\nMore speciûcally, they are problems that can be solved in O.n \\nk / time for some \\nconstant k, where n is the size of the input to the problem. Most of the problems \\nexamined in previous chapters belong to P. \\nThe class NP consists of those problems that are <veriûable= in polynomial time. \\nWhat do we mean by a problem being veriûable? If you were somehow given \\na <certiûcate= of a solution, then you could verify that the certiûcate is correct \\nin time polynomial in the size of the input to the problem. For example, in the \\nhamiltonian-cycle problem, given a directed graph G D .V; E/, a certiûcate would \\n', metadata={'source': 'data/web_data/introduction to algo4 new.pdf', 'file_path': 'data/web_data/introduction to algo4 new.pdf', 'page': 1064, 'total_pages': 1312, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = MathLatexRecovery()\n",
    "outputs = post_processor.recover_math(ori_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Graph application\\nNguy¹n An Kh÷ìng,\\nTr¦n Tu§n Anh, Nguy¹n\\nTi¸n Thinh\\n4.14\\nExercise\\nFind all the cut vertices, cut edges of the graphs\\na) Cn, where n ≥3\\nb) Wn where n ≥3\\nc) Km,n where m ≥2, n ≥2\\n', metadata={'source': 'data/web_data/Graph application 2.pdf', 'file_path': 'data/web_data/Graph application 2.pdf', 'page': 32, 'total_pages': 284, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyn An Khng, Trn Tun Anh, Nguyn Tin Thinh', 'subject': 'Lecturer Mathematical Foundation For Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20230917205231+07'00'\", 'modDate': \"D:20230917205231+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Graph application\\nNguy¹n An Kh÷ìng,\\nTr¦n Tu§n Anh, Nguy¹n\\nTi¸n Thinh\\n4.15\\nExercise\\nFor each of these graphs, find κ(G), λ(G)\\na)\\nb)\\n', metadata={'source': 'data/web_data/Graph application 2.pdf', 'file_path': 'data/web_data/Graph application 2.pdf', 'page': 33, 'total_pages': 284, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyn An Khng, Trn Tun Anh, Nguyn Tin Thinh', 'subject': 'Lecturer Mathematical Foundation For Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20230917205231+07'00'\", 'modDate': \"D:20230917205231+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='.', metadata={'source': 'data/web_data/Graph application 2.pdf', 'file_path': 'data/web_data/Graph application 2.pdf', 'page': 34, 'total_pages': 284, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyn An Khng, Trn Tun Anh, Nguyn Tin Thinh', 'subject': 'Lecturer Mathematical Foundation For Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20230917205231+07'00'\", 'modDate': \"D:20230917205231+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='For each of these graphs, find \\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G)\\\\)\\n\\n\\\\(\\\\kappa(G),\\\\lambda(G', metadata={'source': 'data/web_data/Graph application 2.pdf', 'file_path': 'data/web_data/Graph application 2.pdf', 'page': 35, 'total_pages': 284, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyn An Khng, Trn Tun Anh, Nguyn Tin Thinh', 'subject': 'Lecturer Mathematical Foundation For Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20230917205231+07'00'\", 'modDate': \"D:20230917205231+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Graph application\\nNguy¹n An Kh÷ìng,\\nTr¦n Tu§n Anh, Nguy¹n\\nTi¸n Thinh\\n4.16\\nConnectedness in Directed Graphs\\nDefinition\\n• An directed graph is strongly connected (li¶n thæng m¤nh) if\\nthere is a path between any two vertices in the graph (for\\nboth directions).\\n• An directed graph is weakly connected (li¶n thæng y¸u) if\\nthere is a path between any two vertices in the underlying\\nundirected graph.\\na\\ne\\nb\\nc\\nd\\nStrongly connected\\na\\ne\\nb\\nc\\nd\\nWeakly connected\\n', metadata={'source': 'data/web_data/Graph application 2.pdf', 'file_path': 'data/web_data/Graph application 2.pdf', 'page': 36, 'total_pages': 284, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyn An Khng, Trn Tun Anh, Nguyn Tin Thinh', 'subject': 'Lecturer Mathematical Foundation For Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20230917205231+07'00'\", 'modDate': \"D:20230917205231+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Graph application\\nNguy¹n An Kh÷ìng,\\nTr¦n Tu§n Anh, Nguy¹n\\nTi¸n Thinh\\n4.17\\nThe Famous Problem of Seven Bridges of K\\x04\\nonigsberg\\n• Is there a route that a person crosses all the seven bridges\\nonce?\\n', metadata={'source': 'data/web_data/Graph application 2.pdf', 'file_path': 'data/web_data/Graph application 2.pdf', 'page': 37, 'total_pages': 284, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyn An Khng, Trn Tun Anh, Nguyn Tin Thinh', 'subject': 'Lecturer Mathematical Foundation For Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20230917205231+07'00'\", 'modDate': \"D:20230917205231+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Graph application\\nNguy¹n An Kh÷ìng,\\nTr¦n Tu§n Anh, Nguy¹n\\nTi¸n Thinh\\n4.18\\nEuler Solution\\n• Euler gave the solution: It is not possible to cross all the\\nbridges exactly once.\\n', metadata={'source': 'data/web_data/Graph application 2.pdf', 'file_path': 'data/web_data/Graph application 2.pdf', 'page': 38, 'total_pages': 284, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyn An Khng, Trn Tun Anh, Nguyn Tin Thinh', 'subject': 'Lecturer Mathematical Foundation For Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20230917205231+07'00'\", 'modDate': \"D:20230917205231+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Graph application\\nNguy¹n An Kh÷ìng,\\nTr¦n Tu§n Anh, Nguy¹n\\nTi¸n Thinh\\n4.19\\nWhat is Euler Path and Circuit?\\n• Euler Path (\\x1f÷íng \\x1fi Euler) is a path in the graph that\\npasses each edge only once.\\nThe problem of Seven Bridges of K\\x04\\nonigsberg can be also\\nstated: Does Euler Path exist in the graph?\\n• Euler Circuit (chu tr¼nh Euler) is a path in the graph that\\npasses each edge only once and return back to its original\\nposition.\\nFrom Definition, Euler Circuit is a subset of Euler Path.\\n', metadata={'source': 'data/web_data/Graph application 2.pdf', 'file_path': 'data/web_data/Graph application 2.pdf', 'page': 39, 'total_pages': 284, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyn An Khng, Trn Tun Anh, Nguyn Tin Thinh', 'subject': 'Lecturer Mathematical Foundation For Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20230917205231+07'00'\", 'modDate': \"D:20230917205231+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Graph application\\nNguy¹n An Kh÷ìng,\\nTr¦n Tu§n Anh, Nguy¹n\\nTi¸n Thinh\\n4.20\\nExamples of Euler Path and Circuit\\n', metadata={'source': 'data/web_data/Graph application 2.pdf', 'file_path': 'data/web_data/Graph application 2.pdf', 'page': 40, 'total_pages': 284, 'format': 'PDF 1.5', 'title': '', 'author': 'Nguyn An Khng, Trn Tun Anh, Nguyn Tin Thinh', 'subject': 'Lecturer Mathematical Foundation For Computer Science', 'keywords': '', 'creator': 'LaTeX with Beamer class', 'producer': 'MiKTeX pdfTeX-1.40.21', 'creationDate': \"D:20230917205231+07'00'\", 'modDate': \"D:20230917205231+07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[30:39]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
